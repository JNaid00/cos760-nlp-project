{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "b7256dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, log_loss\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import sentencepiece as spm\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "e19fc9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_reader import MultiLangDataset, SplitSet\n",
    "from data_reader import ns_dataset, as_dataset\n",
    "from data_reader import Languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "59edfa13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jesse\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "5c0b58d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Custom Transformer using SentencePiece\n",
    "class SentencePieceVectorizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, model_file='yoruba.model', vocab_size=200):\n",
    "        self.model_file = model_file\n",
    "        self.vocab_size = vocab_size\n",
    "        self.sp = spm.SentencePieceProcessor()\n",
    "        self.sp.load(self.model_file)\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        rows, cols, data = [], [], []\n",
    "        for i, text in enumerate(X):\n",
    "            ids = self.sp.encode(text, out_type=int)\n",
    "            for idx in ids:\n",
    "                rows.append(i)\n",
    "                cols.append(idx)\n",
    "                data.append(1)\n",
    "        return csr_matrix((data, (rows, cols)), shape=(len(X), self.vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "77a57877",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def analyze_text(lang: Languages):\n",
    "\n",
    "    def compare_results(normal_result: dict, subword_result: dict):\n",
    "        # Convert to DataFrames\n",
    "        df = pd.DataFrame(normal_result).transpose()\n",
    "        subword_df = pd.DataFrame(subword_result).transpose()\n",
    "\n",
    "        print(f'Results for {lang} Language:')\n",
    "        print(\"Normal Tokenization Results:\")\n",
    "        print(df.round(3))\n",
    "        print(\"--------------------------------------------------\")\n",
    "        print(\"Subword Tokenization Results:\")\n",
    "        print(subword_df.round(3))\n",
    "        print(\"--------------------------------------------------\")\n",
    "\n",
    "    def delete_files():\n",
    "        import os\n",
    "        for filename in ['tweets.txt', 'lang_model.model', 'lang_model.vocab']:\n",
    "            try:\n",
    "                os.remove(filename)\n",
    "            except FileNotFoundError:\n",
    "                pass\n",
    "\n",
    "    lang_dataset: SplitSet = ns_dataset.get(lang)\n",
    "    X_train = lang_dataset.train[\"tweet\"]\n",
    "    lang_dataset.train[\"tweet\"].to_csv('tweets.txt', index=False, header=False)\n",
    "    y_train = lang_dataset.train[\"label\"]\n",
    "\n",
    "    X_test = lang_dataset.test[\"tweet\"]\n",
    "    y_test = lang_dataset.test[\"label\"]\n",
    "    stop_words = lang_dataset.stopwords\n",
    "\n",
    "    # Peform word tokenization\n",
    "    vectorizer = CountVectorizer(tokenizer=word_tokenize,stop_words=stop_words)\n",
    "    model = Pipeline([\n",
    "        ('vectorizer', vectorizer),  # word-level tokenizer by default\n",
    "        ('classifier', MultinomialNB())\n",
    "    ])\n",
    "    # Train model\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    word_result: dict = classification_report(y_test, y_pred, output_dict=True)\n",
    "\n",
    "\n",
    "    # SentencePiece tokenizer\n",
    "    spm.SentencePieceTrainer.Train(input='tweets.txt', model_prefix='lang_model', vocab_size=8000, model_type='bpe')\n",
    "    sp = spm.SentencePieceProcessor()\n",
    "    sp.load('lang_model.model')\n",
    "\n",
    "    def encode_as_bow(tweets, vocab_size):\n",
    "        rows, cols, data = [], [], []\n",
    "        for i, tweet in enumerate(tweets):\n",
    "            ids = sp.encode(tweet, out_type=int)\n",
    "            for idx in ids:\n",
    "                rows.append(i)\n",
    "                cols.append(idx)\n",
    "                data.append(1)\n",
    "        return csr_matrix((data, (rows, cols)), shape=(len(tweets), vocab_size))\n",
    "\n",
    "    X_yor_train = encode_as_bow(X_train, vocab_size=8000)\n",
    "    X_yor_test = encode_as_bow(X_test, vocab_size=8000)\n",
    "\n",
    "    clf = MultinomialNB()\n",
    "    clf.fit(X_yor_train, y_train)\n",
    "\n",
    "    y_pred = clf.predict(X_yor_test)\n",
    "\n",
    "    spm_result: dict = classification_report(y_test, y_pred, output_dict=True)\n",
    "    # loss = log_loss(y_test, y_pred)\n",
    "    # print(f\"Loss for {lang} Language: {loss:.3f}\")\n",
    "    \n",
    "    compare_results(word_result, spm_result)\n",
    "    delete_files()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "3f7e890b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jesse\\Dev\\cos760-nlp-project\\.venv\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for yor Language:\n",
      "Normal Tokenization Results:\n",
      "              precision  recall  f1-score   support\n",
      "negative          0.649   0.542     0.591   981.000\n",
      "neutral           0.719   0.701     0.710  1616.000\n",
      "positive          0.742   0.820     0.779  1918.000\n",
      "accuracy          0.717   0.717     0.717     0.717\n",
      "macro avg         0.703   0.688     0.693  4515.000\n",
      "weighted avg      0.714   0.717     0.713  4515.000\n",
      "--------------------------------------------------\n",
      "Subword Tokenization Results:\n",
      "              precision  recall  f1-score   support\n",
      "negative          0.487   0.715     0.579   981.000\n",
      "neutral           0.748   0.533     0.623  1616.000\n",
      "positive          0.745   0.746     0.745  1918.000\n",
      "accuracy          0.663   0.663     0.663     0.663\n",
      "macro avg         0.660   0.665     0.649  4515.000\n",
      "weighted avg      0.690   0.663     0.665  4515.000\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jesse\\Dev\\cos760-nlp-project\\.venv\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for hau Language:\n",
      "Normal Tokenization Results:\n",
      "              precision  recall  f1-score   support\n",
      "negative          0.580   0.819     0.679  1759.000\n",
      "neutral           0.702   0.434     0.537  1789.000\n",
      "positive          0.789   0.770     0.779  1755.000\n",
      "accuracy          0.673   0.673     0.673     0.673\n",
      "macro avg         0.690   0.674     0.665  5303.000\n",
      "weighted avg      0.690   0.673     0.664  5303.000\n",
      "--------------------------------------------------\n",
      "Subword Tokenization Results:\n",
      "              precision  recall  f1-score   support\n",
      "negative          0.549   0.873     0.674  1759.000\n",
      "neutral           0.736   0.394     0.513  1789.000\n",
      "positive          0.852   0.752     0.799  1755.000\n",
      "accuracy          0.671   0.671     0.671     0.671\n",
      "macro avg         0.712   0.673     0.662  5303.000\n",
      "weighted avg      0.712   0.671     0.661  5303.000\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jesse\\Dev\\cos760-nlp-project\\.venv\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for ibo Language:\n",
      "Normal Tokenization Results:\n",
      "              precision  recall  f1-score   support\n",
      "negative          0.769   0.561     0.649   943.000\n",
      "neutral           0.682   0.813     0.742  1621.000\n",
      "positive          0.767   0.728     0.747  1118.000\n",
      "accuracy          0.723   0.723     0.723     0.723\n",
      "macro avg         0.739   0.701     0.713  3682.000\n",
      "weighted avg      0.730   0.723     0.720  3682.000\n",
      "--------------------------------------------------\n",
      "Subword Tokenization Results:\n",
      "              precision  recall  f1-score   support\n",
      "negative          0.795   0.509     0.621   943.000\n",
      "neutral           0.656   0.903     0.760  1621.000\n",
      "positive          0.849   0.644     0.732  1118.000\n",
      "accuracy          0.723   0.723     0.723     0.723\n",
      "macro avg         0.767   0.685     0.704  3682.000\n",
      "weighted avg      0.750   0.723     0.716  3682.000\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jesse\\Dev\\cos760-nlp-project\\.venv\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "c:\\Users\\jesse\\Dev\\cos760-nlp-project\\.venv\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:402: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [',', 'tins'] not in stop_words.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jesse\\Dev\\cos760-nlp-project\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\jesse\\Dev\\cos760-nlp-project\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\jesse\\Dev\\cos760-nlp-project\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for pcm Language:\n",
      "Normal Tokenization Results:\n",
      "              precision  recall  f1-score   support\n",
      "negative          0.632   0.929     0.753  2326.000\n",
      "neutral           0.000   0.000     0.000   431.000\n",
      "positive          0.703   0.369     0.484  1397.000\n",
      "accuracy          0.645   0.645     0.645     0.645\n",
      "macro avg         0.445   0.433     0.412  4154.000\n",
      "weighted avg      0.590   0.645     0.584  4154.000\n",
      "--------------------------------------------------\n",
      "Subword Tokenization Results:\n",
      "              precision  recall  f1-score   support\n",
      "negative          0.639   0.899     0.747  2326.000\n",
      "neutral           0.000   0.000     0.000   431.000\n",
      "positive          0.648   0.408     0.501  1397.000\n",
      "accuracy          0.641   0.641     0.641     0.641\n",
      "macro avg         0.429   0.436     0.416  4154.000\n",
      "weighted avg      0.576   0.641     0.587  4154.000\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "analyze_text(Languages.YORUBA)\n",
    "analyze_text(Languages.HAUSA)\n",
    "analyze_text(Languages.IGBO)\n",
    "analyze_text(Languages.NIGERIAN_PIDGIN)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
