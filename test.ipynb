{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "b7256dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, log_loss\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import sentencepiece as spm\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "e19fc9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_reader import MultiLangDataset, SplitSet\n",
    "from data_reader import ns_dataset, as_dataset\n",
    "from data_reader import Languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "59edfa13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jesse\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "adb8a089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['negative' 'neutral' 'positive']\n"
     ]
    }
   ],
   "source": [
    "yor_dataset: SplitSet = ns_dataset.get(Languages.YORUBA)\n",
    "test = yor_dataset.train[\"label\"]\n",
    "print(test.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "5c0b58d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Custom Transformer using SentencePiece\n",
    "class SentencePieceVectorizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, model_file='yoruba.model', vocab_size=200):\n",
    "        self.model_file = model_file\n",
    "        self.vocab_size = vocab_size\n",
    "        self.sp = spm.SentencePieceProcessor()\n",
    "        self.sp.load(self.model_file)\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        rows, cols, data = [], [], []\n",
    "        for i, text in enumerate(X):\n",
    "            ids = self.sp.encode(text, out_type=int)\n",
    "            for idx in ids:\n",
    "                rows.append(i)\n",
    "                cols.append(idx)\n",
    "                data.append(1)\n",
    "        return csr_matrix((data, (rows, cols)), shape=(len(X), self.vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a57877",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def analyze_text(lang: Languages):\n",
    "\n",
    "    def compare_results(normal_result: dict, subword_result: dict):\n",
    "        # Convert to DataFrames\n",
    "        df = pd.DataFrame(normal_result).transpose()\n",
    "        subword_df = pd.DataFrame(subword_result).transpose()\n",
    "\n",
    "        print(f'Results for {lang} Language:')\n",
    "        print(\"Normal Tokenization Results:\")\n",
    "        print(df.round(3))\n",
    "        print(\"--------------------------------------------------\")\n",
    "        print(\"Subword Tokenization Results:\")\n",
    "        print(subword_df.round(3))\n",
    "        print(\"--------------------------------------------------\")\n",
    "\n",
    "    def delete_files():\n",
    "        import os\n",
    "        for filename in ['tweets.txt', 'lang_model.model', 'lang_model.vocab']:\n",
    "            try:\n",
    "                os.remove(filename)\n",
    "            except FileNotFoundError:\n",
    "                pass\n",
    "\n",
    "    lang_dataset: SplitSet = ns_dataset.get(lang)\n",
    "    X_train = lang_dataset.train[\"tweet\"]\n",
    "    lang_dataset.train[\"tweet\"].to_csv('tweets.txt', index=False, header=False)\n",
    "    y_train = lang_dataset.train[\"label\"]\n",
    "\n",
    "    X_test = lang_dataset.test[\"tweet\"]\n",
    "    y_test = lang_dataset.test[\"label\"]\n",
    "    stop_words = lang_dataset.stopwords\n",
    "\n",
    "    # Peform word tokenization\n",
    "    vectorizer = CountVectorizer(tokenizer=word_tokenize,stop_words=stop_words)\n",
    "    model = Pipeline([\n",
    "        ('vectorizer', vectorizer),  # word-level tokenizer by default\n",
    "        ('classifier', MultinomialNB())\n",
    "    ])\n",
    "    # Train model\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    word_result: dict = classification_report(y_test, y_pred, output_dict=True)\n",
    "\n",
    "\n",
    "    # SentencePiece tokenizer\n",
    "    spm.SentencePieceTrainer.Train(input='tweets.txt', model_prefix='lang_model', vocab_size=8000, model_type='bpe')\n",
    "    sp = spm.SentencePieceProcessor()\n",
    "    sp.load('lang_model.model')\n",
    "\n",
    "    def encode_as_bow(tweets, vocab_size):\n",
    "        rows, cols, data = [], [], []\n",
    "        for i, tweet in enumerate(tweets):\n",
    "            ids = sp.encode(tweet, out_type=int)\n",
    "            for idx in ids:\n",
    "                rows.append(i)\n",
    "                cols.append(idx)\n",
    "                data.append(1)\n",
    "        return csr_matrix((data, (rows, cols)), shape=(len(tweets), vocab_size))\n",
    "\n",
    "    X_yor_train = encode_as_bow(X_train, vocab_size=8000)\n",
    "    X_yor_test = encode_as_bow(X_test, vocab_size=8000)\n",
    "\n",
    "    clf = MultinomialNB()\n",
    "    clf.fit(X_yor_train, y_train)\n",
    "\n",
    "    y_pred = clf.predict(X_yor_test)\n",
    "\n",
    "    spm_result: dict = classification_report(y_test, y_pred, output_dict=True)\n",
    "    # loss = log_loss(y_test, y_pred)\n",
    "    # print(f\"Loss for {lang} Language: {loss:.3f}\")\n",
    "    \n",
    "    compare_results(word_result, spm_result)\n",
    "    delete_files()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "3f7e890b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jesse\\Dev\\cos760-nlp-project\\.venv\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: np.str_('positive')",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[151]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43manalyze_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mLanguages\u001b[49m\u001b[43m.\u001b[49m\u001b[43mYORUBA\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m analyze_text(Languages.HAUSA)\n\u001b[32m      3\u001b[39m analyze_text(Languages.IGBO)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[150]\u001b[39m\u001b[32m, line 70\u001b[39m, in \u001b[36manalyze_text\u001b[39m\u001b[34m(lang)\u001b[39m\n\u001b[32m     67\u001b[39m y_pred = clf.predict(X_yor_test)\n\u001b[32m     69\u001b[39m spm_result: \u001b[38;5;28mdict\u001b[39m = classification_report(y_test, y_pred, output_dict=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m loss = \u001b[43mlog_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[38;5;66;03m# print(f\"Loss for {lang} Language: {loss:.3f}\")\u001b[39;00m\n\u001b[32m     73\u001b[39m compare_results(word_result, spm_result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jesse\\Dev\\cos760-nlp-project\\.venv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:216\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    211\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    212\u001b[39m         skip_parameter_validation=(\n\u001b[32m    213\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    214\u001b[39m         )\n\u001b[32m    215\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    218\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    219\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    222\u001b[39m     msg = re.sub(\n\u001b[32m    223\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    224\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    225\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    226\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jesse\\Dev\\cos760-nlp-project\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:2956\u001b[39m, in \u001b[36mlog_loss\u001b[39m\u001b[34m(y_true, y_pred, normalize, sample_weight, labels)\u001b[39m\n\u001b[32m   2878\u001b[39m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[32m   2879\u001b[39m     {\n\u001b[32m   2880\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33my_true\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[33m\"\u001b[39m\u001b[33marray-like\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m   2887\u001b[39m )\n\u001b[32m   2888\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlog_loss\u001b[39m(y_true, y_pred, *, normalize=\u001b[38;5;28;01mTrue\u001b[39;00m, sample_weight=\u001b[38;5;28;01mNone\u001b[39;00m, labels=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m   2889\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Log loss, aka logistic loss or cross-entropy loss.\u001b[39;00m\n\u001b[32m   2890\u001b[39m \n\u001b[32m   2891\u001b[39m \u001b[33;03m    This is the loss function used in (multinomial) logistic regression\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2954\u001b[39m \u001b[33;03m    0.21616...\u001b[39;00m\n\u001b[32m   2955\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2956\u001b[39m     y_pred = \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2957\u001b[39m \u001b[43m        \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat64\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat32\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat16\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m   2958\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2960\u001b[39m     check_consistent_length(y_pred, y_true, sample_weight)\n\u001b[32m   2961\u001b[39m     lb = LabelBinarizer()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jesse\\Dev\\cos760-nlp-project\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:1055\u001b[39m, in \u001b[36mcheck_array\u001b[39m\u001b[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[39m\n\u001b[32m   1053\u001b[39m         array = xp.astype(array, dtype, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1054\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1055\u001b[39m         array = \u001b[43m_asarray_with_order\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[43m=\u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1056\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[32m   1057\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1058\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.format(array)\n\u001b[32m   1059\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcomplex_warning\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jesse\\Dev\\cos760-nlp-project\\.venv\\Lib\\site-packages\\sklearn\\utils\\_array_api.py:839\u001b[39m, in \u001b[36m_asarray_with_order\u001b[39m\u001b[34m(array, dtype, order, copy, xp, device)\u001b[39m\n\u001b[32m    837\u001b[39m     array = numpy.array(array, order=order, dtype=dtype)\n\u001b[32m    838\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m839\u001b[39m     array = \u001b[43mnumpy\u001b[49m\u001b[43m.\u001b[49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[43m=\u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    841\u001b[39m \u001b[38;5;66;03m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[32m    842\u001b[39m \u001b[38;5;66;03m# container that is consistent with the input's namespace.\u001b[39;00m\n\u001b[32m    843\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m xp.asarray(array)\n",
      "\u001b[31mValueError\u001b[39m: could not convert string to float: np.str_('positive')"
     ]
    }
   ],
   "source": [
    "analyze_text(Languages.YORUBA)\n",
    "analyze_text(Languages.HAUSA)\n",
    "analyze_text(Languages.IGBO)\n",
    "analyze_text(Languages.NIGERIAN_PIDGIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c948bc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# yor_dataset: SplitSet = ns_dataset.get(Languages.YORUBA)\n",
    "# X_train = yor_dataset.train[\"tweet\"]\n",
    "# yor_dataset.train[\"tweet\"].to_csv('tweets.txt', index=False, header=False)\n",
    "# y_train = yor_dataset.train[\"label\"]\n",
    "\n",
    "# X_test = yor_dataset.test[\"tweet\"]\n",
    "# y_test = yor_dataset.test[\"label\"]\n",
    "# stop_words = yor_dataset.stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30a2e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the SentencePiece model\n",
    "# spm.SentencePieceTrainer.Train(input='tweets.txt', model_prefix='yoruba', vocab_size=8000, model_type='bpe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5f0a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sp = spm.SentencePieceProcessor()\n",
    "# sp.load('yoruba.model')\n",
    "\n",
    "# def encode_as_bow(tweets, vocab_size):\n",
    "#     rows, cols, data = [], [], []\n",
    "#     for i, tweet in enumerate(tweets):\n",
    "#         ids = sp.encode(tweet, out_type=int)\n",
    "#         for idx in ids:\n",
    "#             rows.append(i)\n",
    "#             cols.append(idx)\n",
    "#             data.append(1)\n",
    "#     return csr_matrix((data, (rows, cols)), shape=(len(tweets), vocab_size))\n",
    "\n",
    "# X_yor_train = encode_as_bow(X_train, vocab_size=8000)\n",
    "# X_yor_test = encode_as_bow(X_test, vocab_size=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e16f0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Build pipeline\n",
    "# pipeline = Pipeline([\n",
    "#     ('sp_vectorizer', SentencePieceVectorizer(model_file='yoruba.model', vocab_size=2000)),\n",
    "#     ('classifier', MultinomialNB())\n",
    "# ])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08dc39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline.fit(X_yor_train, y_train)\n",
    "# clf = MultinomialNB()\n",
    "# clf.fit(X_yor_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48656d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = clf.predict(X_yor_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fab1942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba955b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizer = CountVectorizer(tokenizer=word_tokenize,stop_words=stop_words)\n",
    "# model = Pipeline([\n",
    "#     ('vectorizer', vectorizer),  # word-level tokenizer by default\n",
    "#     ('classifier', MultinomialNB())\n",
    "# ])\n",
    "\n",
    "# # Train model\n",
    "# model.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebac294",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Predict and evaluate\n",
    "# y_pred = model.predict(X_test)\n",
    "# print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
