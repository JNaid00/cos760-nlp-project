{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b7256dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import sentencepiece as spm\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e19fc9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_reader import MultiLangDataset, SplitSet\n",
    "from data_reader import ns_dataset, as_dataset\n",
    "from data_reader import Languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "59edfa13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jesse\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5c0b58d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Custom Transformer using SentencePiece\n",
    "class SentencePieceVectorizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, model_file='yoruba.model', vocab_size=200):\n",
    "        self.model_file = model_file\n",
    "        self.vocab_size = vocab_size\n",
    "        self.sp = spm.SentencePieceProcessor()\n",
    "        self.sp.load(self.model_file)\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        rows, cols, data = [], [], []\n",
    "        for i, text in enumerate(X):\n",
    "            ids = self.sp.encode(text, out_type=int)\n",
    "            for idx in ids:\n",
    "                rows.append(i)\n",
    "                cols.append(idx)\n",
    "                data.append(1)\n",
    "        return csr_matrix((data, (rows, cols)), shape=(len(X), self.vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "77a57877",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_text(lang: Languages):\n",
    "\n",
    "    def compare_results(normal_result: dict, subword_result: dict):\n",
    "        # Convert to DataFrames\n",
    "        df = pd.DataFrame(normal_result).transpose()\n",
    "        subword_df = pd.DataFrame(subword_result).transpose()\n",
    "\n",
    "        print(f'Results for {lang} Language:')\n",
    "        print(\"Normal Tokenization Results:\")\n",
    "        print(df.round(3))\n",
    "        print(\"--------------------------------------------------\")\n",
    "        print(\"Subword Tokenization Results:\")\n",
    "        print(subword_df.round(3))\n",
    "        print(\"--------------------------------------------------\")\n",
    "\n",
    "    lang_dataset: SplitSet = ns_dataset.get(lang)\n",
    "    X_train = lang_dataset.train[\"tweet\"]\n",
    "    lang_dataset.train[\"tweet\"].to_csv('tweets.txt', index=False, header=False)\n",
    "    y_train = lang_dataset.train[\"label\"]\n",
    "\n",
    "    X_test = lang_dataset.test[\"tweet\"]\n",
    "    y_test = lang_dataset.test[\"label\"]\n",
    "    stop_words = lang_dataset.stopwords\n",
    "\n",
    "    # Peform word tokenization\n",
    "    vectorizer = CountVectorizer(tokenizer=word_tokenize,stop_words=stop_words)\n",
    "    model = Pipeline([\n",
    "        ('vectorizer', vectorizer),  # word-level tokenizer by default\n",
    "        ('classifier', MultinomialNB())\n",
    "    ])\n",
    "    # Train model\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    word_result: dict = classification_report(y_test, y_pred, output_dict=True)\n",
    "\n",
    "\n",
    "    # SentencePiece tokenizer\n",
    "    spm.SentencePieceTrainer.Train(input='tweets.txt', model_prefix='lang_model', vocab_size=8000, model_type='bpe')\n",
    "    sp = spm.SentencePieceProcessor()\n",
    "    sp.load('lang_model.model')\n",
    "\n",
    "    def encode_as_bow(tweets, vocab_size):\n",
    "        rows, cols, data = [], [], []\n",
    "        for i, tweet in enumerate(tweets):\n",
    "            ids = sp.encode(tweet, out_type=int)\n",
    "            for idx in ids:\n",
    "                rows.append(i)\n",
    "                cols.append(idx)\n",
    "                data.append(1)\n",
    "        return csr_matrix((data, (rows, cols)), shape=(len(tweets), vocab_size))\n",
    "\n",
    "    X_yor_train = encode_as_bow(X_train, vocab_size=8000)\n",
    "    X_yor_test = encode_as_bow(X_test, vocab_size=8000)\n",
    "\n",
    "    clf = MultinomialNB()\n",
    "    clf.fit(X_yor_train, y_train)\n",
    "\n",
    "    y_pred = clf.predict(X_yor_test)\n",
    "\n",
    "    spm_result: dict = classification_report(y_test, y_pred, output_dict=True)\n",
    "\n",
    "    compare_results(word_result, spm_result)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3f7e890b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jesse\\Dev\\cos760-nlp-project\\.venv\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for yor Language:\n",
      "Normal Tokenization Results:\n",
      "              precision  recall  f1-score   support\n",
      "negative          0.649   0.542     0.591   981.000\n",
      "neutral           0.719   0.701     0.710  1616.000\n",
      "positive          0.742   0.820     0.779  1918.000\n",
      "accuracy          0.717   0.717     0.717     0.717\n",
      "macro avg         0.703   0.688     0.693  4515.000\n",
      "weighted avg      0.714   0.717     0.713  4515.000\n",
      "--------------------------------------------------\n",
      "Subword Tokenization Results:\n",
      "              precision  recall  f1-score   support\n",
      "negative          0.487   0.715     0.579   981.000\n",
      "neutral           0.748   0.533     0.623  1616.000\n",
      "positive          0.745   0.746     0.745  1918.000\n",
      "accuracy          0.663   0.663     0.663     0.663\n",
      "macro avg         0.660   0.665     0.649  4515.000\n",
      "weighted avg      0.690   0.663     0.665  4515.000\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "analyze_text(Languages.YORUBA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c948bc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# yor_dataset: SplitSet = ns_dataset.get(Languages.YORUBA)\n",
    "# X_train = yor_dataset.train[\"tweet\"]\n",
    "# yor_dataset.train[\"tweet\"].to_csv('tweets.txt', index=False, header=False)\n",
    "# y_train = yor_dataset.train[\"label\"]\n",
    "\n",
    "# X_test = yor_dataset.test[\"tweet\"]\n",
    "# y_test = yor_dataset.test[\"label\"]\n",
    "# stop_words = yor_dataset.stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a30a2e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the SentencePiece model\n",
    "# spm.SentencePieceTrainer.Train(input='tweets.txt', model_prefix='yoruba', vocab_size=8000, model_type='bpe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "8b5f0a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sp = spm.SentencePieceProcessor()\n",
    "# sp.load('yoruba.model')\n",
    "\n",
    "# def encode_as_bow(tweets, vocab_size):\n",
    "#     rows, cols, data = [], [], []\n",
    "#     for i, tweet in enumerate(tweets):\n",
    "#         ids = sp.encode(tweet, out_type=int)\n",
    "#         for idx in ids:\n",
    "#             rows.append(i)\n",
    "#             cols.append(idx)\n",
    "#             data.append(1)\n",
    "#     return csr_matrix((data, (rows, cols)), shape=(len(tweets), vocab_size))\n",
    "\n",
    "# X_yor_train = encode_as_bow(X_train, vocab_size=8000)\n",
    "# X_yor_test = encode_as_bow(X_test, vocab_size=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "0e16f0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Build pipeline\n",
    "# pipeline = Pipeline([\n",
    "#     ('sp_vectorizer', SentencePieceVectorizer(model_file='yoruba.model', vocab_size=2000)),\n",
    "#     ('classifier', MultinomialNB())\n",
    "# ])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e08dc39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline.fit(X_yor_train, y_train)\n",
    "# clf = MultinomialNB()\n",
    "# clf.fit(X_yor_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "48656d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = clf.predict(X_yor_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fab1942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ba955b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizer = CountVectorizer(tokenizer=word_tokenize,stop_words=stop_words)\n",
    "# model = Pipeline([\n",
    "#     ('vectorizer', vectorizer),  # word-level tokenizer by default\n",
    "#     ('classifier', MultinomialNB())\n",
    "# ])\n",
    "\n",
    "# # Train model\n",
    "# model.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "6ebac294",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Predict and evaluate\n",
    "# y_pred = model.predict(X_test)\n",
    "# print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
