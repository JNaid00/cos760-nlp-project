{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7256dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, log_loss\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import sentencepiece as spm\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19fc9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_reader import MultiLangDataset, SplitSet\n",
    "from data_reader import ns_dataset, as_dataset\n",
    "from data_reader import Languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59edfa13",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0b58d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Custom Transformer using SentencePiece\n",
    "class SentencePieceVectorizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, model_file='yoruba.model', vocab_size=200):\n",
    "        self.model_file = model_file\n",
    "        self.vocab_size = vocab_size\n",
    "        self.sp = spm.SentencePieceProcessor()\n",
    "        self.sp.load(self.model_file)\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        rows, cols, data = [], [], []\n",
    "        for i, text in enumerate(X):\n",
    "            ids = self.sp.encode(text, out_type=int)\n",
    "            for idx in ids:\n",
    "                rows.append(i)\n",
    "                cols.append(idx)\n",
    "                data.append(1)\n",
    "        return csr_matrix((data, (rows, cols)), shape=(len(X), self.vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a57877",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_text(lang: Languages):\n",
    "\n",
    "    def compare_results(normal_result: dict, subword_result: dict):\n",
    "        print(normal_result)\n",
    "        print(subword_result)\n",
    "        # Convert to DataFrames\n",
    "        df = pd.DataFrame(normal_result).transpose()\n",
    "        subword_df = pd.DataFrame(subword_result).transpose()\n",
    "\n",
    "        print(f'Results for {lang} Language:\\n')\n",
    "        print(\"Normal Tokenization Results:\")\n",
    "        print(df.round(3))\n",
    "        print(\"--------------------------------------------------\")\n",
    "        print(\"Subword Tokenization Results:\")\n",
    "        print(subword_df.round(3))\n",
    "        print(\"--------------------------------------------------\")\n",
    "\n",
    "        labels = ['negative', 'neutral', 'positive']\n",
    "        metrics = ['precision', 'recall', 'f1-score']\n",
    "        data = []\n",
    "        for label in labels:\n",
    "            row = {'class': label}\n",
    "            for metric in metrics:\n",
    "                n_val = normal_result[label][metric]\n",
    "                s_val = subword_result[label][metric]\n",
    "                row[f'normal_{metric}'] = n_val\n",
    "                row[f'subword_{metric}'] = s_val\n",
    "                row[f'diff_{metric}'] = s_val - n_val\n",
    "            data.append(row)\n",
    "\n",
    "        df = pd.DataFrame(data)\n",
    "\n",
    "                # Add macro and weighted averages\n",
    "        for avg_type in ['macro avg', 'weighted avg']:\n",
    "            row = {'class': avg_type}\n",
    "            for metric in metrics:\n",
    "                n_val = normal_result[avg_type][metric]\n",
    "                s_val = subword_result[avg_type][metric]\n",
    "                row[f'normal_{metric}'] = n_val\n",
    "                row[f'subword_{metric}'] = s_val\n",
    "                row[f'diff_{metric}'] = s_val - n_val\n",
    "            df = pd.concat([df, pd.DataFrame([row])], ignore_index=True)\n",
    "\n",
    "        # Add accuracy row\n",
    "        df_accuracy = pd.DataFrame([{\n",
    "            'class': 'accuracy',\n",
    "            'normal_precision': normal_result['accuracy'],\n",
    "            'subword_precision': subword_result['accuracy'],\n",
    "            'diff_precision': subword_result['accuracy'] - normal_result['accuracy']\n",
    "        }])\n",
    "\n",
    "        print(\"\\nüìä Summary of Results:\")\n",
    "\n",
    "        # Final display\n",
    "        print(\"\\nüîç Per-Class Metric Comparison:\")\n",
    "        print(df.to_string(index=False))\n",
    "\n",
    "        print(\"\\n‚úÖ Accuracy Comparison:\")\n",
    "        print(df_accuracy.to_string(index=False))\n",
    "\n",
    "        # End print\n",
    "        print(\"\\nAnalysis complete.\")\n",
    "\n",
    "        print(\"\\n--------------------------------------------------\")\n",
    "        print(\"--------------------------------------------------\\n\")\n",
    "\n",
    "        \n",
    "    def delete_files():\n",
    "        import os\n",
    "        for filename in ['tweets.txt', 'lang_model.model', 'lang_model.vocab']:\n",
    "            try:\n",
    "                os.remove(filename)\n",
    "            except FileNotFoundError:\n",
    "                pass\n",
    "\n",
    "    lang_dataset: SplitSet = ns_dataset.get(lang)\n",
    "    X_train = lang_dataset.train[\"tweet\"]\n",
    "    lang_dataset.train[\"tweet\"].to_csv('tweets.txt', index=False, header=False)\n",
    "    y_train = lang_dataset.train[\"label\"]\n",
    "\n",
    "    X_test = lang_dataset.test[\"tweet\"]\n",
    "    y_test = lang_dataset.test[\"label\"]\n",
    "    stop_words = lang_dataset.stopwords\n",
    "\n",
    "    # Peform word tokenization\n",
    "    vectorizer = CountVectorizer(tokenizer=word_tokenize,stop_words=stop_words)\n",
    "    model = Pipeline([\n",
    "        ('vectorizer', vectorizer),  # word-level tokenizer by default\n",
    "        ('classifier', MultinomialNB())\n",
    "    ])\n",
    "    # Train model\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    word_result: dict = classification_report(y_test, y_pred, output_dict=True)\n",
    "\n",
    "\n",
    "    # SentencePiece tokenizer\n",
    "    spm.SentencePieceTrainer.Train(input='tweets.txt', model_prefix='lang_model', vocab_size=8000, model_type='bpe')\n",
    "    sp = spm.SentencePieceProcessor()\n",
    "    sp.load('lang_model.model')\n",
    "\n",
    "    def encode_as_bow(tweets, vocab_size):\n",
    "        rows, cols, data = [], [], []\n",
    "        for i, tweet in enumerate(tweets):\n",
    "            ids = sp.encode(tweet, out_type=int)\n",
    "            for idx in ids:\n",
    "                rows.append(i)\n",
    "                cols.append(idx)\n",
    "                data.append(1)\n",
    "        return csr_matrix((data, (rows, cols)), shape=(len(tweets), vocab_size))\n",
    "\n",
    "    X_yor_train = encode_as_bow(X_train, vocab_size=8000)\n",
    "    X_yor_test = encode_as_bow(X_test, vocab_size=8000)\n",
    "\n",
    "    clf = MultinomialNB()\n",
    "    clf.fit(X_yor_train, y_train)\n",
    "\n",
    "    y_pred = clf.predict(X_yor_test)\n",
    "\n",
    "    spm_result: dict = classification_report(y_test, y_pred, output_dict=True)\n",
    "    # loss = log_loss(y_test, y_pred)\n",
    "    # print(f\"Loss for {lang} Language: {loss:.3f}\")\n",
    "    \n",
    "    compare_results(word_result, spm_result)\n",
    "    delete_files()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7e890b",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_text(Languages.YORUBA)\n",
    "# analyze_text(Languages.HAUSA)\n",
    "# analyze_text(Languages.IGBO)\n",
    "# analyze_text(Languages.NIGERIAN_PIDGIN)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
