{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232c4b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "from typing import Dict, List, Optional\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from tokenizers.models import WordPiece\n",
    "from tokenizers.trainers import WordPieceTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.normalizers import Sequence, Lowercase, NFD, StripAccents\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, GlobalAveragePooling1D, Dense, Dropout,Input, BatchNormalization, Activation\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from models import BasicModelEncapsulator, NeuralNetworkModel\n",
    "from custom_vectorizers import initialise_count_vectorizer, initialise_tfidf_vectorizer\n",
    "from constants import LOCAL_DIR_AS, LOCAL_DIR_NS, REPO_URL_AS, REPO_URL_NS, NS_LANGUAGES\n",
    "from custom_datasets import MultiLangDataset, load_local_datasets\n",
    "\n",
    "from custom_datasets import Languages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be9680c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clone_repo(repo_url: str, local_dir: str) -> None:\n",
    "    if os.path.isdir(local_dir):\n",
    "        print(\"Repository exists. Updating...\")\n",
    "        subprocess.run([\"git\", \"-C\", local_dir, \"pull\", \"origin\", \"main\"], check=True)\n",
    "    else:\n",
    "        print(\"Repository not found. Cloning...\")\n",
    "        subprocess.run([\"git\", \"clone\", repo_url], check=True)\n",
    "\n",
    "clone_repo(REPO_URL_NS, LOCAL_DIR_NS)\n",
    "clone_repo(REPO_URL_AS, LOCAL_DIR_AS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b5a129",
   "metadata": {},
   "outputs": [],
   "source": [
    "ns_dataset: MultiLangDataset = load_local_datasets(local_base_dir=LOCAL_DIR_NS + '/data/annotated_tweets', languages=NS_LANGUAGES) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947ce503",
   "metadata": {},
   "outputs": [],
   "source": [
    "as_dataset: MultiLangDataset = load_local_datasets(local_base_dir=f'afrisent-semeval-2023/data', languages=NS_LANGUAGES,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d6eb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"NaijaSenti dataset loaded with languages:\", ns_dataset.all_languages())\n",
    "print(\"Afrisenti dataset loaded with languages:\", as_dataset.all_languages())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d903e898",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(\"NaijaSenti hau: \", ns_dataset.get(Languages.HAUSA).test)\n",
    "# Print each row in the dev set for the column 'tweet'\n",
    "for index, row in ns_dataset.get(Languages.HAUSA).test.iterrows():\n",
    "    print(f\"Index: {index}, Tweet: {row['tweet']}\")\n",
    "\n",
    "# write all the tweets into a textfile\n",
    "# check if the dir data exists, if not create it\n",
    "if not os.path.exists('data'):\n",
    "    os.makedirs('data')\n",
    "with open('data/naija_senti_hau_dev_tweets.txt', 'w', encoding='utf-8') as f:\n",
    "    for index, row in ns_dataset.get(Languages.HAUSA).dev.iterrows():\n",
    "        f.write(f\"{row['tweet']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0078212",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ns_dataset.get(Languages.HAUSA).train\n",
    "text_train, text_test, y_train, y_test = train_test_split(df.tweet, df.label, test_size = 0.3)\n",
    "X_train_tfidf, vectorizer_tfidf = initialise_tfidf_vectorizer(text_train)\n",
    "X_train_count, vectorizer_count = initialise_count_vectorizer(text_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996f8d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_labels(df):\n",
    "    label_mapping = {'negative': 0, 'neutral': 1, 'positive': 2}\n",
    "    df['label_encoded'] = df['label'].str.lower().map(label_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1372129",
   "metadata": {},
   "outputs": [],
   "source": [
    "from subword_tokenizer import get_tokenizer, wordpiece_tokenize_dataframe\n",
    "class NeuralNetworkInput:\n",
    "    def __init__(self, X_train, y_train, X_test, y_test, num_classes, num_features):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "        self.num_classes = num_classes\n",
    "        self.num_features = num_features\n",
    "\n",
    "\n",
    "    def get_dense_X_train(self):\n",
    "        X_train = self.X_train.toarray()\n",
    "        return X_train.astype(np.float32)\n",
    "\n",
    "    def get_dense_X_test(self):\n",
    "        X_test = self.X_test.toarray()\n",
    "        return X_test.astype(np.float32)\n",
    "    def get_input_shape(self):\n",
    "        return (self.X_train.shape[1],)\n",
    "\n",
    "    def get_num_classes(self):\n",
    "        return self.num_classes\n",
    "\n",
    "    def get_num_features(self):\n",
    "        return self.num_features\n",
    "    \n",
    "def get_wordpiece_tokeized_data(df, vocab_size=None, vectorizer_kwargs: Optional[Dict] = None,tweet_column: str = \"tweet\") -> NeuralNetworkInput:\n",
    "    tokenizer = get_tokenizer(df=df, vocab_size=vocab_size, tweet_column=tweet_column)\n",
    "\n",
    "    train_df = df\n",
    "\n",
    "    # train_df = encode_labels(train_df)\n",
    "\n",
    "    # Naive Bayes with wordpiece tokenized data\n",
    "    wp_train_df = wordpiece_tokenize_dataframe(train_df, tokenizer, )\n",
    "\n",
    "    wp_X_train_list = wp_train_df['tokenized_tweets'].tolist()\n",
    "\n",
    "    # join sub lists into strings\n",
    "    wp_X_train_list = [' '.join(tokens) for tokens in wp_X_train_list]\n",
    "\n",
    "\n",
    "    wp_y_train = wp_train_df['label_encoded'].tolist()\n",
    "\n",
    "    if vectorizer_kwargs is None:\n",
    "        vectorizer_kwargs = {}\n",
    "    tfidf_wp_train, vectorizer_wp = initialise_tfidf_vectorizer(wp_X_train_list, **vectorizer_kwargs)\n",
    "\n",
    "\n",
    "    wp_tfidf_features = tfidf_wp_train.shape[1]  # Number of TF-IDF features\n",
    "    wp_num_classes = len(np.unique(wp_y_train)) \n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(tfidf_wp_train, wp_y_train, test_size=0.3, random_state=42)\n",
    "    return NeuralNetworkInput(X_train, y_train, X_test, y_test, wp_num_classes, wp_tfidf_features)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659b418a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a61a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    Input,\n",
    "    BatchNormalization,\n",
    "    Activation,\n",
    ")\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ec8985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab_list = [1000,1500, 2000,2250, 2500,2750, 3000, 4000, 5000,3000, 6000, 8000, 10000, 12000, 15000, 20000]\n",
    "vocab_accuracy = {}\n",
    "# for index in vocab_list:\n",
    "# for index in range(2000, 12000, 100):\n",
    "#     print(f\"Training model with vocabulary size: {index}\")\n",
    "#     df = ns_dataset.get(Languages.HAUSA).train\n",
    "#     encode_labels(df)\n",
    "#     neural_input = get_wordpiece_tokeized_data(df, vocab_size=index)\n",
    "\n",
    "#     model = Sequential()\n",
    "#     model.add(Input(shape=(neural_input.X_train.shape[1],))) \n",
    "\n",
    "#     # Dense layers for TF-IDF input\n",
    "#     model.add(Dense(256))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(Activation(\"relu\"))\n",
    "#     model.add(Dropout(0.4))\n",
    "#     model.add(Dense(128))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(Activation(\"relu\"))\n",
    "#     model.add(Dropout(0.3))\n",
    "#     model.add(Dense(64))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(Dense(3, activation=\"softmax\"))\n",
    "\n",
    "#     model.compile(\n",
    "#     loss=\"sparse_categorical_crossentropy\",\n",
    "#     optimizer=Adam(),\n",
    "#     metrics=[\"accuracy\"],\n",
    "#     )\n",
    "   \n",
    "#     X = np.array(neural_input.get_dense_X_train())\n",
    "#     y = np.array(neural_input.y_train)\n",
    "\n",
    "    \n",
    "#     model.fit(X, y, epochs=10, batch_size=32, verbose=0)\n",
    "\n",
    "#     # Evaluate the model\n",
    "# #model.evaluate(neural_input.X_test, neural_input.y_test)\n",
    "#     y_pred = model.predict(np.array(neural_input.get_dense_X_test()), )\n",
    "#     y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "#     accuracy = accuracy_score(neural_input.y_test, y_pred_classes)\n",
    "#     print(f\"Accuracy for vocabulary size {index}: {accuracy:.4f}\")\n",
    "#     vocab_accuracy[index] = accuracy\n",
    "#     print(classification_report(neural_input.y_test, y_pred_classes, target_names=['positive', 'neutral', 'negative']))\n",
    "#     print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75761dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the vocab_accuracy dictionary by keys (vocabulary size)\n",
    "\n",
    "print(\"Vocabulary size vs Accuracy:\")\n",
    "for vocab_size, accuracy in vocab_accuracy.items():\n",
    "    print(f\"Vocabulary Size: {vocab_size}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# save the vocab_accuracy dictionary to a file\n",
    "import json\n",
    "with open('vocab_accuracy.json', 'w') as f:\n",
    "    json.dump(vocab_accuracy, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00698774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizer_params = [{'ngram': (1,2), 'max_features': 5000},\n",
    "#     {'ngram': (1,3), 'max_features': 5000},\n",
    "#     {'ngram': (1,2), 'max_features': 10000},\n",
    "#     {'ngram': (1,3), 'max_features': 10000},\n",
    "#     {'ngram': (1,2), 'max_features': None},\n",
    "#     {'ngram': (1,3), 'max_features': None},\n",
    "#     {'ngram': (1,2), 'max_features': 2000},\n",
    "#     {'ngram': (1,3), 'max_features': 2000},\n",
    "#     {'ngram': (1,2), 'max_features': 3000},\n",
    "#     {'ngram': (1,3), 'max_features': 3000},\n",
    "#     {'ngram': (1,2), 'max_features': 4000},\n",
    "#     {'ngram': (1,3), 'max_features': 4000},\n",
    "#     {'ngram': (1,2), 'max_features': 6000},\n",
    "#     {'ngram': (1,3), 'max_features': 6000},\n",
    "#     {'ngram': (1,2), 'max_features': 7000},\n",
    "#     {'ngram': (1,3), 'max_features': 7000},\n",
    "#     {'ngram': (1,2), 'max_features': 8000},\n",
    "#     {'ngram': (1,3), 'max_features': 8000},\n",
    "#     {'ngram': (1,2), 'max_features': 9000},\n",
    "#     {'ngram': (1,3), 'max_features': 9000},\n",
    "#     {'ngram': (1,2), 'max_features': 10000},\n",
    "#     {'ngram': (1,3), 'max_features': 10000},\n",
    "#     {'ngram': (1,2), 'max_features': 12000},\n",
    "#     {'ngram': (1,4), 'max_features': 5000},\n",
    "#     {'ngram': (1,4), 'max_features': 10000},\n",
    "#     {'ngram': (1,4), 'max_features': None},\n",
    "#     {'ngram': (1,4), 'max_features': 2000},\n",
    "#     {'ngram': (1,4), 'max_features': 3000},\n",
    "#     {'ngram': (1,4), 'max_features': 4000},\n",
    "#     {'ngram': (1,4), 'max_features': 6000},\n",
    "#     {'ngram': (1,4), 'max_features': 7000},\n",
    "#     {'ngram': (1,4), 'max_features': 8000},\n",
    "#     {'ngram': (1,4), 'max_features': 9000},\n",
    "#     {'ngram': (1,4), 'max_features': 10000},\n",
    "#     {'ngram': (1,4), 'max_features': 12000},\n",
    "#     {'ngram': (2,5), 'max_features': 5000},\n",
    "#     {'ngram': (2,5), 'max_features': 10000},\n",
    "#     {'ngram': (2,5), 'max_features': None},\n",
    "#     {'ngram': (2,5), 'max_features': 2000},\n",
    "#     {'ngram': (2,5), 'max_features': 3000},\n",
    "#     {'ngram': (2,5), 'max_features': 4000},\n",
    "#     {'ngram': (2,5), 'max_features': 6000},\n",
    "#     {'ngram': (2,5), 'max_features': 7000},\n",
    "#     {'ngram': (2,5), 'max_features': 8000},\n",
    "#     {'ngram': (2,5), 'max_features': 9000},\n",
    "#     {'ngram': (2,5), 'max_features': 10000},\n",
    "#     {'ngram': (2,5), 'max_features': 12000},\n",
    "#     {'ngram': (3,5), 'max_features': 5000},\n",
    "#     {'ngram': (3,5), 'max_features': 10000},\n",
    "#     {'ngram': (3,5), 'max_features': None},\n",
    "#     {'ngram': (3,5), 'max_features': 2000},\n",
    "#     {'ngram': (3,5), 'max_features': 3000},\n",
    "#     {'ngram': (3,5), 'max_features': 4000},\n",
    "#     {'ngram': (3,5), 'max_features': 6000},\n",
    "#     {'ngram': (3,5), 'max_features': 7000},\n",
    "#     {'ngram': (3,5), 'max_features': 8000},\n",
    "#     {'ngram': (3,5), 'max_features': 9000},\n",
    "#     {'ngram': (3,5), 'max_features': 10000},\n",
    "#     {'ngram': (3,5), 'max_features': 12000}]\n",
    "# ngram = (1, 2) and max features adjustments\n",
    "vectorizer_params = [\n",
    "    {'ngram': (1, 2), 'max_features': 5000},\n",
    "    {'ngram': (1, 2), 'max_features': 10000},\n",
    "    {'ngram': (1, 2), 'max_features': None},\n",
    "    {'ngram': (1, 2), 'max_features': 2000},\n",
    "    {'ngram': (1, 2), 'max_features': 3000},\n",
    "    {'ngram': (1, 2), 'max_features': 4000},\n",
    "    {'ngram': (1, 2), 'max_features': 6000},\n",
    "    {'ngram': (1, 2), 'max_features': 7000},\n",
    "    {'ngram': (1, 2), 'max_features': 8000},\n",
    "    {'ngram': (1, 2), 'max_features': 9000},\n",
    "    {'ngram': (1, 2), 'max_features': 10000},\n",
    "    {'ngram': (1, 2), 'max_features': 12000},\n",
    "    {'ngram': (1, 2), 'max_features': 15000},\n",
    "    {'ngram': (1, 2), 'max_features': 20000},\n",
    "    {'ngram': (1, 2), 'max_features': 500},\n",
    "    {'ngram': (1, 2), 'max_features': 1000},\n",
    "    {'ngram': (1, 2), 'max_features': 1500},\n",
    "    {'ngram': (1, 2), 'max_features': 2500},\n",
    "    {'ngram': (1, 2), 'max_features': 3000},\n",
    "    {'ngram': (1, 2), 'max_features': 4000},\n",
    "]\n",
    "vocab_accuracy = {}\n",
    "\n",
    "\n",
    "# for index in vocab_list:\n",
    "# for index in vectorizer_params:\n",
    "#     print(f\"Training model with vocabulary size: {index}\")\n",
    "#     df = ns_dataset.get(Languages.HAUSA).train\n",
    "#     encode_labels(df)\n",
    "#     neural_input = get_wordpiece_tokeized_data(df, vocab_size=8000, vectorizer_kwargs=index)\n",
    "\n",
    "#     model = Sequential()\n",
    "#     model.add(Input(shape=(neural_input.X_train.shape[1],))) \n",
    "\n",
    "#     # Dense layers for TF-IDF input\n",
    "#     model.add(Dense(256))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(Activation(\"relu\"))\n",
    "#     model.add(Dropout(0.4))\n",
    "#     model.add(Dense(128))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(Activation(\"relu\"))\n",
    "#     model.add(Dropout(0.3))\n",
    "#     model.add(Dense(64))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(Dense(3, activation=\"softmax\"))\n",
    "\n",
    "#     model.compile(\n",
    "#     loss=\"sparse_categorical_crossentropy\",\n",
    "#     optimizer=Adam(),\n",
    "#     metrics=[\"accuracy\"],\n",
    "#     )\n",
    "   \n",
    "#     X = np.array(neural_input.get_dense_X_train())\n",
    "#     y = np.array(neural_input.y_train)\n",
    "\n",
    "    \n",
    "#     model.fit(X, y, epochs=10, batch_size=32, verbose=0)\n",
    "\n",
    "#     # Evaluate the model\n",
    "# #model.evaluate(neural_input.X_test, neural_input.y_test)\n",
    "#     y_pred = model.predict(np.array(neural_input.get_dense_X_test()), )\n",
    "#     y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "#     accuracy = accuracy_score(neural_input.y_test, y_pred_classes)\n",
    "#     print(f\"Accuracy for vocabulary size {index}: {accuracy:.4f}\")\n",
    "#     vocab_accuracy[str(index)] = accuracy\n",
    "#     print(classification_report(neural_input.y_test, y_pred_classes, target_names=['positive', 'neutral', 'negative']))\n",
    "#     print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e888c5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Vocabulary size vs Accuracy:\")\n",
    "for vocab_size, accuracy in vocab_accuracy.items():\n",
    "    print(f\"Vocabulary Size: {vocab_size}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# save the vocab_accuracy dictionary to a file\n",
    "import json\n",
    "with open('vectorizer_params_accuracy.json', 'w') as f:\n",
    "    json.dump(vocab_accuracy, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4edc234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense layers size tuple list (3 items each)\n",
    "dense_layer_sizes = [\n",
    "    (256, 128, 64),\n",
    "    (512, 256, 128),\n",
    "    (1024, 512, 256),\n",
    "    (128, 64, 32),\n",
    "    (64, 32, 16),\n",
    "    (32, 16, 8),\n",
    "    (16, 8, 4),\n",
    "    (8, 4, 2),\n",
    "    (256, 256, 128),\n",
    "    (512, 512, 256),\n",
    "    (1024, 1024, 512),\n",
    "    (128, 128, 64),\n",
    "    (64, 64, 32),\n",
    "    (32, 32, 16),\n",
    "    (16, 16, 8),\n",
    "    (8, 8, 4),\n",
    "    (256, 128, 64),\n",
    "    (512, 256, 128),\n",
    "    (1024, 512, 256),\n",
    "    (2048, 1024, 512),\n",
    "    \n",
    "]\n",
    "dense_layer_accuracy = {}\n",
    "\n",
    "# for index in vocab_list:\n",
    "# for index in dense_layer_sizes:\n",
    "#     print(f\"Training model with vocabulary size: {index}\")\n",
    "#     df = ns_dataset.get(Languages.HAUSA).train\n",
    "#     encode_labels(df)\n",
    "#     neural_input = get_wordpiece_tokeized_data(\n",
    "#         df, vocab_size=8000,\n",
    "#     )\n",
    "\n",
    "#     model = Sequential()\n",
    "#     model.add(Input(shape=(neural_input.X_train.shape[1],)))\n",
    "\n",
    "#     # Dense layers for TF-IDF input\n",
    "#     model.add(Dense(index[0]))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(Activation(\"relu\"))\n",
    "#     model.add(Dropout(0.4))\n",
    "#     model.add(Dense(index[1]))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(Activation(\"relu\"))\n",
    "#     model.add(Dropout(0.3))\n",
    "#     model.add(Dense(index[2]))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(Dense(3, activation=\"softmax\"))\n",
    "\n",
    "#     model.compile(\n",
    "#         loss=\"sparse_categorical_crossentropy\",\n",
    "#         optimizer=Adam(),\n",
    "#         metrics=[\"accuracy\"],\n",
    "#     )\n",
    "\n",
    "#     X = np.array(neural_input.get_dense_X_train())\n",
    "#     y = np.array(neural_input.y_train)\n",
    "\n",
    "#     model.fit(X, y, epochs=10, batch_size=32, verbose=0)\n",
    "\n",
    "#     # Evaluate the model\n",
    "#     # model.evaluate(neural_input.X_test, neural_input.y_test)\n",
    "#     y_pred = model.predict(\n",
    "#         np.array(neural_input.get_dense_X_test()),\n",
    "#     )\n",
    "#     y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "#     accuracy = accuracy_score(neural_input.y_test, y_pred_classes)\n",
    "#     print(f\"Accuracy for vocabulary size {index}: {accuracy:.4f}\")\n",
    "#     dense_layer_accuracy[str(index)] = accuracy\n",
    "#     print(\n",
    "#         classification_report(\n",
    "#             neural_input.y_test,\n",
    "#             y_pred_classes,\n",
    "#             target_names=[\"positive\", \"neutral\", \"negative\"],\n",
    "#         )\n",
    "#     )\n",
    "#     print(\"\\n\" + \"=\" * 50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498ee374",
   "metadata": {},
   "outputs": [],
   "source": [
    "for size, accuracy in dense_layer_accuracy.items():\n",
    "    print(f\"Dense layer Size: {size}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# save the vocab_accuracy dictionary to a file\n",
    "import json\n",
    "with open('dense_layer_accuracy.json', 'w') as f:\n",
    "    json.dump(dense_layer_accuracy, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5fbf92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try cleaned tweets with wordpiece tokenization\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "\n",
    "def clean_tweet(tweet):\n",
    "    \"\"\"\n",
    "    Clean tweet by replacing punctuation, emojis, and @mentions with whitespaces\n",
    "    \"\"\"\n",
    "    if pd.isna(tweet):\n",
    "        return tweet\n",
    "    # print (f\"Original Tweet: {tweet}\")\n",
    "    # Convert to string in case of mixed types\n",
    "    tweet = str(tweet)\n",
    "\n",
    "    # Remove @mentions (replace with space)\n",
    "    tweet = re.sub(r\"@\\w+\", \" \", tweet)\n",
    "\n",
    "    # Remove punctuation (replace with space)\n",
    "    tweet = re.sub(r\"[^\\w\\s]\", \" \", tweet)\n",
    "\n",
    "    # Remove emojis (replace with space)\n",
    "    # This regex matches most Unicode emoji ranges\n",
    "    emoji_pattern = re.compile(\n",
    "        \"[\"\n",
    "        \"\\U0001f600-\\U0001f64f\"  # emoticons\n",
    "        \"\\U0001f300-\\U0001f5ff\"  # symbols & pictographs\n",
    "        \"\\U0001f680-\\U0001f6ff\"  # transport & map symbols\n",
    "        \"\\U0001f1e0-\\U0001f1ff\"  # flags (iOS)\n",
    "        \"\\U00002500-\\U00002bef\"  # chinese char\n",
    "        \"\\U00002702-\\U000027b0\"\n",
    "        \"\\U00002702-\\U000027b0\"\n",
    "        \"\\U000024c2-\\U0001f251\"\n",
    "        \"\\U0001f926-\\U0001f937\"\n",
    "        \"\\U00010000-\\U0010ffff\"\n",
    "        \"\\u2640-\\u2642\"\n",
    "        \"\\u2600-\\u2b55\"\n",
    "        \"\\u200d\"\n",
    "        \"\\u23cf\"\n",
    "        \"\\u23e9\"\n",
    "        \"\\u231a\"\n",
    "        \"\\ufe0f\"  # dingbats\n",
    "        \"\\u3030\"\n",
    "        \"]+\",\n",
    "        flags=re.UNICODE,\n",
    "    )\n",
    "\n",
    "    tweet = emoji_pattern.sub(\" \", tweet)\n",
    "\n",
    "    # Replace multiple consecutive spaces with single space\n",
    "    tweet = re.sub(r\"\\s+\", \" \", tweet)\n",
    "\n",
    "    # Strip leading and trailing whitespace\n",
    "    tweet = tweet.strip()\n",
    "\n",
    "    # Compare 5 normal tweets with cleaned tweets\n",
    "    \n",
    "    # Return cleaned tweet\n",
    "    print(f\"Cleaned Tweet: {tweet}\")\n",
    "\n",
    "    return tweet\n",
    "\n",
    "\n",
    "df = ns_dataset.get(Languages.HAUSA).train\n",
    "encode_labels(df)\n",
    "\n",
    "# df['cleaned_tweet'] = df['tweet'].apply(clean_tweet)\n",
    "\n",
    "neural_input = get_wordpiece_tokeized_data(\n",
    "    df,\n",
    "    vocab_size=3700,\n",
    "    tweet_column='cleaned_tweet',\n",
    "    vectorizer_kwargs={'ngram': (1, 2), 'max_features': None},\n",
    "    \n",
    ")\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(neural_input.X_train.shape[1],)))\n",
    "\n",
    "# Dense layers for TF-IDF input\n",
    "# (512, 256, 128)\n",
    "# (8, 4, 2)\n",
    "model.add(Dense(16))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(16))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(8))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(3, activation=\"softmax\"))\n",
    "\n",
    "model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    optimizer=Adam(),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "X = np.array(neural_input.get_dense_X_train())\n",
    "y = np.array(neural_input.y_train)\n",
    "\n",
    "model.fit(X, y, epochs=100, batch_size=64, verbose=1)\n",
    "\n",
    "# Evaluate the model\n",
    "# model.evaluate(neural_input.X_test, neural_input.y_test)\n",
    "y_pred = model.predict(\n",
    "    np.array(neural_input.get_dense_X_test()),\n",
    ")\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "accuracy = accuracy_score(neural_input.y_test, y_pred_classes)\n",
    "print(f\"Accuracy with filtered tweets {accuracy:.4f}\")\n",
    "\n",
    "print(\n",
    "    classification_report(\n",
    "        neural_input.y_test,\n",
    "        y_pred_classes,\n",
    "        target_names=[\"positive\", \"neutral\", \"negative\"],\n",
    "    )\n",
    ")\n",
    "print(\"\\n\" + \"=\" * 50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8706c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(X_test_dense, y_test)\n",
    "print(f\"Test Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9179c7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(tweet: str):\n",
    "    # Subword tokenization\n",
    "    subwords = tokenizer.encode(tweet).tokens\n",
    "    subwords = ' '.join(subwords)  \n",
    "    \n",
    "    # TF-IDF vectorization\n",
    "    vector = vectorizer_wp.transform([subwords]).toarray()\n",
    "    \n",
    "    # Predict\n",
    "    probs = model.predict(vector)[0]  # Returns probabilities\n",
    "    \n",
    "    # Get the class label\n",
    "    predicted_class = np.argmax(probs)  # For multi-class softmax output\n",
    "    label_mapping = {\n",
    "        'positive': 0,\n",
    "        'neutral': 1,\n",
    "        'negative': 2\n",
    "    }\n",
    "    \n",
    "    return list(label_mapping.keys())[list(label_mapping.values()).index(predicted_class)], probs\n",
    "\n",
    "curr_df = ns_dataset.get(Languages.HAUSA).test\n",
    "curr_df = encode_labels(curr_df)\n",
    "\n",
    "\n",
    "correct_count = 0\n",
    "total_count = len(curr_df)\n",
    "label_mapping = {\n",
    "        'positive': 0,\n",
    "        'neutral': 1,\n",
    "        'negative': 2\n",
    "    }\n",
    "\n",
    "for index, row in curr_df.iterrows():\n",
    "    print(f\"Index: {index}\")\n",
    "    print(f\"Label: {row}\")\n",
    "    tweet = row['tweet']\n",
    "    \n",
    "    sentiment, probabilities = predict_sentiment(tweet)\n",
    "    print(f\"Tweet: {tweet}\\nPredicted Sentiment: {sentiment}, Probabilities: {probabilities}\\n\")\n",
    "    print(f\"Actual Sentiment: {row['label']}\\n\")\n",
    "\n",
    "    if sentiment.lower() == row:\n",
    "        correct_count += 1\n",
    "\n",
    "accuracy = correct_count / total_count\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb664a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of features from your TF-IDF matrix\n",
    "# tfidf_features = X_train_tfidf.shape[1]  # Number of TF-IDF features\n",
    "# num_classes = len(np.unique(y_train))    # Number of sentiment classes\n",
    "\n",
    "# Initialize models\n",
    "\n",
    "\n",
    "logistic_regression_model = BasicModelEncapsulator(LogisticRegression(max_iter=1000), name=\"Logistic Regression\")\n",
    "naive_bayes_model = BasicModelEncapsulator(MultinomialNB(), name=\"Naive Bayes\")\n",
    "\n",
    "tfidf_features = X_train_tfidf.shape[1]  # Number of TF-IDF features\n",
    "num_classes = len(np.unique(y_train))    # Number of classes\n",
    "\n",
    "\n",
    "# Initialize the corrected neural network\n",
    "neural_network_model = NeuralNetworkModel(\n",
    "    input_dim=tfidf_features, \n",
    "    num_classes=num_classes,\n",
    "    name=\"Neural Network\",\n",
    ")\n",
    "\n",
    "wordpiece_neural_network_model = NeuralNetworkModel(\n",
    "    input_dim=wp_tfidf_features, \n",
    "    num_classes=wp_num_classes\n",
    ")\n",
    "\n",
    "# accuracy_nn_count, report_nn_count = wordpiece_neural_network_model.perform_pipeline(tfidf_wp_train, wp_y_train)\n",
    "# print(\"Neural Network with Count Vectorizer Accuracy:\", accuracy_nn_count)\n",
    "# print(\"Neural Network with Count Vectorizer Classification Report:\\n\", report_nn_count)\n",
    "\n",
    "\n",
    "# Perform pipelines\n",
    "print(\"Training models...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e023f231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluator class\n",
    "# Map integer predictions back to string labels\n",
    "\n",
    "model = wordpiece_neural_network_model.model\n",
    "\n",
    "label_mapping = {0: \"neutral\", 1: \"positive\", 2: \"negative\"}\n",
    "\n",
    "\n",
    "model.fit(tfidf_wp_train, wp_y_train)\n",
    "predictions = wordpiece_neural_network_model.predict(tfidf_wp_test)\n",
    "y_test_str = [label_mapping[label] for label in wp_y_test]\n",
    "predictions_str = [label_mapping[label] for label in predictions]\n",
    "\n",
    "accuracy = accuracy_score(y_test_str, predictions_str)\n",
    "report = classification_report(y_test_str, predictions_str, output_dict=True)\n",
    "from evaluator import Evaluator\n",
    "# evaluator = Evaluator(\n",
    "#     {  \"Logistic_regression\" : logistic_regression_model,\n",
    "#        \"Naive_Bayes\" : naive_bayes_model,\n",
    "#         \"Neural_Network\" :neural_network_model\n",
    "#     }\n",
    "#     )\n",
    "\n",
    "# results, timings = evaluator.evaluate(X_train_tfidf, y_train)\n",
    "\n",
    "\n",
    "\n",
    "# evaluator.compare_classification_reports(reports=results, timings=timings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b7dbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wp_evaluator = Evaluator(\n",
    "#     {  \"Logistic_regression\" : logistic_regression_model,\n",
    "#        \"Naive_Bayes\" : naive_bayes_model,\n",
    "#         \"Wordpiece_Neural_Network\" : wordpiece_neural_network_model\n",
    "#     }\n",
    "#     )\n",
    "\n",
    "# results, timings = wp_evaluator.evaluate(tfidf_wp_train, wp_y_train)\n",
    "\n",
    "\n",
    "\n",
    "# wp_evaluator.compare_classification_reports(reports=results, timings=timings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b3a23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression with TF-IDF\n",
    "accuracy_lr, report_lr = logistic_regression_model.perform_pipeline(X_train_tfidf, y_train)\n",
    "print(\"Logistic Regression Accuracy:\", accuracy_lr)\n",
    "print(\"Logistic Regression Classification Report:\\n\", report_lr)\n",
    "\n",
    "# Logistic Regression with Count Vectorizer\n",
    "X_train_count, vectorizer_count = initialise_count_vectorizer(text_train)\n",
    "accuracy_lr_count, report_lr_count = logistic_regression_model.perform_pipeline(X_train_count, y_train)\n",
    "print(\"Logistic Regression with Count Vectorizer Accuracy:\", accuracy_lr_count)\n",
    "print(\"Logistic Regression with Count Vectorizer Classification Report:\\n\", report_lr_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad9c304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes with TF-IDF\n",
    "accuracy_nb, report_nb = naive_bayes_model.perform_pipeline(X_train_tfidf, y_train)\n",
    "print(\"Naive Bayes Accuracy:\", accuracy_nb)\n",
    "print(\"Naive Bayes Classification Report:\\n\", report_nb)\n",
    "\n",
    "# Naive Bayes with Count Vectorizer\n",
    "X_train_count, vectorizer_count = initialise_count_vectorizer(text_train)\n",
    "accuracy_nb_count, report_nb_count = naive_bayes_model.perform_pipeline(X_train_count, y_train)\n",
    "print(\"Naive Bayes with Count Vectorizer Accuracy:\", accuracy_nb_count)\n",
    "print(\"Naive Bayes with Count Vectorizer Classification Report:\\n\", report_nb_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2e6351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network with TF-IDF\n",
    "ohe_labels = [0 if label == 'neutral' else 1 if label == 'positive' else 2 for label in y_train]\n",
    "\n",
    "accuracy_nn, report_nn = neural_network_model.perform_pipeline(X_train_tfidf, y_train)\n",
    "print(\"Neural Network Accuracy:\", accuracy_nn)\n",
    "print(\"Neural Network Classification Report:\\n\", report_nn)\n",
    "\n",
    "# Neural Network with Count Vectorizer\n",
    "accuracy_nn_count, report_nn_count = neural_network_model.perform_pipeline(X_train_count, y_train)\n",
    "print(\"Neural Network with Count Vectorizer Accuracy:\", accuracy_nn_count)\n",
    "print(\"Neural Network with Count Vectorizer Classification Report:\\n\", report_nn_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d864c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wordpiece tokenized models TFIDF\n",
    "\n",
    "# from subword_tokenizer import get_tokenizer, wordpiece_tokenize_dataframe\n",
    "\n",
    "# tokenizer = get_tokenizer(df=ns_dataset.get(Languages.HAUSA).train)\n",
    "\n",
    "# train_df = ns_dataset.get(Languages.HAUSA).train\n",
    "# test_df = ns_dataset.get(Languages.HAUSA).test\n",
    "# # Naive Bayes with wordpiece tokenized data\n",
    "# wp_train_df = wordpiece_tokenize_dataframe(train_df, tokenizer)\n",
    "# wp_test_df = wordpiece_tokenize_dataframe(test_df, tokenizer)\n",
    "\n",
    "# wp_X_train_list = wp_train_df['tokenized_tweets'].tolist()\n",
    "# wp_X_test_list = wp_test_df['tokenized_tweets'].tolist()\n",
    "\n",
    "# # join sub lists into strings\n",
    "# wp_X_train_list = [' '.join(tokens) for tokens in wp_X_train_list]\n",
    "# wp_X_test_list = [' '.join(tokens) for tokens in wp_X_test_list]\n",
    "# # Convert labels to numerical format (0 for neutral, 1 for positive, 2 for negative)\n",
    "# wp_train_df['label'] = wp_train_df['label'].apply(lambda x: 0 if x == 'neutral' else 1 if x == 'positive' else 2)\n",
    "# wp_test_df['label'] = wp_test_df['label'].apply(lambda x: 0 if x == 'neutral' else 1 if x == 'positive' else 2)\n",
    "\n",
    "# wp_y_train = wp_train_df['label'].tolist()\n",
    "# wp_y_test = wp_test_df['label'].tolist()\n",
    "\n",
    "# tfidf_wp_train, vectorizer_wp = initialise_tfidf_vectorizer(wp_X_train_list)\n",
    "# tfidf_wp_test, _ = initialise_tfidf_vectorizer(wp_X_test_list)\n",
    "\n",
    "# tfidf_features = tfidf_wp_train.shape[1]  # Number of TF-IDF features\n",
    "# num_classes = len(np.unique(wp_y_train))    # Number of classes\n",
    "\n",
    "\n",
    "# Initialize the corrected neural network\n",
    "neural_network_model = NeuralNetworkModel(\n",
    "    input_dim=tfidf_features, \n",
    "    num_classes=num_classes\n",
    ")\n",
    "\n",
    "\n",
    "# Naive Bayes with WordPiece tokenized data\n",
    "accuracy_nb, report_nb = naive_bayes_model.perform_pipeline(tfidf_wp_train, wp_y_train)\n",
    "print(\"Naive Bayes Accuracy:\", accuracy_nb)\n",
    "print(\"Naive Bayes Classification Report:\\n\", report_nb)\n",
    "\n",
    "# Logistic Regression with WordPiece tokenized data\n",
    "accuracy_lr_wp, report_lr_wp = logistic_regression_model.perform_pipeline(tfidf_wp_train, wp_y_train)\n",
    "print(\"Logistic Regression Accuracy:\", accuracy_lr_wp)\n",
    "print(\"Logistic Regression Classification Report:\\n\", report_lr_wp)\n",
    "\n",
    "# Neural Network with WordPiece tokenized data\n",
    "accuracy_nn_wp, report_nn_wp = neural_network_model.perform_pipeline(tfidf_wp_train, wp_y_train)\n",
    "print(\"Neural Network Accuracy:\", accuracy_nn_wp)\n",
    "print(\"Neural Network Classification Report:\\n\", report_nn_wp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe730df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to optimize n-grams and max features for TF-IDF\n",
    "def tfidf_score(input_x, y_train, score = None):\n",
    "    clf = LogisticRegression(max_iter=1000)\n",
    "    return cross_val_score(clf, X=input_x, y=y_train, scoring=score)\n",
    "scores_tfidf = tfidf_score(X_train_tfidf, y_train)\n",
    "print(\"5-fold Cross-Validation Accuracy for TFIDF: %0.2f (+/- %0.2f)\" % (scores_tfidf.mean(), scores_tfidf.std() * 2))\n",
    "\n",
    "scores_tfidf_f1 = tfidf_score(X_train_tfidf, y_train, score= 'f1_macro')\n",
    "\n",
    "print(\"5-fold Cross-Validation F1 score for TFIDF: %0.2f (+/- %0.2f)\" % (scores_tfidf_f1.mean(), scores_tfidf_f1.std() * 2))\n",
    "\n",
    "def test_param_combos(X_train, y_train, param_combos):\n",
    "    results = []\n",
    "    for params in param_combos:\n",
    "        X_train_tfidf, vectorizer_tfidf = initialise_tfidf_vectorizer(X_train, ngram=params.get('ngram'), max_features=params.get('max_features'))\n",
    "        score = tfidf_score(X_train_tfidf, y_train)\n",
    "        results.append({\n",
    "            'ngram': params.get('ngram'),\n",
    "            'max_features': params.get('max_features'),\n",
    "            'score': score.mean(),\n",
    "            'std_dev': score.std(),\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Example parameter combinations to test\n",
    "param_combos = [\n",
    "    {'ngram': (1,2), 'max_features': 5000},\n",
    "    {'ngram': (1,3), 'max_features': 5000},\n",
    "    {'ngram': (1,2), 'max_features': 10000},\n",
    "    {'ngram': (1,3), 'max_features': 10000},\n",
    "    {'ngram': (1,2), 'max_features': None},\n",
    "    {'ngram': (1,3), 'max_features': None},\n",
    "    {'ngram': (1,2), 'max_features': 2000},\n",
    "    {'ngram': (1,3), 'max_features': 2000},\n",
    "    {'ngram': (1,2), 'max_features': 3000},\n",
    "    {'ngram': (1,3), 'max_features': 3000},\n",
    "    {'ngram': (1,2), 'max_features': 4000},\n",
    "    {'ngram': (1,3), 'max_features': 4000},\n",
    "    {'ngram': (1,2), 'max_features': 6000},\n",
    "    {'ngram': (1,3), 'max_features': 6000},\n",
    "    {'ngram': (1,2), 'max_features': 7000},\n",
    "    {'ngram': (1,3), 'max_features': 7000},\n",
    "    {'ngram': (1,2), 'max_features': 8000},\n",
    "    {'ngram': (1,3), 'max_features': 8000},\n",
    "    {'ngram': (1,2), 'max_features': 9000},\n",
    "    {'ngram': (1,3), 'max_features': 9000},\n",
    "    {'ngram': (1,2), 'max_features': 10000},\n",
    "    {'ngram': (1,3), 'max_features': 10000},\n",
    "    {'ngram': (1,2), 'max_features': 12000},\n",
    "    {'ngram': (1,4), 'max_features': 5000},\n",
    "    {'ngram': (1,4), 'max_features': 10000},\n",
    "    {'ngram': (1,4), 'max_features': None},\n",
    "    {'ngram': (1,4), 'max_features': 2000},\n",
    "    {'ngram': (1,4), 'max_features': 3000},\n",
    "    {'ngram': (1,4), 'max_features': 4000},\n",
    "    {'ngram': (1,4), 'max_features': 6000},\n",
    "    {'ngram': (1,4), 'max_features': 7000},\n",
    "    {'ngram': (1,4), 'max_features': 8000},\n",
    "    {'ngram': (1,4), 'max_features': 9000},\n",
    "    {'ngram': (1,4), 'max_features': 10000},\n",
    "    {'ngram': (1,4), 'max_features': 12000},\n",
    "    {'ngram': (2,5), 'max_features': 5000},\n",
    "    {'ngram': (2,5), 'max_features': 10000},\n",
    "    {'ngram': (2,5), 'max_features': None},\n",
    "    {'ngram': (2,5), 'max_features': 2000},\n",
    "    {'ngram': (2,5), 'max_features': 3000},\n",
    "    {'ngram': (2,5), 'max_features': 4000},\n",
    "    {'ngram': (2,5), 'max_features': 6000},\n",
    "    {'ngram': (2,5), 'max_features': 7000},\n",
    "    {'ngram': (2,5), 'max_features': 8000},\n",
    "    {'ngram': (2,5), 'max_features': 9000},\n",
    "    {'ngram': (2,5), 'max_features': 10000},\n",
    "    {'ngram': (2,5), 'max_features': 12000},\n",
    "    {'ngram': (3,5), 'max_features': 5000},\n",
    "    {'ngram': (3,5), 'max_features': 10000},\n",
    "    {'ngram': (3,5), 'max_features': None},\n",
    "    {'ngram': (3,5), 'max_features': 2000},\n",
    "    {'ngram': (3,5), 'max_features': 3000},\n",
    "    {'ngram': (3,5), 'max_features': 4000},\n",
    "    {'ngram': (3,5), 'max_features': 6000},\n",
    "    {'ngram': (3,5), 'max_features': 7000},\n",
    "    {'ngram': (3,5), 'max_features': 8000},\n",
    "    {'ngram': (3,5), 'max_features': 9000},\n",
    "    {'ngram': (3,5), 'max_features': 10000},\n",
    "    {'ngram': (3,5), 'max_features': 12000}\n",
    "]\n",
    "# Test the parameter combinations\n",
    "results_df = test_param_combos(text_train, y_train, param_combos)\n",
    "# Sort the results by mean score\n",
    "results_df = results_df.sort_values(by='score', ascending=False)\n",
    "# Save the results to a CSV file\n",
    "results_df.to_csv('data/tfidf_param_combos_results.csv', index=False)\n",
    "# Print the top results\n",
    "print(\"Top parameter combinations based on accuracy:\")\n",
    "print(results_df.head(10))\n",
    "# Print the results DataFrame\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a37d7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot playground (based off neural network training history)\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.figure(figsize=(12, 4))\n",
    "\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "# plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "# plt.title('Model Accuracy')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.legend()\n",
    "\n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.plot(history.history['loss'], label='Training Loss')\n",
    "# plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "# plt.title('Model Loss')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.legend()\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
