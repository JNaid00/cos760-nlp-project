{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232c4b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "from typing import Dict, List, Optional\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from tokenizers.models import WordPiece\n",
    "from tokenizers.trainers import WordPieceTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.normalizers import Sequence, Lowercase, NFD, StripAccents\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, GlobalAveragePooling1D, Dense, Dropout,Input, BatchNormalization, Activation\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from models import BasicModelEncapsulator, NeuralNetworkModel\n",
    "from custom_vectorizers import initialise_count_vectorizer, initialise_tfidf_vectorizer\n",
    "from constants import LOCAL_DIR_AS, LOCAL_DIR_NS, REPO_URL_AS, REPO_URL_NS, NS_LANGUAGES\n",
    "from custom_datasets import MultiLangDataset, load_local_datasets\n",
    "\n",
    "from custom_datasets import Languages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be9680c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clone_repo(repo_url: str, local_dir: str) -> None:\n",
    "    if os.path.isdir(local_dir):\n",
    "        print(\"Repository exists. Updating...\")\n",
    "        subprocess.run([\"git\", \"-C\", local_dir, \"pull\", \"origin\", \"main\"], check=True)\n",
    "    else:\n",
    "        print(\"Repository not found. Cloning...\")\n",
    "        subprocess.run([\"git\", \"clone\", repo_url], check=True)\n",
    "\n",
    "clone_repo(REPO_URL_NS, LOCAL_DIR_NS)\n",
    "clone_repo(REPO_URL_AS, LOCAL_DIR_AS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b5a129",
   "metadata": {},
   "outputs": [],
   "source": [
    "ns_dataset: MultiLangDataset = load_local_datasets(local_base_dir=LOCAL_DIR_NS + '/data/annotated_tweets', languages=NS_LANGUAGES) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947ce503",
   "metadata": {},
   "outputs": [],
   "source": [
    "as_dataset: MultiLangDataset = load_local_datasets(local_base_dir=f'afrisent-semeval-2023/data', languages=NS_LANGUAGES,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d6eb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"NaijaSenti dataset loaded with languages:\", ns_dataset.all_languages())\n",
    "print(\"Afrisenti dataset loaded with languages:\", as_dataset.all_languages())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d903e898",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(\"NaijaSenti hau: \", ns_dataset.get(Languages.HAUSA).test)\n",
    "# Print each row in the dev set for the column 'tweet'\n",
    "for index, row in ns_dataset.get(Languages.HAUSA).test.iterrows():\n",
    "    print(f\"Index: {index}, Tweet: {row['tweet']}\")\n",
    "\n",
    "# write all the tweets into a textfile\n",
    "# check if the dir data exists, if not create it\n",
    "if not os.path.exists('data'):\n",
    "    os.makedirs('data')\n",
    "with open('data/naija_senti_hau_dev_tweets.txt', 'w', encoding='utf-8') as f:\n",
    "    for index, row in ns_dataset.get(Languages.HAUSA).dev.iterrows():\n",
    "        f.write(f\"{row['tweet']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0078212",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ns_dataset.get(Languages.HAUSA).train\n",
    "text_train, text_test, y_train, y_test = train_test_split(df.tweet, df.label, test_size = 0.3)\n",
    "X_train_tfidf, vectorizer_tfidf = initialise_tfidf_vectorizer(text_train)\n",
    "X_train_count, vectorizer_count = initialise_count_vectorizer(text_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1372129",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb664a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of features from your TF-IDF matrix\n",
    "# tfidf_features = X_train_tfidf.shape[1]  # Number of TF-IDF features\n",
    "# num_classes = len(np.unique(y_train))    # Number of sentiment classes\n",
    "\n",
    "# Initialize models\n",
    "\n",
    "\n",
    "logistic_regression_model = BasicModelEncapsulator(LogisticRegression(max_iter=1000), name=\"Logistic Regression\")\n",
    "naive_bayes_model = BasicModelEncapsulator(MultinomialNB(), name=\"Naive Bayes\")\n",
    "\n",
    "tfidf_features = X_train_tfidf.shape[1]  # Number of TF-IDF features\n",
    "num_classes = len(np.unique(y_train))    # Number of classes\n",
    "\n",
    "\n",
    "# Initialize the corrected neural network\n",
    "neural_network_model = NeuralNetworkModel(\n",
    "    input_dim=tfidf_features, \n",
    "    num_classes=num_classes,\n",
    "    name=\"Neural Network\",\n",
    ")\n",
    "\n",
    "\n",
    "# Perform pipelines\n",
    "print(\"Training models...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e023f231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluator class\n",
    "from evaluator import Evaluator\n",
    "evaluator = Evaluator(\n",
    "    {  \"Logistic_regression\" : logistic_regression_model,\n",
    "       \"Naive_Bayes\" : naive_bayes_model,\n",
    "        \"Neural_Network\" :neural_network_model\n",
    "    }\n",
    "    )\n",
    "\n",
    "results, timings = evaluator.evaluate(X_train_tfidf, y_train)\n",
    "\n",
    "\n",
    "\n",
    "evaluator.compare_classification_reports(reports=results, timings=timings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b3a23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression with TF-IDF\n",
    "accuracy_lr, report_lr = logistic_regression_model.perform_pipeline(X_train_tfidf, y_train)\n",
    "print(\"Logistic Regression Accuracy:\", accuracy_lr)\n",
    "print(\"Logistic Regression Classification Report:\\n\", report_lr)\n",
    "\n",
    "# Logistic Regression with Count Vectorizer\n",
    "X_train_count, vectorizer_count = initialise_count_vectorizer(text_train)\n",
    "accuracy_lr_count, report_lr_count = logistic_regression_model.perform_pipeline(X_train_count, y_train)\n",
    "print(\"Logistic Regression with Count Vectorizer Accuracy:\", accuracy_lr_count)\n",
    "print(\"Logistic Regression with Count Vectorizer Classification Report:\\n\", report_lr_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad9c304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes with TF-IDF\n",
    "accuracy_nb, report_nb = naive_bayes_model.perform_pipeline(X_train_tfidf, y_train)\n",
    "print(\"Naive Bayes Accuracy:\", accuracy_nb)\n",
    "print(\"Naive Bayes Classification Report:\\n\", report_nb)\n",
    "\n",
    "# Naive Bayes with Count Vectorizer\n",
    "X_train_count, vectorizer_count = initialise_count_vectorizer(text_train)\n",
    "accuracy_nb_count, report_nb_count = naive_bayes_model.perform_pipeline(X_train_count, y_train)\n",
    "print(\"Naive Bayes with Count Vectorizer Accuracy:\", accuracy_nb_count)\n",
    "print(\"Naive Bayes with Count Vectorizer Classification Report:\\n\", report_nb_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2e6351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network with TF-IDF\n",
    "ohe_labels = [0 if label == 'neutral' else 1 if label == 'positive' else 2 for label in y_train]\n",
    "\n",
    "accuracy_nn, report_nn = neural_network_model.perform_pipeline(X_train_tfidf, y_train)\n",
    "print(\"Neural Network Accuracy:\", accuracy_nn)\n",
    "print(\"Neural Network Classification Report:\\n\", report_nn)\n",
    "\n",
    "# Neural Network with Count Vectorizer\n",
    "accuracy_nn_count, report_nn_count = neural_network_model.perform_pipeline(X_train_count, y_train)\n",
    "print(\"Neural Network with Count Vectorizer Accuracy:\", accuracy_nn_count)\n",
    "print(\"Neural Network with Count Vectorizer Classification Report:\\n\", report_nn_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d864c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wordpiece tokenized models TFIDF\n",
    "\n",
    "from subword_tokenizer import get_tokenizer, wordpiece_tokenize_dataframe\n",
    "\n",
    "tokenizer = get_tokenizer(df=ns_dataset.get(Languages.HAUSA).train)\n",
    "\n",
    "train_df = ns_dataset.get(Languages.HAUSA).train\n",
    "test_df = ns_dataset.get(Languages.HAUSA).test\n",
    "# Naive Bayes with wordpiece tokenized data\n",
    "wp_train_df = wordpiece_tokenize_dataframe(train_df, tokenizer)\n",
    "wp_test_df = wordpiece_tokenize_dataframe(test_df, tokenizer)\n",
    "\n",
    "wp_X_train_list = wp_train_df['tokenized_tweets'].tolist()\n",
    "wp_X_test_list = wp_test_df['tokenized_tweets'].tolist()\n",
    "\n",
    "# join sub lists into strings\n",
    "wp_X_train_list = [' '.join(tokens) for tokens in wp_X_train_list]\n",
    "wp_X_test_list = [' '.join(tokens) for tokens in wp_X_test_list]\n",
    "# Convert labels to numerical format (0 for neutral, 1 for positive, 2 for negative)\n",
    "wp_train_df['label'] = wp_train_df['label'].apply(lambda x: 0 if x == 'neutral' else 1 if x == 'positive' else 2)\n",
    "wp_test_df['label'] = wp_test_df['label'].apply(lambda x: 0 if x == 'neutral' else 1 if x == 'positive' else 2)\n",
    "\n",
    "wp_y_train = wp_train_df['label'].tolist()\n",
    "wp_y_test = wp_test_df['label'].tolist()\n",
    "\n",
    "tfidf_wp_train, vectorizer_wp = initialise_tfidf_vectorizer(wp_X_train_list)\n",
    "tfidf_wp_test, _ = initialise_tfidf_vectorizer(wp_X_test_list)\n",
    "\n",
    "tfidf_features = tfidf_wp_train.shape[1]  # Number of TF-IDF features\n",
    "num_classes = len(np.unique(wp_y_train))    # Number of classes\n",
    "\n",
    "\n",
    "# Initialize the corrected neural network\n",
    "neural_network_model = NeuralNetworkModel(\n",
    "    input_dim=tfidf_features, \n",
    "    num_classes=num_classes\n",
    ")\n",
    "\n",
    "\n",
    "# Naive Bayes with WordPiece tokenized data\n",
    "accuracy_nb, report_nb = naive_bayes_model.perform_pipeline(tfidf_wp_train, wp_y_train)\n",
    "print(\"Naive Bayes Accuracy:\", accuracy_nb)\n",
    "print(\"Naive Bayes Classification Report:\\n\", report_nb)\n",
    "\n",
    "# Logistic Regression with WordPiece tokenized data\n",
    "accuracy_lr_wp, report_lr_wp = logistic_regression_model.perform_pipeline(tfidf_wp_train, wp_y_train)\n",
    "print(\"Logistic Regression Accuracy:\", accuracy_lr_wp)\n",
    "print(\"Logistic Regression Classification Report:\\n\", report_lr_wp)\n",
    "\n",
    "# Neural Network with WordPiece tokenized data\n",
    "accuracy_nn_wp, report_nn_wp = neural_network_model.perform_pipeline(tfidf_wp_train, wp_y_train)\n",
    "print(\"Neural Network Accuracy:\", accuracy_nn_wp)\n",
    "print(\"Neural Network Classification Report:\\n\", report_nn_wp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe730df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to optimize n-grams and max features for TF-IDF\n",
    "def tfidf_score(input_x, y_train, score = None):\n",
    "    clf = LogisticRegression(max_iter=1000)\n",
    "    return cross_val_score(clf, X=input_x, y=y_train, scoring=score)\n",
    "scores_tfidf = tfidf_score(X_train_tfidf, y_train)\n",
    "print(\"5-fold Cross-Validation Accuracy for TFIDF: %0.2f (+/- %0.2f)\" % (scores_tfidf.mean(), scores_tfidf.std() * 2))\n",
    "\n",
    "scores_tfidf_f1 = tfidf_score(X_train_tfidf, y_train, score= 'f1_macro')\n",
    "\n",
    "print(\"5-fold Cross-Validation F1 score for TFIDF: %0.2f (+/- %0.2f)\" % (scores_tfidf_f1.mean(), scores_tfidf_f1.std() * 2))\n",
    "\n",
    "def test_param_combos(X_train, y_train, param_combos):\n",
    "    results = []\n",
    "    for params in param_combos:\n",
    "        X_train_tfidf, vectorizer_tfidf = initialise_tfidf_vectorizer(X_train, ngram=params.get('ngram_range'), max_features=params.get('max_features'))\n",
    "        score = tfidf_score(X_train_tfidf, y_train)\n",
    "        results.append({\n",
    "            'ngram_range': params.get('ngram_range'),\n",
    "            'max_features': params.get('max_features'),\n",
    "            'score': score.mean(),\n",
    "            'std_dev': score.std(),\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Example parameter combinations to test\n",
    "param_combos = [\n",
    "    {'ngram_range': (1,2), 'max_features': 5000},\n",
    "    {'ngram_range': (1,3), 'max_features': 5000},\n",
    "    {'ngram_range': (1,2), 'max_features': 10000},\n",
    "    {'ngram_range': (1,3), 'max_features': 10000},\n",
    "    {'ngram_range': (1,2), 'max_features': None},\n",
    "    {'ngram_range': (1,3), 'max_features': None},\n",
    "    {'ngram_range': (1,2), 'max_features': 2000},\n",
    "    {'ngram_range': (1,3), 'max_features': 2000},\n",
    "    {'ngram_range': (1,2), 'max_features': 3000},\n",
    "    {'ngram_range': (1,3), 'max_features': 3000},\n",
    "    {'ngram_range': (1,2), 'max_features': 4000},\n",
    "    {'ngram_range': (1,3), 'max_features': 4000},\n",
    "    {'ngram_range': (1,2), 'max_features': 6000},\n",
    "    {'ngram_range': (1,3), 'max_features': 6000},\n",
    "    {'ngram_range': (1,2), 'max_features': 7000},\n",
    "    {'ngram_range': (1,3), 'max_features': 7000},\n",
    "    {'ngram_range': (1,2), 'max_features': 8000},\n",
    "    {'ngram_range': (1,3), 'max_features': 8000},\n",
    "    {'ngram_range': (1,2), 'max_features': 9000},\n",
    "    {'ngram_range': (1,3), 'max_features': 9000},\n",
    "    {'ngram_range': (1,2), 'max_features': 10000},\n",
    "    {'ngram_range': (1,3), 'max_features': 10000},\n",
    "    {'ngram_range': (1,2), 'max_features': 12000},\n",
    "    {'ngram_range': (1,4), 'max_features': 5000},\n",
    "    {'ngram_range': (1,4), 'max_features': 10000},\n",
    "    {'ngram_range': (1,4), 'max_features': None},\n",
    "    {'ngram_range': (1,4), 'max_features': 2000},\n",
    "    {'ngram_range': (1,4), 'max_features': 3000},\n",
    "    {'ngram_range': (1,4), 'max_features': 4000},\n",
    "    {'ngram_range': (1,4), 'max_features': 6000},\n",
    "    {'ngram_range': (1,4), 'max_features': 7000},\n",
    "    {'ngram_range': (1,4), 'max_features': 8000},\n",
    "    {'ngram_range': (1,4), 'max_features': 9000},\n",
    "    {'ngram_range': (1,4), 'max_features': 10000},\n",
    "    {'ngram_range': (1,4), 'max_features': 12000},\n",
    "    {'ngram_range': (2,5), 'max_features': 5000},\n",
    "    {'ngram_range': (2,5), 'max_features': 10000},\n",
    "    {'ngram_range': (2,5), 'max_features': None},\n",
    "    {'ngram_range': (2,5), 'max_features': 2000},\n",
    "    {'ngram_range': (2,5), 'max_features': 3000},\n",
    "    {'ngram_range': (2,5), 'max_features': 4000},\n",
    "    {'ngram_range': (2,5), 'max_features': 6000},\n",
    "    {'ngram_range': (2,5), 'max_features': 7000},\n",
    "    {'ngram_range': (2,5), 'max_features': 8000},\n",
    "    {'ngram_range': (2,5), 'max_features': 9000},\n",
    "    {'ngram_range': (2,5), 'max_features': 10000},\n",
    "    {'ngram_range': (2,5), 'max_features': 12000},\n",
    "    {'ngram_range': (3,5), 'max_features': 5000},\n",
    "    {'ngram_range': (3,5), 'max_features': 10000},\n",
    "    {'ngram_range': (3,5), 'max_features': None},\n",
    "    {'ngram_range': (3,5), 'max_features': 2000},\n",
    "    {'ngram_range': (3,5), 'max_features': 3000},\n",
    "    {'ngram_range': (3,5), 'max_features': 4000},\n",
    "    {'ngram_range': (3,5), 'max_features': 6000},\n",
    "    {'ngram_range': (3,5), 'max_features': 7000},\n",
    "    {'ngram_range': (3,5), 'max_features': 8000},\n",
    "    {'ngram_range': (3,5), 'max_features': 9000},\n",
    "    {'ngram_range': (3,5), 'max_features': 10000},\n",
    "    {'ngram_range': (3,5), 'max_features': 12000}\n",
    "]\n",
    "# Test the parameter combinations\n",
    "results_df = test_param_combos(text_train, y_train, param_combos)\n",
    "# Sort the results by mean score\n",
    "results_df = results_df.sort_values(by='score', ascending=False)\n",
    "# Save the results to a CSV file\n",
    "results_df.to_csv('data/tfidf_param_combos_results.csv', index=False)\n",
    "# Print the top results\n",
    "print(\"Top parameter combinations based on accuracy:\")\n",
    "print(results_df.head(10))\n",
    "# Print the results DataFrame\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a37d7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot playground (based off neural network training history)\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.figure(figsize=(12, 4))\n",
    "\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "# plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "# plt.title('Model Accuracy')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.legend()\n",
    "\n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.plot(history.history['loss'], label='Training Loss')\n",
    "# plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "# plt.title('Model Loss')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.legend()\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
