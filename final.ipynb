{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_datasets import MultiLangDataset, SplitSet\n",
    "from custom_datasets import ns_dataset\n",
    "from custom_datasets import Languages\n",
    "from custom_datasets import clean_tweet\n",
    "from constants import TokenizerEnum, VectorizerEnum\n",
    "from custom_vectorizers import get_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, GlobalAveragePooling1D, Dense, Dropout,Input, BatchNormalization, Activation\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YOR_DATASET: SplitSet = ns_dataset.get(Languages.YORUBA)\n",
    "HAU_DATASET: SplitSet = ns_dataset.get(Languages.HAUSA)\n",
    "IBO_DATASET: SplitSet = ns_dataset.get(Languages.IGBO)\n",
    "PCM_DATASET: SplitSet = ns_dataset.get(Languages.NIGERIAN_PIDGIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluator\n",
    "from analysis import compare_results\n",
    "\n",
    "# Tokenizer\n",
    "from subword_tokenizer import (\n",
    "    get_tokenizer,\n",
    "    wordpiece_tokenize_dataframe,\n",
    "    get_sentencepiece_tokenizer,\n",
    "    sentencepiece_tokenize_dataframe,\n",
    "    get_wordpiece_tokeized_data,\n",
    "    get_sentencepiece_tokeized_data\n",
    ")\n",
    "\n",
    "# Compare Results\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_labels(df: pd.DataFrame):\n",
    "    label_mapping = {\"negative\": 0, \"neutral\": 1, \"positive\": 2}\n",
    "    df[\"label_encoded\"] = df[\"label\"].str.lower().map(label_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VECTORIZER_KWARGS = {\n",
    "#     \"ngram\": (1, 2),\n",
    "#     \"max_features\": 3700,}\n",
    "\n",
    "VECTORIZER_KWARGS = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "\n",
    "def naive_bayes(\n",
    "    dataset: SplitSet, vectorizer: VectorizerEnum,  tokenizer: Optional[TokenizerEnum] = None\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Naive Bayes classifier for text classification.\n",
    "    Args:\n",
    "        dataset (SplitSet): The dataset to use for training and testing.\n",
    "        tokenizer (TokenizerEnum): The tokenizer to use.\n",
    "        If tokenizer is None, then no tokenization is applied. Only vectorization is applied.\n",
    "        vectorizer (VectorizerEnum): The vectorizer to use.\n",
    "    Returns:\n",
    "        dict: A dictionary containing the results of the classification.\n",
    "        Returns a classification report\n",
    "    \"\"\"\n",
    "    \n",
    "    selected_vectorizer = get_vectorizer(vectorizer)\n",
    "    \n",
    "    \n",
    "    data: pd.DataFrame = dataset.train\n",
    "    \n",
    "    if tokenizer is not None:\n",
    "        if tokenizer == TokenizerEnum.WORDPIECE:\n",
    "            tokenized_data = wordpiece_tokenize_dataframe(data, get_tokenizer(data))\n",
    "        elif tokenizer == TokenizerEnum.SENTENCEPIECE:\n",
    "            tokenized_data = sentencepiece_tokenize_dataframe(data, get_tokenizer(data))\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported tokenizer: {tokenizer}\")\n",
    "        tokenized_list = tokenized_data[\"tokenized_tweets\"].tolist()\n",
    "        \n",
    "\n",
    "        # join sub lists into strings\n",
    "        tokenized_list = [\" \".join(tokens) for tokens in tokenized_list]\n",
    "\n",
    "        y = tokenized_data[\"label\"].tolist()\n",
    "\n",
    "        \n",
    "        tokenized_vectorized_data, vectorizer_wp = selected_vectorizer(\n",
    "            tokenized_list, **VECTORIZER_KWARGS\n",
    "        )\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            tokenized_vectorized_data, y, test_size=0.3, random_state=42\n",
    "        )\n",
    "    else:\n",
    "        vectorized_data, trained_vectorizer = selected_vectorizer(data[\"tweet\"].tolist(), **VECTORIZER_KWARGS)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            vectorized_data, data[\"label\"], test_size=0.3, random_state=42\n",
    "        )\n",
    "    model = MultinomialNB()\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    # accuracy = accuracy_score(y_test, predictions)\n",
    "    report = classification_report(y_test, predictions, output_dict=True)\n",
    "    return report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "def logistic_regression(\n",
    "    dataset: SplitSet, vectorizer: VectorizerEnum,  tokenizer: Optional[TokenizerEnum] = None\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Logistic Regression classifier for text classification.\n",
    "    Args:\n",
    "        dataset (SplitSet): The dataset to use for training and testing.\n",
    "        tokenizer (TokenizerEnum): The tokenizer to use.\n",
    "        If tokenizer is None, then no tokenization is applied. Only vectorization is applied.\n",
    "        vectorizer (VectorizerEnum): The vectorizer to use.\n",
    "    Returns:\n",
    "        dict: A dictionary containing the results of the classification.\n",
    "        Returns a classification report\n",
    "    \"\"\"\n",
    "    selected_vectorizer = get_vectorizer(vectorizer)\n",
    "    \n",
    "    \n",
    "    data: pd.DataFrame = dataset.train\n",
    "    \n",
    "    if tokenizer is not None:\n",
    "        if tokenizer == TokenizerEnum.WORDPIECE:\n",
    "            tokenized_data = wordpiece_tokenize_dataframe(data, get_tokenizer(data))\n",
    "        elif tokenizer == TokenizerEnum.SENTENCEPIECE:\n",
    "            tokenized_data = sentencepiece_tokenize_dataframe(data, get_tokenizer(data))\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported tokenizer: {tokenizer}\")\n",
    "        tokenized_list = tokenized_data[\"tokenized_tweets\"].tolist()\n",
    "        \n",
    "\n",
    "        # join sub lists into strings\n",
    "        tokenized_list = [\" \".join(tokens) for tokens in tokenized_list]\n",
    "\n",
    "        y = tokenized_data[\"label\"].tolist()\n",
    "\n",
    "        \n",
    "        tokenized_vectorized_data, vectorizer_wp = selected_vectorizer(\n",
    "            tokenized_list, **VECTORIZER_KWARGS\n",
    "        )\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            tokenized_vectorized_data, y, test_size=0.3, random_state=42\n",
    "        )\n",
    "    else:\n",
    "        vectorized_data, trained_vectorizer = selected_vectorizer(data[\"tweet\"].tolist(), **VECTORIZER_KWARGS)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            vectorized_data, data[\"label\"], test_size=0.3, random_state=42\n",
    "        )\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    # accuracy = accuracy_score(y_test, predictions)\n",
    "    report = classification_report(y_test, predictions, output_dict=True)\n",
    "    return report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def neural_network(\n",
    "    dataset: SplitSet, vectorizer: VectorizerEnum,  tokenizer: Optional[TokenizerEnum] = None, clean_tweets: bool = True\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Neural Network classifier for text classification.\n",
    "    Args:\n",
    "        dataset (SplitSet): The dataset to use for training and testing.\n",
    "        tokenizer (TokenizerEnum): The tokenizer to use.\n",
    "        If tokenizer is None, then no tokenization is applied. Only vectorization is applied.\n",
    "        vectorizer (VectorizerEnum): The vectorizer to use.\n",
    "    Returns:\n",
    "        dict: A dictionary containing the results of the classification.\n",
    "        Returns a classification report\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "    df = dataset.train\n",
    "    # encode_labels(df)\n",
    "    if clean_tweets:\n",
    "        df['cleaned_tweet'] = df['tweet'].apply(clean_tweet)\n",
    "    encode_labels(df)\n",
    "\n",
    "    if tokenizer is not None:\n",
    "        neural_input = get_wordpiece_tokeized_data(\n",
    "            df,\n",
    "            vocab_size=3700,\n",
    "            tweet_column=\"cleaned_tweet\",\n",
    "            vectorizer_kwargs={\"ngram\": (1, 2), \"max_features\": None},\n",
    "        )\n",
    "    elif tokenizer == TokenizerEnum.SENTENCEPIECE:\n",
    "        neural_input = get_sentencepiece_tokeized_data(\n",
    "            df,\n",
    "            vocab_size=3700,\n",
    "            tweet_column=\"cleaned_tweet\",\n",
    "            vectorizer_kwargs={\"ngram\": (1, 2), \"max_features\": None},\n",
    "        )\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(neural_input.X_train.shape[1],)))\n",
    "\n",
    "    # Dense layers for TF-IDF input\n",
    "    # (512, 256, 128)\n",
    "    # (8, 4, 2)\n",
    "    model.add(Dense(512))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(256))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(128))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(3, activation=\"softmax\"))\n",
    "\n",
    "    model.compile(\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        optimizer=Adam(),\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "\n",
    "    X = np.array(neural_input.get_dense_X_train())\n",
    "    y = np.array(neural_input.y_train)\n",
    "\n",
    "    model.fit(X, y, epochs=10, batch_size=64, verbose=1)\n",
    "\n",
    "    # Evaluate the model\n",
    "    # model.evaluate(neural_input.X_test, neural_input.y_test)\n",
    "    y_pred = model.predict(\n",
    "        np.array(neural_input.get_dense_X_test()),\n",
    "    )\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    accuracy = accuracy_score(neural_input.y_test, y_pred_classes)\n",
    "    print(f\"Accuracy with filtered tweets {accuracy:.4f}\")\n",
    "\n",
    "    \n",
    "    return classification_report(\n",
    "            neural_input.y_test,\n",
    "            y_pred_classes,\n",
    "            target_names=[\"positive\", \"neutral\", \"negative\"],\n",
    "            output_dict=True\n",
    "        )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_yor_kwargs ={\n",
    "    \"nn_yor_wp_clean\":{\n",
    "        \"dataset\": YOR_DATASET,\n",
    "        \"vectorizer\": VectorizerEnum.TFIDF,\n",
    "        \"tokenizer\": TokenizerEnum.WORDPIECE,\n",
    "        \"clean_tweets\": True\n",
    "    },\n",
    "    \"nn_yor_wp_no_clean\":{\n",
    "        \"dataset\": YOR_DATASET,\n",
    "        \"vectorizer\": VectorizerEnum.TFIDF,\n",
    "        \"tokenizer\": TokenizerEnum.WORDPIECE,\n",
    "        \"clean_tweets\": False\n",
    "    },\n",
    "    \"nn_yor_sp_clean\" : {\n",
    "        \"dataset\": YOR_DATASET,\n",
    "        \"vectorizer\": VectorizerEnum.TFIDF,\n",
    "        \"tokenizer\": TokenizerEnum.SENTENCEPIECE,\n",
    "        \"clean_tweets\": True\n",
    "    },\n",
    "    \"nn_yor_sp_no_clean\" : {\n",
    "        \"dataset\": YOR_DATASET,\n",
    "        \"vectorizer\": VectorizerEnum.TFIDF,\n",
    "        \"tokenizer\": TokenizerEnum.SENTENCEPIECE,\n",
    "        \"clean_tweets\": False\n",
    "    }\n",
    "}\n",
    "\n",
    "nn_yor_wp = neural_network(**nn_yor_kwargs[\"nn_yor_wp_clean\"])\n",
    "nn_yor_wp_no_clean = neural_network(**nn_yor_kwargs[\"nn_yor_wp_no_clean\"])\n",
    "nn_yor_sp = neural_network(**nn_yor_kwargs[\"nn_yor_sp_clean\"])\n",
    "nn_yor_sp_no_clean = neural_network(**nn_yor_kwargs[\"nn_yor_sp_no_clean\"])\n",
    "\n",
    "\n",
    "compare_results(\n",
    "    normal_result=nn_yor_wp,\n",
    "    subword_result=nn_yor_sp,\n",
    ")\n",
    "\n",
    "compare_results(\n",
    "    normal_result=nn_yor_sp,\n",
    "    subword_result=nn_yor_wp,\n",
    ")\n",
    "\n",
    "compare_results(\n",
    "    normal_result=nn_yor_wp_no_clean,\n",
    "    subword_result=nn_yor_sp_no_clean,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result1 = neural_network(\n",
    "    dataset=YOR_DATASET,\n",
    "    vectorizer=VectorizerEnum.TFIDF,\n",
    "    tokenizer=TokenizerEnum.WORDPIECE,\n",
    ")\n",
    "result2 = neural_network(\n",
    "    dataset=YOR_DATASET,\n",
    "    vectorizer=VectorizerEnum.TFIDF,\n",
    "    tokenizer=TokenizerEnum.WORDPIECE,\n",
    "    clean_tweets=False\n",
    ")\n",
    "\n",
    "compare_results(normal_result=result1, subword_result=result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result1 = logistic_regression(dataset=YOR_DATASET, vectorizer=VectorizerEnum.TFIDF)\n",
    "result2 = logistic_regression(\n",
    "    dataset=YOR_DATASET,\n",
    "    vectorizer=VectorizerEnum.TFIDF,\n",
    "    tokenizer=TokenizerEnum.WORDPIECE,\n",
    ")\n",
    "\n",
    "compare_results(normal_result=result1, subword_result=result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result1 = naive_bayes(dataset=YOR_DATASET, vectorizer=VectorizerEnum.TFIDF)\n",
    "result2 = naive_bayes(\n",
    "    dataset=YOR_DATASET,\n",
    "    vectorizer=VectorizerEnum.TFIDF,\n",
    "    tokenizer=TokenizerEnum.WORDPIECE,\n",
    ")\n",
    "\n",
    "compare_results(normal_result=result1, subword_result=result2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
