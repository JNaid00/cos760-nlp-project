{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository exists. Updating...\n",
      "Repository exists. Updating...\n"
     ]
    }
   ],
   "source": [
    "from custom_datasets import MultiLangDataset, SplitSet\n",
    "from custom_datasets import ns_dataset\n",
    "from custom_datasets import Languages\n",
    "from constants import TokenizerEnum, VectorizerEnum\n",
    "from custom_vectorizers import get_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YOR_DATASET: SplitSet = ns_dataset.get(Languages.YORUBA)\n",
    "HAU_DATASET: SplitSet = ns_dataset.get(Languages.HAUSA)\n",
    "IBO_DATASET: SplitSet = ns_dataset.get(Languages.IGBO)\n",
    "PCM_DATASET: SplitSet = ns_dataset.get(Languages.NIGERIAN_PIDGIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluator\n",
    "from analysis import compare_results\n",
    "\n",
    "# Tokenizer\n",
    "from subword_tokenizer import (\n",
    "    get_tokenizer,\n",
    "    wordpiece_tokenize_dataframe,\n",
    "    get_sentencepiece_tokenizer,\n",
    "    sentencepiece_tokenize_dataframe,\n",
    "    get_wordpiece_tokeized_data,\n",
    ")\n",
    "\n",
    "# Compare Results\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_labels(df: pd.DataFrame):\n",
    "    label_mapping = {\"negative\": 0, \"neutral\": 1, \"positive\": 2}\n",
    "    df[\"label_encoded\"] = df[\"label\"].str.lower().map(label_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes(\n",
    "    dataset: SplitSet, vectorizer: VectorizerEnum,  tokenizer: Optional[TokenizerEnum] = None\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Naive Bayes classifier for text classification.\n",
    "    Args:\n",
    "        dataset (SplitSet): The dataset to use for training and testing.\n",
    "        tokenizer (TokenizerEnum): The tokenizer to use.\n",
    "        If tokenizer is None, then no tokenization is applied. Only vectorization is applied.\n",
    "        vectorizer (VectorizerEnum): The vectorizer to use.\n",
    "    Returns:\n",
    "        dict: A dictionary containing the results of the classification.\n",
    "        Returns a classification report\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(\n",
    "    dataset: SplitSet, vectorizer: VectorizerEnum,  tokenizer: Optional[TokenizerEnum] = None\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Logistic Regression classifier for text classification.\n",
    "    Args:\n",
    "        dataset (SplitSet): The dataset to use for training and testing.\n",
    "        tokenizer (TokenizerEnum): The tokenizer to use.\n",
    "        If tokenizer is None, then no tokenization is applied. Only vectorization is applied.\n",
    "        vectorizer (VectorizerEnum): The vectorizer to use.\n",
    "    Returns:\n",
    "        dict: A dictionary containing the results of the classification.\n",
    "        Returns a classification report\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_network(\n",
    "    dataset: SplitSet, vectorizer: VectorizerEnum,  tokenizer: Optional[TokenizerEnum] = None\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Neural Network classifier for text classification.\n",
    "    Args:\n",
    "        dataset (SplitSet): The dataset to use for training and testing.\n",
    "        tokenizer (TokenizerEnum): The tokenizer to use.\n",
    "        If tokenizer is None, then no tokenization is applied. Only vectorization is applied.\n",
    "        vectorizer (VectorizerEnum): The vectorizer to use.\n",
    "    Returns:\n",
    "        dict: A dictionary containing the results of the classification.\n",
    "        Returns a classification report\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "    df = ns_dataset.get(Languages.HAUSA).train\n",
    "    # encode_labels(df)\n",
    "\n",
    "    # df['cleaned_tweet'] = df['tweet'].apply(clean_tweet)\n",
    "\n",
    "    neural_input = get_wordpiece_tokeized_data(\n",
    "        df,\n",
    "        vocab_size=3700,\n",
    "        tweet_column=\"cleaned_tweet\",\n",
    "        vectorizer_kwargs={\"ngram\": (1, 2), \"max_features\": None},\n",
    "    )\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(neural_input.X_train.shape[1],)))\n",
    "\n",
    "    # Dense layers for TF-IDF input\n",
    "    # (512, 256, 128)\n",
    "    # (8, 4, 2)\n",
    "    model.add(Dense(16))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(16))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(8))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(3, activation=\"softmax\"))\n",
    "\n",
    "    model.compile(\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        optimizer=Adam(),\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "\n",
    "    X = np.array(neural_input.get_dense_X_train())\n",
    "    y = np.array(neural_input.y_train)\n",
    "\n",
    "    model.fit(X, y, epochs=100, batch_size=64, verbose=1)\n",
    "\n",
    "    # Evaluate the model\n",
    "    # model.evaluate(neural_input.X_test, neural_input.y_test)\n",
    "    y_pred = model.predict(\n",
    "        np.array(neural_input.get_dense_X_test()),\n",
    "    )\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    accuracy = accuracy_score(neural_input.y_test, y_pred_classes)\n",
    "    print(f\"Accuracy with filtered tweets {accuracy:.4f}\")\n",
    "\n",
    "    print(\n",
    "        classification_report(\n",
    "            neural_input.y_test,\n",
    "            y_pred_classes,\n",
    "            target_names=[\"positive\", \"neutral\", \"negative\"],\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result1 = naive_bayes(dataset=YOR_DATASET, vectorizer=VectorizerEnum.TFIDF)\n",
    "# result2 = naive_bayes(\n",
    "#     dataset=YOR_DATASET,\n",
    "#     vectorizer=VectorizerEnum.TFIDF,\n",
    "#     tokenizer=TokenizerEnum.WORDPIECE,\n",
    "# )\n",
    "\n",
    "# compare_results(result1=result1, result2=result2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
