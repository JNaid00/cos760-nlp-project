{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_datasets import MultiLangDataset, SplitSet\n",
    "from custom_datasets import ns_dataset\n",
    "from custom_datasets import Languages\n",
    "from custom_datasets import clean_tweet\n",
    "from constants import TokenizerEnum, VectorizerEnum\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from custom_vectorizers import get_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, GlobalAveragePooling1D, Dense, Dropout,Input, BatchNormalization, Activation\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "YOR_DATASET: SplitSet = ns_dataset.get(Languages.YORUBA)\n",
    "HAU_DATASET: SplitSet = ns_dataset.get(Languages.HAUSA)\n",
    "IBO_DATASET: SplitSet = ns_dataset.get(Languages.IGBO)\n",
    "PCM_DATASET: SplitSet = ns_dataset.get(Languages.NIGERIAN_PIDGIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluator\n",
    "from analysis import compare_results\n",
    "\n",
    "# Tokenizer\n",
    "from subword_tokenizer import (\n",
    "    get_tokenizer,\n",
    "    wordpiece_tokenize_dataframe,\n",
    "    get_sentencepiece_tokenizer,\n",
    "    sentencepiece_tokenize_dataframe,\n",
    "    get_wordpiece_tokeized_data,\n",
    "    get_sentencepiece_tokeized_data\n",
    ")\n",
    "\n",
    "# Compare Results\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_labels(df: pd.DataFrame):\n",
    "    label_mapping = {\"positive\": 0, \"neutral\": 1, \"negative\": 2}\n",
    "    df[\"label_encoded\"] = df[\"label\"].str.lower().map(label_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VECTORIZER_KWARGS = {\n",
    "#     \"ngram\": (1, 2),\n",
    "#     \"max_features\": 3700,}\n",
    "\n",
    "VECTORIZER_KWARGS = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "\n",
    "def naive_bayes(\n",
    "    dataset: SplitSet, vectorizer: VectorizerEnum,  tokenizer: Optional[TokenizerEnum] = None\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Naive Bayes classifier for text classification.\n",
    "    Args:\n",
    "        dataset (SplitSet): The dataset to use for training and testing.\n",
    "        tokenizer (TokenizerEnum): The tokenizer to use.\n",
    "        If tokenizer is None, then no tokenization is applied. Only vectorization is applied.\n",
    "        vectorizer (VectorizerEnum): The vectorizer to use.\n",
    "    Returns:\n",
    "        dict: A dictionary containing the results of the classification.\n",
    "        Returns a classification report\n",
    "    \"\"\"\n",
    "    \n",
    "    X_train, y_train = dataset.train[\"tweet\"], dataset.train[\"label\"]\n",
    "    X_test, y_test = dataset.test[\"tweet\"], dataset.test[\"label\"]\n",
    "\n",
    "    if tokenizer is TokenizerEnum.SENTENCEPIECE:\n",
    "        print(\"Using SentencePiece tokenizer\")\n",
    "        sp_tokenizer = get_sentencepiece_tokenizer(\n",
    "            df=dataset.train,\n",
    "        )\n",
    "\n",
    "        def sentencepiece_tokenizer(text):\n",
    "            return sp_tokenizer.encode(text, out_type=str)\n",
    "\n",
    "        vectorizer = (\n",
    "            CountVectorizer(tokenizer=sentencepiece_tokenizer)\n",
    "            if vectorizer == VectorizerEnum.BOW\n",
    "            else TfidfVectorizer(tokenizer=sentencepiece_tokenizer)\n",
    "        )\n",
    "    elif tokenizer is TokenizerEnum.WORDPIECE:\n",
    "        print(\"Using WordPiece tokenizer\")\n",
    "        wp_tokenizer = get_tokenizer(\n",
    "            df=dataset.train,\n",
    "        )\n",
    "\n",
    "        def wordpiece_tokenizer(text):\n",
    "            return wp_tokenizer.encode(text).tokens\n",
    "\n",
    "        vectorizer = (\n",
    "            CountVectorizer(tokenizer=wordpiece_tokenizer)\n",
    "            if vectorizer == VectorizerEnum.BOW\n",
    "            else TfidfVectorizer(tokenizer=wordpiece_tokenizer)\n",
    "        )\n",
    "    elif tokenizer is None:\n",
    "        vectorizer = (\n",
    "            CountVectorizer(stop_words=dataset.stopwords)\n",
    "            if vectorizer == VectorizerEnum.BOW\n",
    "            else TfidfVectorizer(stop_words=dataset.stopwords)\n",
    "        )\n",
    "\n",
    "    model = Pipeline(\n",
    "        [\n",
    "            (\"vectorizer\", vectorizer),\n",
    "            (\"classifier\", MultinomialNB()),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    word_result: dict = classification_report(y_test, y_pred, output_dict=True)\n",
    "    return word_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "def logistic_regression(\n",
    "    dataset: SplitSet, vectorizer: VectorizerEnum,  tokenizer: Optional[TokenizerEnum] = None\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Logistic Regression classifier for text classification.\n",
    "    Args:\n",
    "        dataset (SplitSet): The dataset to use for training and testing.\n",
    "        tokenizer (TokenizerEnum): The tokenizer to use.\n",
    "        If tokenizer is None, then no tokenization is applied. Only vectorization is applied.\n",
    "        vectorizer (VectorizerEnum): The vectorizer to use.\n",
    "    Returns:\n",
    "        dict: A dictionary containing the results of the classification.\n",
    "        Returns a classification report\n",
    "    \"\"\"\n",
    "    X_train, y_train = dataset.train[\"tweet\"], dataset.train[\"label\"]\n",
    "    X_test, y_test = dataset.test[\"tweet\"], dataset.test[\"label\"]\n",
    "\n",
    "    if tokenizer is TokenizerEnum.SENTENCEPIECE:\n",
    "        print(\"Using SentencePiece tokenizer\")\n",
    "        sp_tokenizer = get_sentencepiece_tokenizer(\n",
    "            df=dataset.train,\n",
    "        )\n",
    "\n",
    "        def sentencepiece_tokenizer(text):\n",
    "            return sp_tokenizer.encode(text, out_type=str)\n",
    "\n",
    "        vectorizer = (\n",
    "            CountVectorizer(tokenizer=sentencepiece_tokenizer)\n",
    "            if vectorizer == VectorizerEnum.BOW\n",
    "            else TfidfVectorizer(tokenizer=sentencepiece_tokenizer)\n",
    "        )\n",
    "    elif tokenizer is TokenizerEnum.WORDPIECE:\n",
    "        print(\"Using WordPiece tokenizer\")\n",
    "        wp_tokenizer = get_tokenizer(\n",
    "            df=dataset.train,\n",
    "        )\n",
    "\n",
    "        def wordpiece_tokenizer(text):\n",
    "            return wp_tokenizer.encode(text).tokens\n",
    "\n",
    "        vectorizer = (\n",
    "            CountVectorizer(tokenizer=wordpiece_tokenizer)\n",
    "            if vectorizer == VectorizerEnum.BOW\n",
    "            else TfidfVectorizer(tokenizer=wordpiece_tokenizer)\n",
    "        )\n",
    "    elif tokenizer is None:\n",
    "        vectorizer = (\n",
    "            CountVectorizer(stop_words=dataset.stopwords)\n",
    "            if vectorizer == VectorizerEnum.BOW\n",
    "            else TfidfVectorizer(stop_words=dataset.stopwords)\n",
    "        )\n",
    "\n",
    "    model = Pipeline(\n",
    "        [\n",
    "            (\"vectorizer\", vectorizer),\n",
    "            (\"classifier\", LogisticRegression(max_iter=2000)),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    word_result: dict = classification_report(y_test, y_pred, output_dict=True)\n",
    "    return word_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "def neural_network(\n",
    "    dataset: SplitSet, vectorizer: VectorizerEnum,  tokenizer: Optional[TokenizerEnum] = None, clean_tweets: bool = True\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Neural Network classifier for text classification.\n",
    "    Args:\n",
    "        dataset (SplitSet): The dataset to use for training and testing.\n",
    "        tokenizer (TokenizerEnum): The tokenizer to use.\n",
    "        If tokenizer is None, then no tokenization is applied. Only vectorization is applied.\n",
    "        vectorizer (VectorizerEnum): The vectorizer to use.\n",
    "    Returns:\n",
    "        dict: A dictionary containing the results of the classification.\n",
    "        Returns a classification report\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "    data = deepcopy(dataset)\n",
    "    train_df = data.train\n",
    "    test_df = data.test\n",
    "    # encode_labels(df)\n",
    "    if clean_tweets:\n",
    "        data.train['cleaned_tweet'] = data.train['tweet'].apply(clean_tweet)\n",
    "        data.test['cleaned_tweet'] = data.test['tweet'].apply(clean_tweet)\n",
    "    encode_labels(data.train)\n",
    "    encode_labels(data.test)\n",
    "\n",
    "    if tokenizer is not None:\n",
    "        neural_input = get_wordpiece_tokeized_data(\n",
    "            data,\n",
    "            vocab_size=3700,\n",
    "            tweet_column=\"cleaned_tweet\",\n",
    "            vectorizer_kwargs={\"ngram\": (1, 2), \"max_features\": None},\n",
    "        )\n",
    "    elif tokenizer == TokenizerEnum.SENTENCEPIECE:\n",
    "        neural_input = get_sentencepiece_tokeized_data(\n",
    "            data,\n",
    "            vocab_size=3700,\n",
    "            tweet_column=\"cleaned_tweet\",\n",
    "            vectorizer_kwargs={\"ngram\": (1, 2), \"max_features\": None},\n",
    "        )\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(neural_input.X_train.shape[1],)))\n",
    "\n",
    "    # Dense layers for TF-IDF input\n",
    "    # (512, 256, 128)\n",
    "    # (8, 4, 2)\n",
    "    model.add(Dense(512))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(256))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(128))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(3, activation=\"softmax\"))\n",
    "\n",
    "    model.compile(\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        optimizer=Adam(),\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "\n",
    "    X = np.array(neural_input.get_dense_X_train())\n",
    "    y = np.array(neural_input.y_train)\n",
    "\n",
    "    model.fit(X, y, epochs=10, batch_size=64, verbose=1)\n",
    "\n",
    "    # Evaluate the model\n",
    "    # model.evaluate(neural_input.X_test, neural_input.y_test)\n",
    "    y_pred = model.predict(\n",
    "        np.array(neural_input.get_dense_X_test()),\n",
    "    )\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    accuracy = accuracy_score(neural_input.y_test, y_pred_classes)\n",
    "    print(f\"Accuracy with filtered tweets {accuracy:.4f}\")\n",
    "\n",
    "    \n",
    "    return classification_report(\n",
    "            neural_input.y_test,\n",
    "            y_pred_classes,\n",
    "            target_names=[\"positive\", \"neutral\", \"negative\"],\n",
    "            output_dict=True\n",
    "        )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_yor_kwargs ={\n",
    "    \"nn_yor_wp_clean\":{\n",
    "        \"dataset\": YOR_DATASET,\n",
    "        \"vectorizer\": VectorizerEnum.TFIDF,\n",
    "        \"tokenizer\": TokenizerEnum.WORDPIECE,\n",
    "        \"clean_tweets\": True\n",
    "    },\n",
    "    \"nn_yor_wp_no_clean\":{\n",
    "        \"dataset\": YOR_DATASET,\n",
    "        \"vectorizer\": VectorizerEnum.TFIDF,\n",
    "        \"tokenizer\": TokenizerEnum.WORDPIECE,\n",
    "        \"clean_tweets\": False\n",
    "    },\n",
    "    \"nn_yor_sp_clean\" : {\n",
    "        \"dataset\": YOR_DATASET,\n",
    "        \"vectorizer\": VectorizerEnum.TFIDF,\n",
    "        \"tokenizer\": TokenizerEnum.SENTENCEPIECE,\n",
    "        \"clean_tweets\": True\n",
    "    },\n",
    "    \"nn_yor_sp_no_clean\" : {\n",
    "        \"dataset\": YOR_DATASET,\n",
    "        \"vectorizer\": VectorizerEnum.TFIDF,\n",
    "        \"tokenizer\": TokenizerEnum.SENTENCEPIECE,\n",
    "        \"clean_tweets\": False\n",
    "    }\n",
    "}\n",
    "\n",
    "nn_hau_kwargs ={\n",
    "    \"nn_hau_wp_clean\":{\n",
    "        \"dataset\": HAU_DATASET,\n",
    "        \"vectorizer\": VectorizerEnum.TFIDF,\n",
    "        \"tokenizer\": TokenizerEnum.WORDPIECE,\n",
    "        \"clean_tweets\": True\n",
    "    },\n",
    "    \"nn_hau_wp_no_clean\":{\n",
    "        \"dataset\": HAU_DATASET,\n",
    "        \"vectorizer\": VectorizerEnum.TFIDF,\n",
    "        \"tokenizer\": TokenizerEnum.WORDPIECE,\n",
    "        \"clean_tweets\": False\n",
    "    },\n",
    "    \"nn_hau_sp_clean\" : {\n",
    "        \"dataset\": HAU_DATASET,\n",
    "        \"vectorizer\": VectorizerEnum.TFIDF,\n",
    "        \"tokenizer\": TokenizerEnum.SENTENCEPIECE,\n",
    "        \"clean_tweets\": True\n",
    "    },\n",
    "    \"nn_hau_sp_no_clean\" : {\n",
    "        \"dataset\": HAU_DATASET,\n",
    "        \"vectorizer\": VectorizerEnum.TFIDF,\n",
    "        \"tokenizer\": TokenizerEnum.SENTENCEPIECE,\n",
    "        \"clean_tweets\": False\n",
    "    }\n",
    "}\n",
    "\n",
    "nn_pcm_kwargs ={\n",
    "    \"nn_pcm_wp_clean\":{\n",
    "        \"dataset\": PCM_DATASET,\n",
    "        \"vectorizer\": VectorizerEnum.TFIDF,\n",
    "        \"tokenizer\": TokenizerEnum.WORDPIECE,\n",
    "        \"clean_tweets\": True\n",
    "    },\n",
    "    \"nn_pcm_wp_no_clean\":{\n",
    "        \"dataset\": PCM_DATASET,\n",
    "        \"vectorizer\": VectorizerEnum.TFIDF,\n",
    "        \"tokenizer\": TokenizerEnum.WORDPIECE,\n",
    "        \"clean_tweets\": False\n",
    "    },\n",
    "    \"nn_pcm_sp_clean\" : {\n",
    "        \"dataset\": PCM_DATASET,\n",
    "        \"vectorizer\": VectorizerEnum.TFIDF,\n",
    "        \"tokenizer\": TokenizerEnum.SENTENCEPIECE,\n",
    "        \"clean_tweets\": True\n",
    "    },\n",
    "    \"nn_pcm_sp_no_clean\" : {\n",
    "        \"dataset\": PCM_DATASET,\n",
    "        \"vectorizer\": VectorizerEnum.TFIDF,\n",
    "        \"tokenizer\": TokenizerEnum.SENTENCEPIECE,\n",
    "        \"clean_tweets\": False\n",
    "    }\n",
    "}\n",
    "\n",
    "nn_ibo_kwargs ={\n",
    "    \"nn_ibo_wp_clean\":{\n",
    "        \"dataset\": IBO_DATASET,\n",
    "        \"vectorizer\": VectorizerEnum.TFIDF,\n",
    "        \"tokenizer\": TokenizerEnum.WORDPIECE,\n",
    "        \"clean_tweets\": True\n",
    "    },\n",
    "    \"nn_ibo_wp_no_clean\":{\n",
    "        \"dataset\": IBO_DATASET,\n",
    "        \"vectorizer\": VectorizerEnum.TFIDF,\n",
    "        \"tokenizer\": TokenizerEnum.WORDPIECE,\n",
    "        \"clean_tweets\": False\n",
    "    },\n",
    "    \"nn_ibo_sp_clean\" : {\n",
    "        \"dataset\": IBO_DATASET,\n",
    "        \"vectorizer\": VectorizerEnum.TFIDF,\n",
    "        \"tokenizer\": TokenizerEnum.SENTENCEPIECE,\n",
    "        \"clean_tweets\": True\n",
    "    },\n",
    "    \"nn_ibo_sp_no_clean\" : {\n",
    "        \"dataset\": IBO_DATASET,\n",
    "        \"vectorizer\": VectorizerEnum.TFIDF,\n",
    "        \"tokenizer\": TokenizerEnum.SENTENCEPIECE,\n",
    "        \"clean_tweets\": False\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_yor_wp = neural_network(**nn_yor_kwargs[\"nn_yor_wp_clean\"])\n",
    "nn_yor_wp_no_clean = neural_network(**nn_yor_kwargs[\"nn_yor_wp_no_clean\"])\n",
    "nn_yor_sp = neural_network(**nn_yor_kwargs[\"nn_yor_sp_clean\"])\n",
    "nn_yor_sp_no_clean = neural_network(**nn_yor_kwargs[\"nn_yor_sp_no_clean\"])\n",
    "\n",
    "\n",
    "nn_hau_wp = neural_network(**nn_hau_kwargs[\"nn_hau_wp_clean\"])\n",
    "nn_hau_wp_no_clean = neural_network(**nn_hau_kwargs[\"nn_hau_wp_no_clean\"])\n",
    "nn_hau_sp = neural_network(**nn_hau_kwargs[\"nn_hau_sp_clean\"])\n",
    "nn_hau_sp_no_clean = neural_network(**nn_hau_kwargs[\"nn_hau_sp_no_clean\"])\n",
    "\n",
    "nn_pcm_wp = neural_network(**nn_pcm_kwargs[\"nn_pcm_wp_clean\"])\n",
    "nn_pcm_wp_no_clean = neural_network(**nn_pcm_kwargs[\"nn_pcm_wp_no_clean\"])\n",
    "nn_pcm_sp = neural_network(**nn_pcm_kwargs[\"nn_pcm_sp_clean\"])\n",
    "nn_pcm_sp_no_clean = neural_network(**nn_pcm_kwargs[\"nn_pcm_sp_no_clean\"])\n",
    "\n",
    "nn_ibo_wp = neural_network(**nn_ibo_kwargs[\"nn_ibo_wp_clean\"])\n",
    "nn_ibo_wp_no_clean = neural_network(**nn_ibo_kwargs[\"nn_ibo_wp_no_clean\"])\n",
    "nn_ibo_sp = neural_network(**nn_ibo_kwargs[\"nn_ibo_sp_clean\"])\n",
    "nn_ibo_sp_no_clean = neural_network(**nn_ibo_kwargs[\"nn_ibo_sp_no_clean\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_results(\n",
    "    normal_result=nn_yor_wp,\n",
    "    subword_result=nn_yor_sp,\n",
    ")\n",
    "\n",
    "compare_results(\n",
    "    normal_result=nn_yor_sp,\n",
    "    subword_result=nn_yor_wp,\n",
    ")\n",
    "\n",
    "compare_results(\n",
    "    normal_result=nn_yor_wp_no_clean,\n",
    "    subword_result=nn_yor_sp_no_clean,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jesse\\Dev\\cos760-nlp-project\\.venv\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:402: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['fẹ', 'jẹ', 'rẹ', 'rọ', 'wọ'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "lg_yor_tfidf = logistic_regression(dataset=YOR_DATASET, vectorizer=VectorizerEnum.TFIDF)\n",
    "# lg_yor_bow = logistic_regression(dataset=YOR_DATASET,vectorizer=VectorizerEnum.BOW)\n",
    " \n",
    "# lg_pcm_tfidf = logistic_regression(dataset=PCM_DATASET, vectorizer=VectorizerEnum.TFIDF)\n",
    "# lg_pcm_bow = logistic_regression(dataset=PCM_DATASET, vectorizer=VectorizerEnum.BOW)\n",
    " \n",
    "# lg_ibo_tfidf = logistic_regression(dataset=IBO_DATASET, vectorizer=VectorizerEnum.TFIDF)\n",
    "# lg_ibo_bow = logistic_regression(dataset=IBO_DATASET, vectorizer=VectorizerEnum.BOW)\n",
    " \n",
    "# lg_hau_tfidf = logistic_regression(dataset=HAU_DATASET, vectorizer=VectorizerEnum.TFIDF)\n",
    "# lg_hau_bow = logistic_regression(dataset=HAU_DATASET, vectorizer=VectorizerEnum.BOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Logistic Regression Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_yor_tfidf = naive_bayes(dataset=YOR_DATASET, vectorizer=VectorizerEnum.TFIDF)\n",
    "nb_yor_bow = naive_bayes(dataset=YOR_DATASET, vectorizer=VectorizerEnum.BOW)\n",
    " \n",
    "nb_pcm_tfidf = naive_bayes(dataset=PCM_DATASET, vectorizer=VectorizerEnum.TFIDF)\n",
    "nb_pcm_bow = naive_bayes(dataset=PCM_DATASET, vectorizer=VectorizerEnum.BOW)\n",
    " \n",
    "nb_ibo_tfidf = naive_bayes(dataset=IBO_DATASET, vectorizer=VectorizerEnum.TFIDF)\n",
    "nb_ibo_bow = naive_bayes(dataset=IBO_DATASET, vectorizer=VectorizerEnum.BOW)\n",
    " \n",
    "nb_hau_tfidf = naive_bayes(dataset=HAU_DATASET, vectorizer=VectorizerEnum.TFIDF)\n",
    "nb_hau_bow = naive_bayes(dataset=HAU_DATASET, vectorizer=VectorizerEnum.BOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Naive Bayes Results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
