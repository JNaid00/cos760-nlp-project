{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "232c4b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "from typing import Dict, List, Optional\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6be9680c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository exists. Updating...\n",
      "Repository exists. Updating...\n"
     ]
    }
   ],
   "source": [
    "REPO_URL_NS = \"https://github.com/hausanlp/NaijaSenti.git\"\n",
    "LOCAL_DIR_NS = \"NaijaSenti\"\n",
    "\n",
    "REPO_URL_AS = \"https://github.com/afrisenti-semeval/afrisent-semeval-2023.git\"\n",
    "LOCAL_DIR_AS = \"afrisent-semeval-2023\"\n",
    "\n",
    "def clone_repo(repo_url: str, local_dir: str) -> None:\n",
    "    if os.path.isdir(local_dir):\n",
    "        print(\"Repository exists. Updating...\")\n",
    "        subprocess.run([\"git\", \"-C\", local_dir, \"pull\", \"origin\", \"main\"], check=True)\n",
    "    else:\n",
    "        print(\"Repository not found. Cloning...\")\n",
    "        subprocess.run([\"git\", \"clone\", repo_url], check=True)\n",
    "\n",
    "clone_repo(REPO_URL_NS, LOCAL_DIR_NS)\n",
    "clone_repo(REPO_URL_AS, LOCAL_DIR_AS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d0d2fc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SplitSet:\n",
    "    \"\"\"\n",
    "    Holds the train, test, dev splits and stopwords for a single language.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 train: pd.DataFrame,\n",
    "                 test: pd.DataFrame,\n",
    "                 dev: pd.DataFrame,\n",
    "                 stopwords: Optional[List[str]] = None):\n",
    "        self.train = train\n",
    "        self.test = test\n",
    "        self.dev = dev\n",
    "        self.stopwords = stopwords if stopwords else []\n",
    "\n",
    "    def summary(self):\n",
    "        return {\n",
    "            \"train_size\": len(self.train),\n",
    "            \"test_size\": len(self.test),\n",
    "            \"dev_size\": len(self.dev),\n",
    "            \"num_stopwords\": len(self.stopwords),\n",
    "        }\n",
    "\n",
    "\n",
    "class MultiLangDataset:\n",
    "    \"\"\"\n",
    "    Manages NLP datasets split by language. Each language contains train/test/dev and stopwords.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.languages: Dict[str, SplitSet] = {}\n",
    "\n",
    "    def add_language(self, lang_code: str, split_set: SplitSet):\n",
    "        self.languages[lang_code] = split_set\n",
    "\n",
    "    def get(self, lang_code: str) -> Optional[SplitSet]:\n",
    "        return self.languages.get(lang_code)\n",
    "\n",
    "    def summary(self) -> Dict[str, Dict[str, int]]:\n",
    "        return {lang: split.summary() for lang, split in self.languages.items()}\n",
    "\n",
    "    def all_languages(self) -> List[str]:\n",
    "        return list(self.languages.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ddf8da7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ns_languages = ['hau', 'ibo', 'pcm', 'yor']\n",
    "class Languages:\n",
    "    \"\"\"\n",
    "    Contains the language codes for NaijaSenti dataset.\n",
    "    \"\"\"\n",
    "    HAUSA = 'hau'\n",
    "    IGBO = 'ibo'\n",
    "    NIGERIAN_PIDGIN = 'pcm'\n",
    "    YORUBA  = 'yor'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f37549",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_local_datasets(local_base_dir, languages=ns_languages, splits=['dev','test','train']):\n",
    "    dataset = MultiLangDataset()\n",
    "    \n",
    "    for lang in languages:\n",
    "        split_data = {}\n",
    "        for split in splits:\n",
    "            path = os.path.join(local_base_dir, lang, f\"{split}.tsv\")\n",
    "            try:\n",
    "                df = pd.read_csv(path, sep='\\t', encoding='utf-8')\n",
    "                # dataset[lang][split] = df\n",
    "                # dataset.add_language(lang, df)\n",
    "                split_data[split] = df\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to load {path}: {e}\")\n",
    "\n",
    "        # Read in stopwords\n",
    "        if local_base_dir.startswith(LOCAL_DIR_NS):\n",
    "            path = os.path.join(f'{LOCAL_DIR_NS}/data/stopwords/{lang}.csv')\n",
    "            try:\n",
    "                stopwords_df = pd.read_csv(path, encoding='utf-8')\n",
    "                split_data['stopwords'] = stopwords_df['word'].tolist()\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to load stopwords for {lang} from {path}: {e}\")\n",
    "\n",
    "        split_set = SplitSet(\n",
    "            train=split_data.get('train', pd.DataFrame()),\n",
    "            test=split_data.get('test', pd.DataFrame()),\n",
    "            dev=split_data.get('dev', pd.DataFrame()),\n",
    "            stopwords=split_data.get('stopwords', [])\n",
    "        )\n",
    "        dataset.add_language(lang, split_set)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "42b5a129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded datasets for languages: ['hau', 'ibo', 'pcm', 'yor']\n"
     ]
    }
   ],
   "source": [
    "ns_dataset: MultiLangDataset = load_local_datasets(local_base_dir=LOCAL_DIR_NS + '/data/annotated_tweets', languages=ns_languages) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947ce503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded datasets for languages: ['hau', 'ibo', 'pcm', 'yor']\n"
     ]
    }
   ],
   "source": [
    "as_dataset: MultiLangDataset = load_local_datasets(local_base_dir=f'afrisent-semeval-2023/data', languages=ns_languages,)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
