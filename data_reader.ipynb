{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232c4b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "from typing import Dict, List, Optional\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be9680c",
   "metadata": {},
   "outputs": [],
   "source": [
    "REPO_URL_NS = \"https://github.com/hausanlp/NaijaSenti.git\"\n",
    "LOCAL_DIR_NS = \"NaijaSenti\"\n",
    "\n",
    "REPO_URL_AS = \"https://github.com/afrisenti-semeval/afrisent-semeval-2023.git\"\n",
    "LOCAL_DIR_AS = \"afrisent-semeval-2023\"\n",
    "\n",
    "def clone_repo(repo_url: str, local_dir: str) -> None:\n",
    "    if os.path.isdir(local_dir):\n",
    "        print(\"Repository exists. Updating...\")\n",
    "        subprocess.run([\"git\", \"-C\", local_dir, \"pull\", \"origin\", \"main\"], check=True)\n",
    "    else:\n",
    "        print(\"Repository not found. Cloning...\")\n",
    "        subprocess.run([\"git\", \"clone\", repo_url], check=True)\n",
    "\n",
    "clone_repo(REPO_URL_NS, LOCAL_DIR_NS)\n",
    "clone_repo(REPO_URL_AS, LOCAL_DIR_AS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d2fc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SplitSet:\n",
    "    \"\"\"\n",
    "    Holds the train, test, dev splits and stopwords for a single language.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 train: pd.DataFrame,\n",
    "                 test: pd.DataFrame,\n",
    "                 dev: pd.DataFrame,\n",
    "                 stopwords: Optional[List[str]] = None):\n",
    "        self.train = train\n",
    "        self.test = test\n",
    "        self.dev = dev\n",
    "        self.stopwords = stopwords if stopwords else []\n",
    "\n",
    "    def summary(self):\n",
    "        return {\n",
    "            \"train_size\": len(self.train),\n",
    "            \"test_size\": len(self.test),\n",
    "            \"dev_size\": len(self.dev),\n",
    "            \"num_stopwords\": len(self.stopwords),\n",
    "        }\n",
    "\n",
    "\n",
    "class MultiLangDataset:\n",
    "    \"\"\"\n",
    "    Manages NLP datasets split by language. Each language contains train/test/dev and stopwords.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.languages: Dict[str, SplitSet] = {}\n",
    "\n",
    "    def add_language(self, lang_code: str, split_set: SplitSet):\n",
    "        self.languages[lang_code] = split_set\n",
    "\n",
    "    def get(self, lang_code: str) -> Optional[SplitSet]:\n",
    "        return self.languages.get(lang_code)\n",
    "\n",
    "    def summary(self) -> Dict[str, Dict[str, int]]:\n",
    "        return {lang: split.summary() for lang, split in self.languages.items()}\n",
    "\n",
    "    def all_languages(self) -> List[str]:\n",
    "        return list(self.languages.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf8da7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ns_languages = ['hau', 'ibo', 'pcm', 'yor']\n",
    "class Languages:\n",
    "    \"\"\"\n",
    "    Contains the language codes for NaijaSenti dataset.\n",
    "    \"\"\"\n",
    "    HAUSA = 'hau'\n",
    "    IGBO = 'ibo'\n",
    "    NIGERIAN_PIDGIN = 'pcm'\n",
    "    YORUBA  = 'yor'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f37549",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_local_datasets(local_base_dir, languages=ns_languages, splits=['dev','test','train']):\n",
    "    dataset = MultiLangDataset()\n",
    "    \n",
    "    for lang in languages:\n",
    "        split_data = {}\n",
    "        for split in splits:\n",
    "            path = os.path.join(local_base_dir, lang, f\"{split}.tsv\")\n",
    "            try:\n",
    "                df = pd.read_csv(path, sep='\\t', encoding='utf-8')\n",
    "                # dataset[lang][split] = df\n",
    "                # dataset.add_language(lang, df)\n",
    "                split_data[split] = df\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to load {path}: {e}\")\n",
    "\n",
    "        # Read in stopwords\n",
    "        if local_base_dir.startswith(LOCAL_DIR_NS):\n",
    "            path = os.path.join(f'{LOCAL_DIR_NS}/data/stopwords/{lang}.csv')\n",
    "            try:\n",
    "                stopwords_df = pd.read_csv(path, encoding='utf-8')\n",
    "                split_data['stopwords'] = stopwords_df['word'].tolist()\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to load stopwords for {lang} from {path}: {e}\")\n",
    "\n",
    "        split_set = SplitSet(\n",
    "            train=split_data.get('train', pd.DataFrame()),\n",
    "            test=split_data.get('test', pd.DataFrame()),\n",
    "            dev=split_data.get('dev', pd.DataFrame()),\n",
    "            stopwords=split_data.get('stopwords', [])\n",
    "        )\n",
    "        dataset.add_language(lang, split_set)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b5a129",
   "metadata": {},
   "outputs": [],
   "source": [
    "ns_dataset: MultiLangDataset = load_local_datasets(local_base_dir=LOCAL_DIR_NS + '/data/annotated_tweets', languages=ns_languages) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947ce503",
   "metadata": {},
   "outputs": [],
   "source": [
    "as_dataset: MultiLangDataset = load_local_datasets(local_base_dir=f'afrisent-semeval-2023/data', languages=ns_languages,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d6eb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"NaijaSenti dataset loaded with languages:\", ns_dataset.all_languages())\n",
    "print(\"Afrisenti dataset loaded with languages:\", as_dataset.all_languages())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d903e898",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"NaijaSenti hau: \", ns_dataset.get(Languages.HAUSA).dev)\n",
    "# Print each row in the dev set for the column 'tweet'\n",
    "for index, row in ns_dataset.get(Languages.HAUSA).dev.iterrows():\n",
    "    print(f\"Index: {index}, Tweet: {row['tweet']}\")\n",
    "\n",
    "# write all the tweets into a textfile\n",
    "# check if the dir data exists, if not create it\n",
    "if not os.path.exists('data'):\n",
    "    os.makedirs('data')\n",
    "with open('data/naija_senti_hau_dev_tweets.txt', 'w', encoding='utf-8') as f:\n",
    "    for index, row in ns_dataset.get(Languages.HAUSA).dev.iterrows():\n",
    "        f.write(f\"{row['tweet']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9238776b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordPiece\n",
    "from tokenizers.trainers import WordPieceTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.normalizers import Sequence, Lowercase, NFD, StripAccents\n",
    "\n",
    "tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n",
    "tokenizer.normalizer = Sequence([NFD(), Lowercase(), StripAccents()])\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "trainer = WordPieceTrainer(vocab_size=8000, special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"])\n",
    "tokenizer.train([\"data/naija_senti_hau_dev_tweets.txt\"], trainer)\n",
    "tokenizer.save(\"data/wordpiece.json\")\n",
    "\n",
    "with open('data/naija_senti_hau_dev_tweets_tokenized.txt', 'w', encoding='utf-8') as f:\n",
    "    for index, row in ns_dataset.get(Languages.HAUSA).dev.iterrows():\n",
    "        tokens = tokenizer.encode(row['tweet']).tokens\n",
    "        f.write(\" \".join(tokens) + \"\\n\")\n",
    "\n",
    "# adjust the below to read into a list of strings\n",
    "\n",
    "# def read_tokenized_file(file_path: str) -> List[List[str]]:\n",
    "#     \"\"\"\n",
    "#     Reads a tokenized file and returns a list of token lists.\n",
    "#     \"\"\"\n",
    "#     with open(file_path, 'r', encoding='utf-8') as f:\n",
    "#         return [line.strip().split() for line in f.readlines()]\n",
    "    \n",
    "def read_tokenized_file(file_path: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Reads a tokenized file and returns a list of strings.\n",
    "    Each string is a space-separated sequence of tokens.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return [line.strip() for line in f.readlines()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ccc6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def compute_tf(texts):\n",
    "    tf_list = []\n",
    "    for text in texts:\n",
    "        counts = Counter(text)\n",
    "        total = len(text)\n",
    "        tf = {subword: count / total for subword, count in counts.items()}\n",
    "        tf_list.append(tf)\n",
    "    return tf_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ce3c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "def compute_idf(texts):\n",
    "    N = len(texts)\n",
    "    df = defaultdict(int)\n",
    "    for text in texts:\n",
    "        unique = set(text)\n",
    "        for subword in unique:\n",
    "            df[subword] += 1\n",
    "    idf = {subword: math.log(N / (1 + df_val)) for subword, df_val in df.items()}\n",
    "    return idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd3ae1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tfidf(tf_list, idf):\n",
    "    tfidf_list = []\n",
    "    for tf in tf_list:\n",
    "        tfidf = {subword: tf_val * idf.get(subword, 0.0) for subword, tf_val in tf.items()}\n",
    "        tfidf_list.append(tfidf)\n",
    "    return tfidf_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e44a002",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data = read_tokenized_file('data/naija_senti_hau_dev_tweets_tokenized.txt')\n",
    "tf_list = compute_tf(tokenized_data)\n",
    "idf = compute_idf(tokenized_data)\n",
    "tfidf_list = compute_tfidf(tf_list, idf)\n",
    "\n",
    "# Save the TF-IDF results to a file\n",
    "with open('data/naija_senti_hau_dev_tweets_tfidf.txt', 'w', encoding='utf-8') as f:\n",
    "    for i, tfidf in enumerate(tfidf_list):\n",
    "        f.write(f\"Tweet {i}:\\n\")\n",
    "        for subword, score in tfidf.items():\n",
    "            f.write(f\"{subword}: {score}\\n\")\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e0f9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer.from_file(\"data/wordpiece.json\")\n",
    "def wordpiece_tokenizer(text):\n",
    "    return tokenizer.encode(text).tokens\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    tokenizer=wordpiece_tokenizer,\n",
    "    lowercase=False,    \n",
    "    preprocessor=None,\n",
    "    token_pattern=None       \n",
    ")\n",
    "\n",
    "tweets = read_tokenized_file('data/naija_senti_hau_dev_tweets_tokenized.txt')\n",
    "\n",
    "tfidf_matrix = vectorizer.fit_transform(tweets)\n",
    "\n",
    "# display the first 5 rows of the TF-IDF matrix\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "df.to_csv(\"tfidf_matrix.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0078212",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe730df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
