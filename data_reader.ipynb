{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232c4b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "from typing import Dict, List, Optional\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from tokenizers import Tokenizer as WPTokenizer\n",
    "from tokenizers.models import WordPiece\n",
    "from tokenizers.trainers import WordPieceTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.normalizers import Sequence, Lowercase, NFD, StripAccents\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, GlobalAveragePooling1D, Dense, Dropout,Input, BatchNormalization, Activation\n",
    "from tensorflow.keras.utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be9680c",
   "metadata": {},
   "outputs": [],
   "source": [
    "REPO_URL_NS = \"https://github.com/hausanlp/NaijaSenti.git\"\n",
    "LOCAL_DIR_NS = \"NaijaSenti\"\n",
    "\n",
    "REPO_URL_AS = \"https://github.com/afrisenti-semeval/afrisent-semeval-2023.git\"\n",
    "LOCAL_DIR_AS = \"afrisent-semeval-2023\"\n",
    "\n",
    "def clone_repo(repo_url: str, local_dir: str) -> None:\n",
    "    if os.path.isdir(local_dir):\n",
    "        print(\"Repository exists. Updating...\")\n",
    "        subprocess.run([\"git\", \"-C\", local_dir, \"pull\", \"origin\", \"main\"], check=True)\n",
    "    else:\n",
    "        print(\"Repository not found. Cloning...\")\n",
    "        subprocess.run([\"git\", \"clone\", repo_url], check=True)\n",
    "\n",
    "clone_repo(REPO_URL_NS, LOCAL_DIR_NS)\n",
    "clone_repo(REPO_URL_AS, LOCAL_DIR_AS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d2fc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SplitSet:\n",
    "    \"\"\"\n",
    "    Holds the train, test, dev splits and stopwords for a single language.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 train: pd.DataFrame,\n",
    "                 test: pd.DataFrame,\n",
    "                 dev: pd.DataFrame,\n",
    "                 stopwords: Optional[List[str]] = None):\n",
    "        self.train = train\n",
    "        self.test = test\n",
    "        self.dev = dev\n",
    "        self.stopwords = stopwords if stopwords else []\n",
    "\n",
    "    def summary(self):\n",
    "        return {\n",
    "            \"train_size\": len(self.train),\n",
    "            \"test_size\": len(self.test),\n",
    "            \"dev_size\": len(self.dev),\n",
    "            \"num_stopwords\": len(self.stopwords),\n",
    "        }\n",
    "\n",
    "\n",
    "class MultiLangDataset:\n",
    "    \"\"\"\n",
    "    Manages NLP datasets split by language. Each language contains train/test/dev and stopwords.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.languages: Dict[str, SplitSet] = {}\n",
    "\n",
    "    def add_language(self, lang_code: str, split_set: SplitSet):\n",
    "        self.languages[lang_code] = split_set\n",
    "\n",
    "    def get(self, lang_code: str) -> Optional[SplitSet]:\n",
    "        return self.languages.get(lang_code)\n",
    "\n",
    "    def summary(self) -> Dict[str, Dict[str, int]]:\n",
    "        return {lang: split.summary() for lang, split in self.languages.items()}\n",
    "\n",
    "    def all_languages(self) -> List[str]:\n",
    "        return list(self.languages.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf8da7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ns_languages = ['hau', 'ibo', 'pcm', 'yor']\n",
    "class Languages:\n",
    "    \"\"\"\n",
    "    Contains the language codes for NaijaSenti dataset.\n",
    "    \"\"\"\n",
    "    HAUSA = 'hau'\n",
    "    IGBO = 'ibo'\n",
    "    NIGERIAN_PIDGIN = 'pcm'\n",
    "    YORUBA  = 'yor'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f37549",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_local_datasets(local_base_dir, languages=ns_languages, splits=['dev','test','train']):\n",
    "    dataset = MultiLangDataset()\n",
    "    \n",
    "    for lang in languages:\n",
    "        split_data = {}\n",
    "        for split in splits:\n",
    "            path = os.path.join(local_base_dir, lang, f\"{split}.tsv\")\n",
    "            try:\n",
    "                df = pd.read_csv(path, sep='\\t', encoding='utf-8')\n",
    "                # dataset[lang][split] = df\n",
    "                # dataset.add_language(lang, df)\n",
    "                split_data[split] = df\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to load {path}: {e}\")\n",
    "\n",
    "        # Read in stopwords\n",
    "        if local_base_dir.startswith(LOCAL_DIR_NS):\n",
    "            path = os.path.join(f'{LOCAL_DIR_NS}/data/stopwords/{lang}.csv')\n",
    "            try:\n",
    "                stopwords_df = pd.read_csv(path, encoding='utf-8')\n",
    "                split_data['stopwords'] = stopwords_df['word'].tolist()\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to load stopwords for {lang} from {path}: {e}\")\n",
    "\n",
    "        split_set = SplitSet(\n",
    "            train=split_data.get('train', pd.DataFrame()),\n",
    "            test=split_data.get('test', pd.DataFrame()),\n",
    "            dev=split_data.get('dev', pd.DataFrame()),\n",
    "            stopwords=split_data.get('stopwords', [])\n",
    "        )\n",
    "        dataset.add_language(lang, split_set)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b5a129",
   "metadata": {},
   "outputs": [],
   "source": [
    "ns_dataset: MultiLangDataset = load_local_datasets(local_base_dir=LOCAL_DIR_NS + '/data/annotated_tweets', languages=ns_languages) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947ce503",
   "metadata": {},
   "outputs": [],
   "source": [
    "as_dataset: MultiLangDataset = load_local_datasets(local_base_dir=f'afrisent-semeval-2023/data', languages=ns_languages,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d6eb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"NaijaSenti dataset loaded with languages:\", ns_dataset.all_languages())\n",
    "print(\"Afrisenti dataset loaded with languages:\", as_dataset.all_languages())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d903e898",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"NaijaSenti hau: \", ns_dataset.get(Languages.HAUSA).test)\n",
    "# Print each row in the dev set for the column 'tweet'\n",
    "for index, row in ns_dataset.get(Languages.HAUSA).test.iterrows():\n",
    "    print(f\"Index: {index}, Tweet: {row['tweet']}\")\n",
    "\n",
    "# write all the tweets into a textfile\n",
    "# check if the dir data exists, if not create it\n",
    "if not os.path.exists('data'):\n",
    "    os.makedirs('data')\n",
    "with open('data/naija_senti_hau_dev_tweets.txt', 'w', encoding='utf-8') as f:\n",
    "    for index, row in ns_dataset.get(Languages.HAUSA).dev.iterrows():\n",
    "        f.write(f\"{row['tweet']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9238776b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "tokenizer = WPTokenizer(WordPiece(unk_token=\"[UNK]\"))\n",
    "tokenizer.normalizer = Sequence([NFD(), Lowercase(), StripAccents()])\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "trainer = WordPieceTrainer(vocab_size=8000, special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"])\n",
    "df = ns_dataset.get(Languages.HAUSA).dev\n",
    "# Collect all tweets into a single list for training\n",
    "tweets = df['tweet'].tolist()\n",
    "tokenizer.train_from_iterator(tweets, trainer)\n",
    "tokenizer.save(\"data/wordpiece.json\")\n",
    "\n",
    "# Method 3: Add both tokens and IDs\n",
    "df['tokenized_tweets'] = df['tweet'].apply(lambda x: tokenizer.encode(x).tokens)\n",
    "df['token_ids'] = df['tweet'].apply(lambda x: tokenizer.encode(x).ids)\n",
    "\n",
    "# Display results\n",
    "print(\"Original vs Tokenized:\")\n",
    "print(\"=\" * 80)\n",
    "for i in range(min(5, len(df))):  # Show first 5 examples\n",
    "    print(f\"Original: {df.iloc[i]['tweet']}\")\n",
    "    print(f\"Tokens:   {df.iloc[i]['tokenized_tweets']}\")\n",
    "    print(f\"IDs:      {df.iloc[i]['token_ids']}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "with open('data/naija_senti_hau_dev_tweets_tokenized.txt', 'w', encoding='utf-8') as f:\n",
    "    for index, row in ns_dataset.get(Languages.HAUSA).dev.iterrows():\n",
    "        tokens = tokenizer.encode(row['tweet']).tokens\n",
    "        f.write(\" \".join(tokens) + \"\\n\")\n",
    "\n",
    "# adjust the below to read into a list of strings\n",
    "\n",
    "# def read_tokenized_file(file_path: str) -> List[List[str]]:\n",
    "#     \"\"\"\n",
    "#     Reads a tokenized file and returns a list of token lists.\n",
    "#     \"\"\"\n",
    "#     with open(file_path, 'r', encoding='utf-8') as f:\n",
    "#         return [line.strip().split() for line in f.readlines()]\n",
    "    \n",
    "def read_tokenized_file(file_path: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Reads a tokenized file and returns a list of strings.\n",
    "    Each string is a space-separated sequence of tokens.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return [line.strip() for line in f.readlines()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98629eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def preprocess_tweet(tweet):\n",
    "    # Remove all words that start with @ (e.g., @user, @someone123)\n",
    "    return re.sub(r'@\\w+', '', tweet).strip()\n",
    "\n",
    "def wordpiece_tokenize_dataframe(df: pd.DataFrame, tokenizer: Tokenizer) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Tokenizes the 'tweet' column of a DataFrame using a WordPiece tokenizer.\n",
    "    Adds two new columns: 'tokenized_tweets' and 'token_ids'.\n",
    "    \"\"\"\n",
    "    \n",
    "    tokenizer = WPTokenizer(WordPiece(unk_token=\"[UNK]\"))\n",
    "    tokenizer.normalizer = Sequence([NFD(), Lowercase(), StripAccents()])\n",
    "    tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "    # trainer = WordPieceTrainer(vocab_size=8000, special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"])\n",
    "    trainer = WordPieceTrainer(vocab_size=8000, special_tokens=[\"[UNK]\"])\n",
    "    tweets = df['tweet'].tolist()\n",
    "    tweets = [preprocess_tweet(tweet) for tweet in tweets]  # Preprocess tweets\n",
    "    tokenizer.train_from_iterator(tweets, trainer)\n",
    "    df['tokenized_tweets'] = df['tweet'].apply(lambda x: tokenizer.encode(x).tokens)\n",
    "    df['token_ids'] = df['tweet'].apply(lambda x: tokenizer.encode(x).ids)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3182ddf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEncapsulator:\n",
    "    \"\"\"Encapsulates models with train, fit and predict methods.\"\"\"\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict labels for features X.\"\"\"\n",
    "        raise NotImplementedError(\"This method should be implemented by subclasses.\")\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Train the model.\"\"\"\n",
    "        raise NotImplementedError(\"This method should be implemented by subclasses.\")\n",
    "\n",
    "    def perform_pipeline(self, X, y):\n",
    "        \"\"\"\n",
    "        Perform the training and evaluation pipeline.\n",
    "        Returns accuracy and classification report.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"This method should be implemented by subclasses.\")\n",
    "\n",
    "class BasicModelEncapsulator(ModelEncapsulator ):\n",
    "    \"\"\"Encapsulates models with train, fit and predict methods.\"\"\"\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict labels for features X.\"\"\"\n",
    "        return self.model.predict(X)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Alias for train method.\"\"\"\n",
    "        self.model.fit(X, y)\n",
    "\n",
    "    def perform_pipeline(self, X, y):\n",
    "        \"\"\"\"\"\"\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)\n",
    "        self.fit(X_train, y_train)\n",
    "        predictions = self.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, predictions)\n",
    "        report = classification_report(y_test, predictions)\n",
    "        return accuracy, report\n",
    "\n",
    "class NeuralNetworkModel(ModelEncapsulator):\n",
    "    \"\"\"Neural network model designed for TF-IDF features.\"\"\"\n",
    "    def __init__(self, input_dim: int, num_classes: int = 3):\n",
    "        self.input_dim = input_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        \"\"\"Build neural network for TF-IDF features.\"\"\"\n",
    "        model = Sequential()\n",
    "        # Explicitly define the input shape using an Input layer\n",
    "        model.add(Input(shape=(self.input_dim,)))  # ✅ Replaces input_dim in Dense\n",
    "\n",
    "        # Dense layers for TF-IDF input\n",
    "        model.add(Dense(512))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "\n",
    "        model.add(Dropout(0.4))\n",
    "        model.add(Dense(256))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dropout(0.3))\n",
    "        model.add(Dense(self.num_classes, activation='softmax'))\n",
    "        \n",
    "        model.compile(\n",
    "            loss='categorical_crossentropy', \n",
    "            optimizer=Adam(learning_rate=1e-4),\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        return model\n",
    "\n",
    "    def fit(self, X, y, epochs=10, batch_size=32, validation_split=0.2):\n",
    "        \"\"\"Train the neural network.\"\"\"\n",
    "        # Convert sparse matrix to dense if needed\n",
    "        if hasattr(X, 'toarray'):\n",
    "            X = X.toarray()\n",
    "        \n",
    "        # Convert labels to categorical (one-hot encoding)\n",
    "\n",
    "        y_categorical = to_categorical(y, num_classes=self.num_classes)\n",
    "        \n",
    "        history = self.model.fit(\n",
    "            X, y_categorical, \n",
    "            epochs=epochs, \n",
    "            batch_size=batch_size,\n",
    "            validation_split=validation_split,\n",
    "            verbose=0\n",
    "        )\n",
    "        return history\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict labels for features X.\"\"\"\n",
    "        # Convert sparse matrix to dense if needed\n",
    "        if hasattr(X, 'toarray'):\n",
    "            X = X.toarray()\n",
    "        \n",
    "        # Get predictions and convert back to class labels\n",
    "        predictions = self.model.predict(X)\n",
    "        return np.argmax(predictions, axis=-1)\n",
    "    \n",
    "    def perform_pipeline(self, X, y, epochs=10, batch_size=32):\n",
    "        \"\"\"\n",
    "        Perform the training and evaluation pipeline.\n",
    "        Returns accuracy and classification report.\n",
    "        \"\"\"\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.3, random_state=42\n",
    "        )\n",
    "        \n",
    "        self.fit(X_train, y_train, epochs=epochs, batch_size=batch_size)\n",
    "        \n",
    "        predictions = self.predict(X_test)\n",
    "        \n",
    "        accuracy = accuracy_score(y_test, predictions)\n",
    "        report = classification_report(y_test, predictions)\n",
    "        \n",
    "        return accuracy, report\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340fc10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def initialise_tfidf_vectorizer(data, ngram = None, max_features = None):\n",
    "    if ngram and max_features:\n",
    "        vectorizer_tf = TfidfVectorizer( ngram_range=ngram,max_features=max_features)\n",
    "    elif ngram and max_features is None:\n",
    "      vectorizer_tf = TfidfVectorizer(ngram_range=ngram)\n",
    "    elif ngram is None and max_features:\n",
    "      vectorizer_tf = TfidfVectorizer(max_features=max_features)\n",
    "    else:\n",
    "      vectorizer_tf = TfidfVectorizer()\n",
    "    # vectorizer_tfidf = TfidfVectorizer(ngram_range=ngram,max_features=max_features)\n",
    "    vectorizer_tf.fit(data)\n",
    "    X = vectorizer_tf.transform(data)\n",
    "    return X, vectorizer_tf\n",
    "\n",
    "def initialise_count_vectorizer(data, ngram = None, max_features = None):\n",
    "    if ngram and max_features:\n",
    "        vectorizer_count = CountVectorizer(ngram_range=ngram, max_features=max_features)\n",
    "    elif ngram and max_features is None:\n",
    "        vectorizer_count = CountVectorizer(ngram_range=ngram)\n",
    "    elif ngram is None and max_features:\n",
    "        vectorizer_count = CountVectorizer(max_features=max_features)\n",
    "    else:\n",
    "        vectorizer_count = CountVectorizer()\n",
    "    \n",
    "    vectorizer_count.fit(data)\n",
    "    X = vectorizer_count.transform(data)\n",
    "    return X, vectorizer_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0078212",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = ns_dataset.get(Languages.HAUSA).train\n",
    "text_train, text_test, y_train, y_test = train_test_split(df.tweet, df.label, test_size = 0.3)\n",
    "X_train_tfidf, vectorizer_tfidf = initialise_tfidf_vectorizer(text_train)\n",
    "X_train_count, vectorizer_count = initialise_count_vectorizer(text_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1372129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural network model\n",
    "# 1. Dummy tweets and labels\n",
    "\n",
    "df = ns_dataset.get(Languages.HAUSA).dev\n",
    "tweets = df['tweet'].tolist()\n",
    "labels = df['label'].tolist()\n",
    "\n",
    "# Convert labels to numerical format (0 for neutral, 1 for positive, 2 for negative) \n",
    "# not binary classification\n",
    "# 0 = neautral, 1 = positive, 2 = negative\n",
    "labels = [0 if label == 'neutral' else 1 if label == 'positive' else 2 for label in labels]\n",
    "\n",
    "\n",
    "print(\"Class distribution:\", np.bincount(labels))\n",
    "\n",
    "# 2. Preprocess text\n",
    "tokenizer = Tokenizer(num_words=5000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(tweets)\n",
    "sequences = tokenizer.texts_to_sequences(tweets)\n",
    "\n",
    "max_length = min(100, int(np.percentile([len(seq) for seq in sequences], 95)))\n",
    "print(f\"Using max_length: {max_length}\")\n",
    "padded = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
    "\n",
    "# 3. One-hot encode labels (needed for softmax)\n",
    "labels = to_categorical(labels, num_classes=3)\n",
    "\n",
    "# 4. Train-test split\n",
    "# X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "#     padded, labels, test_size=0.2, random_state=42, stratify=labels\n",
    "# )\n",
    "# X_train, X_val, y_train, y_val = train_test_split(\n",
    "#     X_temp, y_temp, test_size=0.25, random_state=42, stratify=y_temp  # 0.25 * 0.8 = 0.2 of total\n",
    "# )\n",
    "\n",
    "# 5. Build model\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=5000, output_dim=32, input_length=max_length),\n",
    "    GlobalAveragePooling1D(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),  # Regularization\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.3),  # Regularization\n",
    "    Dense(3, activation='softmax')\n",
    "])\n",
    "# 6. Compile and train\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# model.summary()\n",
    "# history = model.fit(X_train, y_train, epochs=50, validation_data=(X_val, y_val))\n",
    "\n",
    "# # 7. Evaluate\n",
    "# loss, accuracy = model.evaluate(X_test, y_test)\n",
    "# print(f\"Test Loss: {loss}, Test Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb664a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of features from your TF-IDF matrix\n",
    "# tfidf_features = X_train_tfidf.shape[1]  # Number of TF-IDF features\n",
    "# num_classes = len(np.unique(y_train))    # Number of sentiment classes\n",
    "\n",
    "# Initialize models\n",
    "logistic_regression_model = BasicModelEncapsulator(LogisticRegression(max_iter=1000))\n",
    "naive_bayes_model = BasicModelEncapsulator(MultinomialNB())\n",
    "\n",
    "tfidf_features = X_train_tfidf.shape[1]  # Number of TF-IDF features\n",
    "num_classes = len(np.unique(y_train))    # Number of classes\n",
    "\n",
    "\n",
    "# Initialize the corrected neural network\n",
    "neural_network_model = NeuralNetworkModel(\n",
    "    input_dim=tfidf_features, \n",
    "    num_classes=num_classes\n",
    ")\n",
    "\n",
    "# Perform pipelines\n",
    "print(\"Training models...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b3a23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression with TF-IDF\n",
    "accuracy_lr, report_lr = logistic_regression_model.perform_pipeline(X_train_tfidf, y_train)\n",
    "print(\"Logistic Regression Accuracy:\", accuracy_lr)\n",
    "print(\"Logistic Regression Classification Report:\\n\", report_lr)\n",
    "\n",
    "# Logistic Regression with Count Vectorizer\n",
    "X_train_count, vectorizer_count = initialise_count_vectorizer(text_train)\n",
    "accuracy_lr_count, report_lr_count = logistic_regression_model.perform_pipeline(X_train_count, y_train)\n",
    "print(\"Logistic Regression with Count Vectorizer Accuracy:\", accuracy_lr_count)\n",
    "print(\"Logistic Regression with Count Vectorizer Classification Report:\\n\", report_lr_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad9c304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes with TF-IDF\n",
    "accuracy_nb, report_nb = naive_bayes_model.perform_pipeline(X_train_tfidf, y_train)\n",
    "print(\"Naive Bayes Accuracy:\", accuracy_nb)\n",
    "print(\"Naive Bayes Classification Report:\\n\", report_nb)\n",
    "\n",
    "# Naive Bayes with Count Vectorizer\n",
    "X_train_count, vectorizer_count = initialise_count_vectorizer(text_train)\n",
    "accuracy_nb_count, report_nb_count = naive_bayes_model.perform_pipeline(X_train_count, y_train)\n",
    "print(\"Naive Bayes with Count Vectorizer Accuracy:\", accuracy_nb_count)\n",
    "print(\"Naive Bayes with Count Vectorizer Classification Report:\\n\", report_nb_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2e6351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network with TF-IDF\n",
    "ohe_labels = [0 if label == 'neutral' else 1 if label == 'positive' else 2 for label in y_train]\n",
    "\n",
    "accuracy_nn, report_nn = neural_network_model.perform_pipeline(X_train_tfidf, ohe_labels)\n",
    "print(\"Neural Network Accuracy:\", accuracy_nn)\n",
    "print(\"Neural Network Classification Report:\\n\", report_nn)\n",
    "\n",
    "# Neural Network with Count Vectorizer\n",
    "accuracy_nn_count, report_nn_count = neural_network_model.perform_pipeline(X_train_count, ohe_labels)\n",
    "print(\"Neural Network with Count Vectorizer Accuracy:\", accuracy_nn_count)\n",
    "print(\"Neural Network with Count Vectorizer Classification Report:\\n\", report_nn_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d864c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wordpiece tokenized models TFIDF\n",
    "\n",
    "train_df = ns_dataset.get(Languages.HAUSA).train\n",
    "test_df = ns_dataset.get(Languages.HAUSA).test\n",
    "# Naive Bayes with wordpiece tokenized data\n",
    "wp_train_df = wordpiece_tokenize_dataframe(train_df, tokenizer)\n",
    "wp_test_df = wordpiece_tokenize_dataframe(test_df, tokenizer)\n",
    "\n",
    "wp_X_train_list = wp_train_df['tokenized_tweets'].tolist()\n",
    "wp_X_test_list = wp_test_df['tokenized_tweets'].tolist()\n",
    "\n",
    "# join sub lists into strings\n",
    "wp_X_train_list = [' '.join(tokens) for tokens in wp_X_train_list]\n",
    "wp_X_test_list = [' '.join(tokens) for tokens in wp_X_test_list]\n",
    "# Convert labels to numerical format (0 for neutral, 1 for positive, 2 for negative)\n",
    "wp_train_df['label'] = wp_train_df['label'].apply(lambda x: 0 if x == 'neutral' else 1 if x == 'positive' else 2)\n",
    "wp_test_df['label'] = wp_test_df['label'].apply(lambda x: 0 if x == 'neutral' else 1 if x == 'positive' else 2)\n",
    "\n",
    "wp_y_train = wp_train_df['label'].tolist()\n",
    "wp_y_test = wp_test_df['label'].tolist()\n",
    "\n",
    "tfidf_wp_train, vectorizer_wp = initialise_tfidf_vectorizer(wp_X_train_list)\n",
    "tfidf_wp_test, _ = initialise_tfidf_vectorizer(wp_X_test_list)\n",
    "\n",
    "tfidf_features = tfidf_wp_train.shape[1]  # Number of TF-IDF features\n",
    "num_classes = len(np.unique(wp_y_train))    # Number of classes\n",
    "\n",
    "\n",
    "# Initialize the corrected neural network\n",
    "neural_network_model = NeuralNetworkModel(\n",
    "    input_dim=tfidf_features, \n",
    "    num_classes=num_classes\n",
    ")\n",
    "\n",
    "\n",
    "# Naive Bayes with WordPiece tokenized data\n",
    "accuracy_nb, report_nb = naive_bayes_model.perform_pipeline(tfidf_wp_train, wp_y_train)\n",
    "print(\"Naive Bayes Accuracy:\", accuracy_nb)\n",
    "print(\"Naive Bayes Classification Report:\\n\", report_nb)\n",
    "\n",
    "# Logistic Regression with WordPiece tokenized data\n",
    "accuracy_lr_wp, report_lr_wp = logistic_regression_model.perform_pipeline(tfidf_wp_train, wp_y_train)\n",
    "print(\"Logistic Regression Accuracy:\", accuracy_lr_wp)\n",
    "print(\"Logistic Regression Classification Report:\\n\", report_lr_wp)\n",
    "\n",
    "# Neural Network with WordPiece tokenized data\n",
    "accuracy_nn_wp, report_nn_wp = neural_network_model.perform_pipeline(tfidf_wp_train, wp_y_train)\n",
    "print(\"Neural Network Accuracy:\", accuracy_nn_wp)\n",
    "print(\"Neural Network Classification Report:\\n\", report_nn_wp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe730df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to optimize n-grams and max features for TF-IDF\n",
    "def tfidf_score(input_x, y_train, score = None):\n",
    "    clf = LogisticRegression(max_iter=1000)\n",
    "    return cross_val_score(clf, X=input_x, y=y_train, scoring=score)\n",
    "scores_tfidf = tfidf_score(X_train_tfidf, y_train)\n",
    "print(\"5-fold Cross-Validation Accuracy for TFIDF: %0.2f (+/- %0.2f)\" % (scores_tfidf.mean(), scores_tfidf.std() * 2))\n",
    "\n",
    "scores_tfidf_f1 = tfidf_score(X_train_tfidf, y_train, score= 'f1_macro')\n",
    "\n",
    "print(\"5-fold Cross-Validation F1 score for TFIDF: %0.2f (+/- %0.2f)\" % (scores_tfidf_f1.mean(), scores_tfidf_f1.std() * 2))\n",
    "\n",
    "def test_param_combos(X_train, y_train, param_combos):\n",
    "    \"\"\"\n",
    "    Test different parameter combinations for a model.\n",
    "    \n",
    "    :param X_train: Training data features\n",
    "    :param y_train: Training data labels\n",
    "    :param param_combos: List of dictionaries with parameter combinations\n",
    "    :param score: Scoring metric to use\n",
    "    :return: DataFrame with results\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for params in param_combos:\n",
    "        X_train_tfidf, vectorizer_tfidf = initialise_tfidf_vectorizer(X_train, ngram=params.get('ngram_range'), max_features=params.get('max_features'))\n",
    "        score = tfidf_score(X_train_tfidf, y_train)\n",
    "        results.append({\n",
    "            'ngram_range': params.get('ngram_range'),\n",
    "            'max_features': params.get('max_features'),\n",
    "            'score': score.mean(),\n",
    "            'std_dev': score.std(),\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Example parameter combinations to test\n",
    "param_combos = [\n",
    "    {'ngram_range': (1,2), 'max_features': 5000},\n",
    "    {'ngram_range': (1,3), 'max_features': 5000},\n",
    "    {'ngram_range': (1,2), 'max_features': 10000},\n",
    "    {'ngram_range': (1,3), 'max_features': 10000},\n",
    "    {'ngram_range': (1,2), 'max_features': None},\n",
    "    {'ngram_range': (1,3), 'max_features': None},\n",
    "    {'ngram_range': (1,2), 'max_features': 2000},\n",
    "    {'ngram_range': (1,3), 'max_features': 2000},\n",
    "    {'ngram_range': (1,2), 'max_features': 3000},\n",
    "    {'ngram_range': (1,3), 'max_features': 3000},\n",
    "    {'ngram_range': (1,2), 'max_features': 4000},\n",
    "    {'ngram_range': (1,3), 'max_features': 4000},\n",
    "    {'ngram_range': (1,2), 'max_features': 6000},\n",
    "    {'ngram_range': (1,3), 'max_features': 6000},\n",
    "    {'ngram_range': (1,2), 'max_features': 7000},\n",
    "    {'ngram_range': (1,3), 'max_features': 7000},\n",
    "    {'ngram_range': (1,2), 'max_features': 8000},\n",
    "    {'ngram_range': (1,3), 'max_features': 8000},\n",
    "    {'ngram_range': (1,2), 'max_features': 9000},\n",
    "    {'ngram_range': (1,3), 'max_features': 9000},\n",
    "    {'ngram_range': (1,2), 'max_features': 10000},\n",
    "    {'ngram_range': (1,3), 'max_features': 10000},\n",
    "    {'ngram_range': (1,2), 'max_features': 12000},\n",
    "    {'ngram_range': (1,4), 'max_features': 5000},\n",
    "    {'ngram_range': (1,4), 'max_features': 10000},\n",
    "    {'ngram_range': (1,4), 'max_features': None},\n",
    "    {'ngram_range': (1,4), 'max_features': 2000},\n",
    "    {'ngram_range': (1,4), 'max_features': 3000},\n",
    "    {'ngram_range': (1,4), 'max_features': 4000},\n",
    "    {'ngram_range': (1,4), 'max_features': 6000},\n",
    "    {'ngram_range': (1,4), 'max_features': 7000},\n",
    "    {'ngram_range': (1,4), 'max_features': 8000},\n",
    "    {'ngram_range': (1,4), 'max_features': 9000},\n",
    "    {'ngram_range': (1,4), 'max_features': 10000},\n",
    "    {'ngram_range': (1,4), 'max_features': 12000},\n",
    "    {'ngram_range': (2,5), 'max_features': 5000},\n",
    "    {'ngram_range': (2,5), 'max_features': 10000},\n",
    "    {'ngram_range': (2,5), 'max_features': None},\n",
    "    {'ngram_range': (2,5), 'max_features': 2000},\n",
    "    {'ngram_range': (2,5), 'max_features': 3000},\n",
    "    {'ngram_range': (2,5), 'max_features': 4000},\n",
    "    {'ngram_range': (2,5), 'max_features': 6000},\n",
    "    {'ngram_range': (2,5), 'max_features': 7000},\n",
    "    {'ngram_range': (2,5), 'max_features': 8000},\n",
    "    {'ngram_range': (2,5), 'max_features': 9000},\n",
    "    {'ngram_range': (2,5), 'max_features': 10000},\n",
    "    {'ngram_range': (2,5), 'max_features': 12000},\n",
    "    {'ngram_range': (3,5), 'max_features': 5000},\n",
    "    {'ngram_range': (3,5), 'max_features': 10000},\n",
    "    {'ngram_range': (3,5), 'max_features': None},\n",
    "    {'ngram_range': (3,5), 'max_features': 2000},\n",
    "    {'ngram_range': (3,5), 'max_features': 3000},\n",
    "    {'ngram_range': (3,5), 'max_features': 4000},\n",
    "    {'ngram_range': (3,5), 'max_features': 6000},\n",
    "    {'ngram_range': (3,5), 'max_features': 7000},\n",
    "    {'ngram_range': (3,5), 'max_features': 8000},\n",
    "    {'ngram_range': (3,5), 'max_features': 9000},\n",
    "    {'ngram_range': (3,5), 'max_features': 10000},\n",
    "    {'ngram_range': (3,5), 'max_features': 12000}\n",
    "]\n",
    "# Test the parameter combinations\n",
    "results_df = test_param_combos(text_train, y_train, param_combos)\n",
    "# Sort the results by mean score\n",
    "results_df = results_df.sort_values(by='score', ascending=False)\n",
    "# Save the results to a CSV file\n",
    "results_df.to_csv('data/tfidf_param_combos_results.csv', index=False)\n",
    "# Print the top results\n",
    "print(\"Top parameter combinations based on accuracy:\")\n",
    "print(results_df.head(10))\n",
    "# Print the results DataFrame\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f394b130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes Classifier\n",
    "\n",
    "\n",
    "# Build a Gaussian Classifier\n",
    "model = MultinomialNB()\n",
    "df = ns_dataset.get(Languages.HAUSA).dev\n",
    "X = df['tweet']\n",
    "y = df['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)\n",
    "\n",
    "vectorizer = CountVectorizer() # No stop words since its a small african language dataset\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "# Model training\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train_vec, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test_vec)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39883754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of words\n",
    "df = ns_dataset.get(Languages.HAUSA).dev\n",
    "tweets = df['tweet']\n",
    "sentiment = df['label']\n",
    "vectorizer = CountVectorizer(lowercase=True, stop_words='english')  # remove stopwords\n",
    "X = vectorizer.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, sentiment, test_size = 0.3)\n",
    "\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Step 4: Evaluate\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00518e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a37d7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. Plot training history (optional)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
