{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232c4b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "from typing import Dict, List, Optional\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from tokenizers.models import WordPiece\n",
    "from tokenizers.trainers import WordPieceTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.normalizers import Sequence, Lowercase, NFD, StripAccents\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, GlobalAveragePooling1D, Dense, Dropout,Input, BatchNormalization, Activation\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from models import BasicModelEncapsulator, NeuralNetworkModel\n",
    "from custom_vectorizers import initialise_count_vectorizer, initialise_tfidf_vectorizer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be9680c",
   "metadata": {},
   "outputs": [],
   "source": [
    "REPO_URL_NS = \"https://github.com/hausanlp/NaijaSenti.git\"\n",
    "LOCAL_DIR_NS = \"NaijaSenti\"\n",
    "\n",
    "REPO_URL_AS = \"https://github.com/afrisenti-semeval/afrisent-semeval-2023.git\"\n",
    "LOCAL_DIR_AS = \"afrisent-semeval-2023\"\n",
    "\n",
    "def clone_repo(repo_url: str, local_dir: str) -> None:\n",
    "    if os.path.isdir(local_dir):\n",
    "        print(\"Repository exists. Updating...\")\n",
    "        subprocess.run([\"git\", \"-C\", local_dir, \"pull\", \"origin\", \"main\"], check=True)\n",
    "    else:\n",
    "        print(\"Repository not found. Cloning...\")\n",
    "        subprocess.run([\"git\", \"clone\", repo_url], check=True)\n",
    "\n",
    "clone_repo(REPO_URL_NS, LOCAL_DIR_NS)\n",
    "clone_repo(REPO_URL_AS, LOCAL_DIR_AS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d2fc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SplitSet:\n",
    "    \"\"\"\n",
    "    Holds the train, test, dev splits and stopwords for a single language.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 train: pd.DataFrame,\n",
    "                 test: pd.DataFrame,\n",
    "                 dev: pd.DataFrame,\n",
    "                 stopwords: Optional[List[str]] = None):\n",
    "        self.train = train\n",
    "        self.test = test\n",
    "        self.dev = dev\n",
    "        self.stopwords = stopwords if stopwords else []\n",
    "\n",
    "    def summary(self):\n",
    "        return {\n",
    "            \"train_size\": len(self.train),\n",
    "            \"test_size\": len(self.test),\n",
    "            \"dev_size\": len(self.dev),\n",
    "            \"num_stopwords\": len(self.stopwords),\n",
    "        }\n",
    "\n",
    "\n",
    "class MultiLangDataset:\n",
    "    \"\"\"\n",
    "    Manages NLP datasets split by language. Each language contains train/test/dev and stopwords.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.languages: Dict[str, SplitSet] = {}\n",
    "\n",
    "    def add_language(self, lang_code: str, split_set: SplitSet):\n",
    "        self.languages[lang_code] = split_set\n",
    "\n",
    "    def get(self, lang_code: str) -> Optional[SplitSet]:\n",
    "        return self.languages.get(lang_code)\n",
    "\n",
    "    def summary(self) -> Dict[str, Dict[str, int]]:\n",
    "        return {lang: split.summary() for lang, split in self.languages.items()}\n",
    "\n",
    "    def all_languages(self) -> List[str]:\n",
    "        return list(self.languages.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf8da7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ns_languages = ['hau', 'ibo', 'pcm', 'yor']\n",
    "class Languages:\n",
    "    \"\"\"\n",
    "    Contains the language codes for NaijaSenti dataset.\n",
    "    \"\"\"\n",
    "    HAUSA = 'hau'\n",
    "    IGBO = 'ibo'\n",
    "    NIGERIAN_PIDGIN = 'pcm'\n",
    "    YORUBA  = 'yor'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f37549",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_local_datasets(local_base_dir, languages=ns_languages, splits=['dev','test','train']):\n",
    "    dataset = MultiLangDataset()\n",
    "    \n",
    "    for lang in languages:\n",
    "        split_data = {}\n",
    "        for split in splits:\n",
    "            path = os.path.join(local_base_dir, lang, f\"{split}.tsv\")\n",
    "            try:\n",
    "                df = pd.read_csv(path, sep='\\t', encoding='utf-8')\n",
    "                # dataset[lang][split] = df\n",
    "                # dataset.add_language(lang, df)\n",
    "                split_data[split] = df\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to load {path}: {e}\")\n",
    "\n",
    "        # Read in stopwords\n",
    "        if local_base_dir.startswith(LOCAL_DIR_NS):\n",
    "            path = os.path.join(f'{LOCAL_DIR_NS}/data/stopwords/{lang}.csv')\n",
    "            try:\n",
    "                stopwords_df = pd.read_csv(path, encoding='utf-8')\n",
    "                split_data['stopwords'] = stopwords_df['word'].tolist()\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to load stopwords for {lang} from {path}: {e}\")\n",
    "\n",
    "        split_set = SplitSet(\n",
    "            train=split_data.get('train', pd.DataFrame()),\n",
    "            test=split_data.get('test', pd.DataFrame()),\n",
    "            dev=split_data.get('dev', pd.DataFrame()),\n",
    "            stopwords=split_data.get('stopwords', [])\n",
    "        )\n",
    "        dataset.add_language(lang, split_set)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b5a129",
   "metadata": {},
   "outputs": [],
   "source": [
    "ns_dataset: MultiLangDataset = load_local_datasets(local_base_dir=LOCAL_DIR_NS + '/data/annotated_tweets', languages=ns_languages) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947ce503",
   "metadata": {},
   "outputs": [],
   "source": [
    "as_dataset: MultiLangDataset = load_local_datasets(local_base_dir=f'afrisent-semeval-2023/data', languages=ns_languages,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d6eb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"NaijaSenti dataset loaded with languages:\", ns_dataset.all_languages())\n",
    "print(\"Afrisenti dataset loaded with languages:\", as_dataset.all_languages())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d903e898",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"NaijaSenti hau: \", ns_dataset.get(Languages.HAUSA).test)\n",
    "# Print each row in the dev set for the column 'tweet'\n",
    "for index, row in ns_dataset.get(Languages.HAUSA).test.iterrows():\n",
    "    print(f\"Index: {index}, Tweet: {row['tweet']}\")\n",
    "\n",
    "# write all the tweets into a textfile\n",
    "# check if the dir data exists, if not create it\n",
    "if not os.path.exists('data'):\n",
    "    os.makedirs('data')\n",
    "with open('data/naija_senti_hau_dev_tweets.txt', 'w', encoding='utf-8') as f:\n",
    "    for index, row in ns_dataset.get(Languages.HAUSA).dev.iterrows():\n",
    "        f.write(f\"{row['tweet']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9238776b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# tokenizer = WPTokenizer(WordPiece(unk_token=\"[UNK]\"))\n",
    "# tokenizer.normalizer = Sequence([NFD(), Lowercase(), StripAccents()])\n",
    "# tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "# trainer = WordPieceTrainer(vocab_size=8000, special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"])\n",
    "# df = ns_dataset.get(Languages.HAUSA).dev\n",
    "# # Collect all tweets into a single list for training\n",
    "# tweets = df['tweet'].tolist()\n",
    "# tokenizer.train_from_iterator(tweets, trainer)\n",
    "# tokenizer.save(\"data/wordpiece.json\")\n",
    "\n",
    "# # Method 3: Add both tokens and IDs\n",
    "# df['tokenized_tweets'] = df['tweet'].apply(lambda x: tokenizer.encode(x).tokens)\n",
    "# df['token_ids'] = df['tweet'].apply(lambda x: tokenizer.encode(x).ids)\n",
    "\n",
    "# # Display results\n",
    "# print(\"Original vs Tokenized:\")\n",
    "# print(\"=\" * 80)\n",
    "# for i in range(min(5, len(df))):  # Show first 5 examples\n",
    "#     print(f\"Original: {df.iloc[i]['tweet']}\")\n",
    "#     print(f\"Tokens:   {df.iloc[i]['tokenized_tweets']}\")\n",
    "#     print(f\"IDs:      {df.iloc[i]['token_ids']}\")\n",
    "#     print(\"-\" * 80)\n",
    "\n",
    "# with open('data/naija_senti_hau_dev_tweets_tokenized.txt', 'w', encoding='utf-8') as f:\n",
    "#     for index, row in ns_dataset.get(Languages.HAUSA).dev.iterrows():\n",
    "#         tokens = tokenizer.encode(row['tweet']).tokens\n",
    "#         f.write(\" \".join(tokens) + \"\\n\")\n",
    "\n",
    "# adjust the below to read into a list of strings\n",
    "\n",
    "# def read_tokenized_file(file_path: str) -> List[List[str]]:\n",
    "#     \"\"\"\n",
    "#     Reads a tokenized file and returns a list of token lists.\n",
    "#     \"\"\"\n",
    "#     with open(file_path, 'r', encoding='utf-8') as f:\n",
    "#         return [line.strip().split() for line in f.readlines()]\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98629eef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3182ddf5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340fc10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0078212",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "df = ns_dataset.get(Languages.HAUSA).train\n",
    "text_train, text_test, y_train, y_test = train_test_split(df.tweet, df.label, test_size = 0.3)\n",
    "X_train_tfidf, vectorizer_tfidf = initialise_tfidf_vectorizer(text_train)\n",
    "X_train_count, vectorizer_count = initialise_count_vectorizer(text_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1372129",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb664a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of features from your TF-IDF matrix\n",
    "# tfidf_features = X_train_tfidf.shape[1]  # Number of TF-IDF features\n",
    "# num_classes = len(np.unique(y_train))    # Number of sentiment classes\n",
    "\n",
    "# Initialize models\n",
    "\n",
    "\n",
    "logistic_regression_model = BasicModelEncapsulator(LogisticRegression(max_iter=1000))\n",
    "naive_bayes_model = BasicModelEncapsulator(MultinomialNB())\n",
    "\n",
    "tfidf_features = X_train_tfidf.shape[1]  # Number of TF-IDF features\n",
    "num_classes = len(np.unique(y_train))    # Number of classes\n",
    "\n",
    "\n",
    "# Initialize the corrected neural network\n",
    "neural_network_model = NeuralNetworkModel(\n",
    "    input_dim=tfidf_features, \n",
    "    num_classes=num_classes\n",
    ")\n",
    "\n",
    "# Perform pipelines\n",
    "print(\"Training models...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b3a23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression with TF-IDF\n",
    "accuracy_lr, report_lr = logistic_regression_model.perform_pipeline(X_train_tfidf, y_train)\n",
    "print(\"Logistic Regression Accuracy:\", accuracy_lr)\n",
    "print(\"Logistic Regression Classification Report:\\n\", report_lr)\n",
    "\n",
    "# Logistic Regression with Count Vectorizer\n",
    "X_train_count, vectorizer_count = initialise_count_vectorizer(text_train)\n",
    "accuracy_lr_count, report_lr_count = logistic_regression_model.perform_pipeline(X_train_count, y_train)\n",
    "print(\"Logistic Regression with Count Vectorizer Accuracy:\", accuracy_lr_count)\n",
    "print(\"Logistic Regression with Count Vectorizer Classification Report:\\n\", report_lr_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad9c304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes with TF-IDF\n",
    "accuracy_nb, report_nb = naive_bayes_model.perform_pipeline(X_train_tfidf, y_train)\n",
    "print(\"Naive Bayes Accuracy:\", accuracy_nb)\n",
    "print(\"Naive Bayes Classification Report:\\n\", report_nb)\n",
    "\n",
    "# Naive Bayes with Count Vectorizer\n",
    "X_train_count, vectorizer_count = initialise_count_vectorizer(text_train)\n",
    "accuracy_nb_count, report_nb_count = naive_bayes_model.perform_pipeline(X_train_count, y_train)\n",
    "print(\"Naive Bayes with Count Vectorizer Accuracy:\", accuracy_nb_count)\n",
    "print(\"Naive Bayes with Count Vectorizer Classification Report:\\n\", report_nb_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2e6351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network with TF-IDF\n",
    "ohe_labels = [0 if label == 'neutral' else 1 if label == 'positive' else 2 for label in y_train]\n",
    "\n",
    "accuracy_nn, report_nn = neural_network_model.perform_pipeline(X_train_tfidf, ohe_labels)\n",
    "print(\"Neural Network Accuracy:\", accuracy_nn)\n",
    "print(\"Neural Network Classification Report:\\n\", report_nn)\n",
    "\n",
    "# Neural Network with Count Vectorizer\n",
    "accuracy_nn_count, report_nn_count = neural_network_model.perform_pipeline(X_train_count, ohe_labels)\n",
    "print(\"Neural Network with Count Vectorizer Accuracy:\", accuracy_nn_count)\n",
    "print(\"Neural Network with Count Vectorizer Classification Report:\\n\", report_nn_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d864c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wordpiece tokenized models TFIDF\n",
    "\n",
    "from subword_tokenizer import get_tokenizer, wordpiece_tokenize_dataframe\n",
    "\n",
    "tokenizer = get_tokenizer(df=ns_dataset.get(Languages.HAUSA).train)\n",
    "\n",
    "train_df = ns_dataset.get(Languages.HAUSA).train\n",
    "test_df = ns_dataset.get(Languages.HAUSA).test\n",
    "# Naive Bayes with wordpiece tokenized data\n",
    "wp_train_df = wordpiece_tokenize_dataframe(train_df, tokenizer)\n",
    "wp_test_df = wordpiece_tokenize_dataframe(test_df, tokenizer)\n",
    "\n",
    "wp_X_train_list = wp_train_df['tokenized_tweets'].tolist()\n",
    "wp_X_test_list = wp_test_df['tokenized_tweets'].tolist()\n",
    "\n",
    "# join sub lists into strings\n",
    "wp_X_train_list = [' '.join(tokens) for tokens in wp_X_train_list]\n",
    "wp_X_test_list = [' '.join(tokens) for tokens in wp_X_test_list]\n",
    "# Convert labels to numerical format (0 for neutral, 1 for positive, 2 for negative)\n",
    "wp_train_df['label'] = wp_train_df['label'].apply(lambda x: 0 if x == 'neutral' else 1 if x == 'positive' else 2)\n",
    "wp_test_df['label'] = wp_test_df['label'].apply(lambda x: 0 if x == 'neutral' else 1 if x == 'positive' else 2)\n",
    "\n",
    "wp_y_train = wp_train_df['label'].tolist()\n",
    "wp_y_test = wp_test_df['label'].tolist()\n",
    "\n",
    "tfidf_wp_train, vectorizer_wp = initialise_tfidf_vectorizer(wp_X_train_list)\n",
    "tfidf_wp_test, _ = initialise_tfidf_vectorizer(wp_X_test_list)\n",
    "\n",
    "tfidf_features = tfidf_wp_train.shape[1]  # Number of TF-IDF features\n",
    "num_classes = len(np.unique(wp_y_train))    # Number of classes\n",
    "\n",
    "\n",
    "# Initialize the corrected neural network\n",
    "neural_network_model = NeuralNetworkModel(\n",
    "    input_dim=tfidf_features, \n",
    "    num_classes=num_classes\n",
    ")\n",
    "\n",
    "\n",
    "# Naive Bayes with WordPiece tokenized data\n",
    "accuracy_nb, report_nb = naive_bayes_model.perform_pipeline(tfidf_wp_train, wp_y_train)\n",
    "print(\"Naive Bayes Accuracy:\", accuracy_nb)\n",
    "print(\"Naive Bayes Classification Report:\\n\", report_nb)\n",
    "\n",
    "# Logistic Regression with WordPiece tokenized data\n",
    "accuracy_lr_wp, report_lr_wp = logistic_regression_model.perform_pipeline(tfidf_wp_train, wp_y_train)\n",
    "print(\"Logistic Regression Accuracy:\", accuracy_lr_wp)\n",
    "print(\"Logistic Regression Classification Report:\\n\", report_lr_wp)\n",
    "\n",
    "# Neural Network with WordPiece tokenized data\n",
    "accuracy_nn_wp, report_nn_wp = neural_network_model.perform_pipeline(tfidf_wp_train, wp_y_train)\n",
    "print(\"Neural Network Accuracy:\", accuracy_nn_wp)\n",
    "print(\"Neural Network Classification Report:\\n\", report_nn_wp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe730df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to optimize n-grams and max features for TF-IDF\n",
    "def tfidf_score(input_x, y_train, score = None):\n",
    "    clf = LogisticRegression(max_iter=1000)\n",
    "    return cross_val_score(clf, X=input_x, y=y_train, scoring=score)\n",
    "scores_tfidf = tfidf_score(X_train_tfidf, y_train)\n",
    "print(\"5-fold Cross-Validation Accuracy for TFIDF: %0.2f (+/- %0.2f)\" % (scores_tfidf.mean(), scores_tfidf.std() * 2))\n",
    "\n",
    "scores_tfidf_f1 = tfidf_score(X_train_tfidf, y_train, score= 'f1_macro')\n",
    "\n",
    "print(\"5-fold Cross-Validation F1 score for TFIDF: %0.2f (+/- %0.2f)\" % (scores_tfidf_f1.mean(), scores_tfidf_f1.std() * 2))\n",
    "\n",
    "def test_param_combos(X_train, y_train, param_combos):\n",
    "    results = []\n",
    "    for params in param_combos:\n",
    "        X_train_tfidf, vectorizer_tfidf = initialise_tfidf_vectorizer(X_train, ngram=params.get('ngram_range'), max_features=params.get('max_features'))\n",
    "        score = tfidf_score(X_train_tfidf, y_train)\n",
    "        results.append({\n",
    "            'ngram_range': params.get('ngram_range'),\n",
    "            'max_features': params.get('max_features'),\n",
    "            'score': score.mean(),\n",
    "            'std_dev': score.std(),\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Example parameter combinations to test\n",
    "param_combos = [\n",
    "    {'ngram_range': (1,2), 'max_features': 5000},\n",
    "    {'ngram_range': (1,3), 'max_features': 5000},\n",
    "    {'ngram_range': (1,2), 'max_features': 10000},\n",
    "    {'ngram_range': (1,3), 'max_features': 10000},\n",
    "    {'ngram_range': (1,2), 'max_features': None},\n",
    "    {'ngram_range': (1,3), 'max_features': None},\n",
    "    {'ngram_range': (1,2), 'max_features': 2000},\n",
    "    {'ngram_range': (1,3), 'max_features': 2000},\n",
    "    {'ngram_range': (1,2), 'max_features': 3000},\n",
    "    {'ngram_range': (1,3), 'max_features': 3000},\n",
    "    {'ngram_range': (1,2), 'max_features': 4000},\n",
    "    {'ngram_range': (1,3), 'max_features': 4000},\n",
    "    {'ngram_range': (1,2), 'max_features': 6000},\n",
    "    {'ngram_range': (1,3), 'max_features': 6000},\n",
    "    {'ngram_range': (1,2), 'max_features': 7000},\n",
    "    {'ngram_range': (1,3), 'max_features': 7000},\n",
    "    {'ngram_range': (1,2), 'max_features': 8000},\n",
    "    {'ngram_range': (1,3), 'max_features': 8000},\n",
    "    {'ngram_range': (1,2), 'max_features': 9000},\n",
    "    {'ngram_range': (1,3), 'max_features': 9000},\n",
    "    {'ngram_range': (1,2), 'max_features': 10000},\n",
    "    {'ngram_range': (1,3), 'max_features': 10000},\n",
    "    {'ngram_range': (1,2), 'max_features': 12000},\n",
    "    {'ngram_range': (1,4), 'max_features': 5000},\n",
    "    {'ngram_range': (1,4), 'max_features': 10000},\n",
    "    {'ngram_range': (1,4), 'max_features': None},\n",
    "    {'ngram_range': (1,4), 'max_features': 2000},\n",
    "    {'ngram_range': (1,4), 'max_features': 3000},\n",
    "    {'ngram_range': (1,4), 'max_features': 4000},\n",
    "    {'ngram_range': (1,4), 'max_features': 6000},\n",
    "    {'ngram_range': (1,4), 'max_features': 7000},\n",
    "    {'ngram_range': (1,4), 'max_features': 8000},\n",
    "    {'ngram_range': (1,4), 'max_features': 9000},\n",
    "    {'ngram_range': (1,4), 'max_features': 10000},\n",
    "    {'ngram_range': (1,4), 'max_features': 12000},\n",
    "    {'ngram_range': (2,5), 'max_features': 5000},\n",
    "    {'ngram_range': (2,5), 'max_features': 10000},\n",
    "    {'ngram_range': (2,5), 'max_features': None},\n",
    "    {'ngram_range': (2,5), 'max_features': 2000},\n",
    "    {'ngram_range': (2,5), 'max_features': 3000},\n",
    "    {'ngram_range': (2,5), 'max_features': 4000},\n",
    "    {'ngram_range': (2,5), 'max_features': 6000},\n",
    "    {'ngram_range': (2,5), 'max_features': 7000},\n",
    "    {'ngram_range': (2,5), 'max_features': 8000},\n",
    "    {'ngram_range': (2,5), 'max_features': 9000},\n",
    "    {'ngram_range': (2,5), 'max_features': 10000},\n",
    "    {'ngram_range': (2,5), 'max_features': 12000},\n",
    "    {'ngram_range': (3,5), 'max_features': 5000},\n",
    "    {'ngram_range': (3,5), 'max_features': 10000},\n",
    "    {'ngram_range': (3,5), 'max_features': None},\n",
    "    {'ngram_range': (3,5), 'max_features': 2000},\n",
    "    {'ngram_range': (3,5), 'max_features': 3000},\n",
    "    {'ngram_range': (3,5), 'max_features': 4000},\n",
    "    {'ngram_range': (3,5), 'max_features': 6000},\n",
    "    {'ngram_range': (3,5), 'max_features': 7000},\n",
    "    {'ngram_range': (3,5), 'max_features': 8000},\n",
    "    {'ngram_range': (3,5), 'max_features': 9000},\n",
    "    {'ngram_range': (3,5), 'max_features': 10000},\n",
    "    {'ngram_range': (3,5), 'max_features': 12000}\n",
    "]\n",
    "# Test the parameter combinations\n",
    "results_df = test_param_combos(text_train, y_train, param_combos)\n",
    "# Sort the results by mean score\n",
    "results_df = results_df.sort_values(by='score', ascending=False)\n",
    "# Save the results to a CSV file\n",
    "results_df.to_csv('data/tfidf_param_combos_results.csv', index=False)\n",
    "# Print the top results\n",
    "print(\"Top parameter combinations based on accuracy:\")\n",
    "print(results_df.head(10))\n",
    "# Print the results DataFrame\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f394b130",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39883754",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00518e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a37d7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 11. Plot training history (optional)\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.figure(figsize=(12, 4))\n",
    "\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "# plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "# plt.title('Model Accuracy')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.legend()\n",
    "\n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.plot(history.history['loss'], label='Training Loss')\n",
    "# plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "# plt.title('Model Loss')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.legend()\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
